\chapter{Research Methodology and Dataset}

To better comprehend ADRs and their usage, we will examine possible topics presented in ADRs. This chapter outlines the mixed-methods research approach used to ADR documents from GitHub, leveraging machine learning techniques and empirical observations. It further discusses the selection, data collection, cleaning process, and validity of the dataset, and sets the stage for interpreting results to answer the defined research questions.

\section{Research Approach}
    % General Overview of what was done
    In this study, a mixed-methods approach was adopted to analyze the topics addressed in ADRs from a dataset gathered from GitHub by practitioners. The goal was to uncover patterns and prominent architectural interests within the practitioner community and how those interests relate to the general topics of software architecture. 

    % General overview of how it was done and why it differs from others 
    Specifically, a dataset from a previous mining software repositories (MSR) study on ADRs, that resulted from a systematic process and guidelines, \cite{Github_study_ADRs, MSR_systematic_process} was leveraged to perform analysis on the records' contents using machine learning techniques and empirical observations to interpret and classify the results. While the researchers focused on finding out how widespread the use and adoption of ADRs has become, aiming to uncover the trends and metadata in regards to the ADRs that were mined, this study's objective is to determine the topics of the aforementioned records by getting insights into their contents and architectural fields of interest.

    % Dataset origin and why use it (skipped my modifications of the dataset since they are mentioned in the next chapter)
    The dataset was selected and deemed appropriate for the study for numerous reasons. Firstly, because of the recency of the data, dating back to 2020, four years before the present study was conducted. Secondly, because of the platform used to derive the dataset, as GitHub is the largest platform for open source software repositories by a significant margin\footnote{https://osssoftware.org/blog/open-source-software-repository-management/}. Lastly, because of the dataset quality and quantity, as it derived from an exhaustive automated search of all publicly available repositories  (estimated 128,411,417 as of February 26 2020\footnote{https://github.com/bugout-dev/mirror/blob/master/notebooks/rise-of-github.ipynb}) and the selection process was strict, involving manual and automated screening of results, applying concrete inclusion and exclusion criteria to identify repositories actually using ADRs for documenting ADDs, while excluding those with low quality documents.

    % Methods of analysis and why i used them
    For the identification of prevalent topics within the records' contents, a number of techniques were used, focusing on machine learning approaches applied using automated methods in Python. In detail, topic modeling, a text mining technique, was utilized for its effectiveness to extract abstract topics and word clusters from a collection of documents. This method was employed as it has also demonstrated efficacy in other software engineering empirical studies, analyzing text mostly related to developer communication and bug reports \cite{Topic_modeling_in_software_engineering_research}. Approaches using Latent Dirichlet Allocation (LDA) were also employed as they constitute common practice in the topic modelling space, with mixed results. Combining all the findings, a specific stack of algorithms and techniques was chosen, described in detail in the following chapters and facilitated by the use of the BERTopic Python library \cite{bertTopic}.

    % Result interpretation
    To interpret the analysis results, multiple approaches were utilized as there is no definitive method to objectively label topic clusters, which also depends on the reader's perspective \cite{datasciencecentralTopicModeling}. In the absence of ground truth, keywords from the record clusters were analyzed and interpreted using large language models (LLMs) as representation models. The labels were then cross-checked with the most representative documents of each cluster, specifically those that are semantically closest to the center of the cluster based on their distance in the vector space. Additionally, in certain cases, manual topic naming was performed based on frequent words within a topic, a common practice in topic modeling to reduce ambiguity \cite{Topic_modeling_in_software_engineering_research}.
    
\section{Research Objectives and Questions}
    As mentioned, The main objective of this study is to uncover prominent software architecture topics by examining a collection of ADRs. To achieve those research objectives,the following research questions were defined (RQs):

    \begin{itemize}
        \item \textbf{RQ1}: What are the most frequently discussed software architecture topics in ADRs?
        \item \textbf{RQ2}: How do topics from the software architecture space map to those present in ADRs?
        \item \textbf{RQ3}: Which ADR topics cannot be classified, and what are the reasons behind this? 
    \end{itemize}. 

    The aim of RQ1 is to outline the most prevalent topics that concern open source maintainers and owners of the repositories whose ADRs are being analyzed, effectively examining the most common design decisions projects face.
    
    RQ2 aims to compare the identified topics with various aspects of software architecture to determine if they are aligned. This comparison could also provide insights into less prevalent software architecture topics as well as the breadth of the ADRs in the dataset and suggest future directions for enriching it.
    
    Finally RQ3, aims to discover the intricacies that outlier ADRs might have. This, on the one hand may help discover new architectural concerns or interesting architecture sub-domains that have not been yet thoroughly researched. On the other hand, it could also just discover ADRs that talk about topics too niche to be considered, validating potential suspicions about their irrelevance.

    A Venn diagram containing all the possible topic intersections derived from the research questions can be seen in figure \ref{fig:RQ_Venn}.

    \begin{figure}[h]
        \centering
        \includegraphics[width=\textwidth]{figures/RQ_VADR_VENN_WHITE.png}
        \caption{Venn diagram of possible ADR topics}
        \label{fig:RQ_Venn}
    \end{figure}        

\section{Data Collection Process}
    The dataset used for the study was obtained from a previous MSR study\cite{Github_study_ADRs}. The researchers preformed an exhaustive search on GitHub in 2020 which included going through all registered platform users to find public repositories that contained files that matched various template formats of ADRs. After this process was completed, the repositories were also verified manually according to rigorous inclusion and exclusion criteria. The resulting dataset contained metadata for 6362 ADRs from 921 software repositories. However, since the researchers focused mainly on measuring trends and use of ADRs, only the metadata were stored and made publicly available in a separate GitHub repository\footnote{https://github.com/software-competence-center-hagenberg/ADR-Study-Dataset}. The dataset files, one for each repository, were stored in a JSON format and contained information about the repository such as its hyperlink, the number and path of ADR directories, and detailed metadata for each ADR as seen in figure 5.1\ref{fig:JSON_data}. This metadata included the directory path, file path, type of ADR template used, status, timestamps of the first and last commits, the number of commits, and details about the authors. Because of the need to focus on the contents of each ADR, each repository needed to be cloned (downloaded) and the ADRs to be extracted from the specified folder as mentioned in the metadata. This also accounted for newly created ADRs after the date the data was scrapped since the projects may have continued their use even after the study was published. It is important to note that some GitHub repositories (~20) were deleted, which prevented their inclusion in the dataset. The whole was done in an automated way via a Python script. The final contents of the respective folders were saved and examined.

    \begin{figure}
        \centering
        \includegraphics[scale=0.4]{figures/JSON_data_example.png}
        \caption{Example of a JSON metadata file derived from the base dataset}
        \label{fig:JSON_data}
    \end{figure}
    
\section{Data Cleaning and Enrichment}
    After the contents of the ADR folders were obtained, at first, the files needed to undergo a cleaning process. I decided to include only markdown files since they comprised the vast majority of the actual files downloaded at a\%age of more than 98\% and was the dominant file type for the ADRs as proposed in the Markdown Architectural Decision Records \cite{MarkdownADRs} article. This also eliminated exogenous files such as any source code that was stored in the folders in question. Next, irrelevant files and files related to ADRs that did not provide any actual information of use needed to be deleted such as licences, ADR templates, contributing guides and ``README'' files. ADR drafts and examples were also excluded since they did not relate to any specific topic. This procedure was also executed automatically via Python scripts and it resulted in 5368 ADRs markdown files.

    The final cleaning process was performed on the ADR text itself to reduce language-dependent factors in order for the topic modelling analysis to produce more coherent results. Text is generally written in natural language for human understanding, but in text mining, this data is not always straightforward for computers to process as it frequently contains words and other elements that provide no information gain to algorithms and models. Therefore, it is common practice in topic modelling, for text to undergo preprocessing where specific tokens from documents are usually removed. \cite{topic-modelling-text-cleaning, text-mining-lda}. In this case, all non-alphabetic characters were removed and all text was converted to lower case. In addition, stop words, stemmed from the ntlk natural language toolkit Python library were also removed as they were deemed insignificant. These included pronouns in many forms from a commonly used list of 127 entries\footnote{https://gist.github.com/sebleier/554280}. Furthermore, all words with less than 3 characters were also filtered out and all remaining words were lemmatized or reduced to their base or root form, by removing inflections and derivational suffixes, ensuring that related forms of a word are treated as a single item. Finally, a list of unwanted terms was manually assembled after careful examination of a subset of documents. This list included words from common ADR template titles such as ``context'', ``decision'', ``status'', ``consequences'', ``motivation'', ``options'', ``option'', ``alternatives'' and others, that were present in almost all documents and did not provide any context on their topics, but instead were used to honor the ADR format. Other common but unwanted words included the name of the proposer of a popular ADR template ``Michael Nygard'' \footnote{https://cognitect.com/blog/2011/11/15/documenting-architecture-decisions} and were also filtered out. Code-specific stopwords such as ``class'', ``interface'' and ``abstract'' were not removed as to provide information about the implementation of the decision. The final form of each document was reduced to a series of words that captured the semantic properties and general meaning of the architectural decision. An example cleaned ADR can be seen in figure \ref{fig:Cleaned_ADR} and an overview of the cleaning process in figure \ref{fig:Data_cleaning_steps}.
    
    \begin{figure}
        \centering
        \includegraphics[scale=0.6]{figures/adr_cleaned_example.jpeg}
        \caption{Example of an ADR about creating a web interface after undergoing preprocessing}
        \label{fig:Cleaned_ADR}
    \end{figure}

    \begin{figure}[ht]
        \centering
        % \fontsize{7}{8}\selectfont
        \includegraphics[width=\textwidth]{figures/data_cleaning_steps_final2.pdf}
        \caption{Data cleaning process}
        \label{fig:Data_cleaning_steps}
    \end{figure}
    
\section{Dataset Validity and Quality}
    While this dataset provides a valuable snapshot of ADR usage in open-source projects, several validity and quality concerns need to be addressed. Firstly, despite it being derived from a broad search on all GitHub repositories, the final sample size remains small. There exist a plethora of reasons for why this may be observed. ADRs are not yet widely adopted, which is understandable since they were introduced recently, with the earliest ADR commit in the dataset dating back to May 2013. Furthermore, it is possible that many ADRs remain private, either in private code repositories or internal company documentation, not found on any code hosting platforms. The later can be confirmed by the statements of companies like Spotify and Amazon, as mentioned in chapter 3, that choose to keep code proprietary and records private.
    In addition, in regards to ADR content and selection, potential over-filtering might have removed relevant information, and the manual assembly of unwanted terms introduces an element of subjectivity. In contrast, it is possible that records and phrases that provide no additional meaning were not filtered and a more thorough approach of examining the records before cleaning them is needed. To promote dataset quality, future research could include expanding the dataset size with private records, or public ADRs that are not stored on GitHub and refining the cleaning process to mitigate these limitations. 