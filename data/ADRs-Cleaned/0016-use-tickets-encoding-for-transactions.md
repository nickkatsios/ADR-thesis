ticket encoding transaction table transaction technical architectural record still work progress transaction table atlasdb atlasdb transaction table keep track whether transaction started given timestamp committed aborted case committed commit timestamp logically table mapping longs longs special value meaning transaction aborted transaction inflight yet either commit abort entry table starttimestamp committimestamp table accessed via transactionservice class atlasdb offer simple interface codeblock java public interface transactionservice checkfornull long getlong starttimestamp maplong long getiterablelong starttimestamps void putunlessexistslong starttimestamp long committimestamp throw keyalreadyexistsexception practice calling single multitimestamp version get may read table though nonnull result may cached putunlessexists performs putunlessexists operation table supported mean thrift call cassandra insert exists sql statement relational physical representation cassandra atlasdb table similar schema cassandra blob partition key store row called key clustering key two component blob column called column biginteger timestamp called column blob value transaction table column always single byte corresponding string column special value practice dont pay much attention value interestingly key varlong encoding start timestamp value similarly varlong encoding commit timestamp cassandra representation transaction table introduced may look follows key column column value xffffffffffffffffff xefefd xefefdb detail varlong encoding fiddly exhibit several desirable property varlong encoding orderpreserving nonnegative number varlongts varlongts vice versa significant allows one perform range scan transaction table given timestamps atlas positive longs straightforward way varlong encoding support negative number meaning encoding positive timestamps transaction successfully committed transaction aborted however choice encoding also issue two particularly relevant one follows varlong encoding number near numerically also close bytespace notice encoded form differ three lowestorder bit furthermore writes transaction table take place number numerically close correspond actively running transaction thus key close bytespace considering cassandra consistent hashing handle data partitioning given point time majority writes cluster end going node hotspotting undesirable lose horizontal scalability writes bottlenecked single node regardless size cluster varlong encoding particularly efficient purpose fails exploit characteristic distribution data particular special value particularly large varlong encoding negative longs always encode byte storing full value commit timestamp also wasteful given know generally slightly higher value start timestamp principle good transaction service thus define three principle help guide make implementation transaction service good one horizontal scalability good transaction service must horizontally scalable possible increase write bandwidth increasing number database node andor service node performing writing compact representation good transaction service unnecessarily excessive disk space addition saving disk usage compact representation also improves ability query result cached memory improve time taken execute query data read disk many key value service allow cache specified memory overhead compact timestamp representation improves ability cache store logical information thus improving hit rate range scan good transaction service support range scan without needing read entire table postfilter various backup restore workflow atlasdb important able execute restores timely fashion reasonable usage pattern rup good transaction service underlying service must follow standard principle constitutes reasonable usage pattern underlying service implement ticket encoding strategy along feature needed support efficient operation strategy supporting feature introduced following section ticket encoding strategy logical overview divide domain positive longs disjoint partition constant size call size partitioning quantum partition contiguous range start multiple thus first partition consists timestamps second assign constant number row partition seek distribute start timestamps evenly possible among row number increase practice least significant bit timestamp row number would thus store value associated timestamps row given partition value interval disambiguate timestamps dynamic column key varlong encoding timestamps offset relative base formally given timestamp proceed follows denotes integer division identify row belongs given identify column belongs given notice given similarly decode original identify relevant partition given identify offset column key given identify offset second part row key given original timestamp may easier think timestamp written tuple row component pair column key divide bijection tuples range exclusive range exclusive furthermore bijection orderpreserving ordering tuples interpreted lexicographically diagram illustrate clearly work physical implementation ticket store information transaction committed new encoding scheme transaction table cassandra given want avoid hotspotting ensure horizontal scalability ensure row may writing data distributed differently bytespace thus reverse bit row encoding codeblock java private static byte encoderownamelong starttimestamp long row starttimestamp partitioningquantum rowsperquantum starttimestamp partitioningquantum rowsperquantum return ptbytestobyteslongreverserow fixedlong encoding constant byte instead variable chosen ensure range scan supported reversed form variable length encoding tend amenable range scan dynamic column key simply varlong encoding changed encoding value well transaction successfully committed delta encoding scheme instead take varlong difference commit start timestamps typically small positive number help keep size table separately transaction aborted store empty byte array special value instead negative number unnecessarily large choosing choose value based characteristic keyvalueservice storing timestamp data recalling principle rup simplify discussion section assume divide cassandra following atlas team recommended cassandra best practice seek bound size individual row considering varlong take byte positive integer explicitly empty byte array represent transaction failed commit estimate maximum size row given choice value notice number timestamps actually store given since row partition different given startcommit timestamp pair row key occupies byte said sstables row key represented thus focus column key value column key varlong encoded number bounded thats largest offset might actually store value theoretically could full byte large positive number though practice likely considerably smaller selected value configuration row store startcommit timestamp pair thus number row varlong encoding able represent within byte account bit space cassandra create composite buffer include column part physical row take additional byte thus byte column key byte value leading total byte per startcommit timestamp pair row bounded leaf quite bit headroom worth mentioning practice implementation reason unlikely full startcommit timestamp pair single row practice value likely byte rather byte case even adverse circumstance still avoid generating excessively wide row streamlining putunlessexists putunlessexists operation performed serial consistency level cassandra meaning read writes paxos consensus thrift expose checkandset operation apis codeblock java casresult casrequired binary key required string columnfamily list expected list update required consistencylevel serialconsistencylevelconsistencylevelserial required consistencylevel commitconsistencylevelconsistencylevelquorum throw invalidrequestexception ire unavailableexception timedoutexception sufficient original transaction schema row key store information one start timestamp putunlessexists operation empty row row one column however notice sufficient transaction row may contain data multiple start timestamps api requires provide list old column dont know beforehand considered reading existing row adding new column cql api behaviour insert exists column match semantics want however solution found unacceptable performance benchmarking thus decided extend thrift interface add support multicolumn putunlessexists operation semantics want different empty list succeeds long existing column column family provided key overlap set column added codeblock java casresult putunlessexistsrequired binary key required string columnfamily list update required consistencylevel serialconsistencylevelconsistencylevelserial required consistencylevel commitconsistencylevelconsistencylevelquorum throw invalidrequestexception ire unavailableexception timedoutexception multinode contention residue improvement still run issue client whether across multiple service node node issue multiple request parallel putunlessexists request requires round paxos cassandra maintains paxos sequence level partition key request would contend far paxos concerned even column actually disjoint internally cassandra node trying apply update partition whether update applied order take place agreed paxos although node accepting multiple proposal dont conflict one round consensus committed time since update conditional also cassandra leaderless implementation paxos meaning dueling proposer issue might slow individual round protocol multiple node trying concurrently propose value batching request client side partition could useful though still limited performance would poor service many node cassandra table tuning creating table cassandra one may specify table property tune way data handled knowledge access pattern data layout transaction table improve performance cassandra able provide specific case bloom filter cassandra keep track bloom filter sstable memory avoid read sstable data file bloom filter keep track whether sstable contains data specific row thus allows cassandra determine without performing operation whether given sstable probably contains data row definitely contain data row probability bloom filter return false positive configurable via bloomfilterfpchance though accurate bloom filter require ram cassandra documentation suggests typical value lie typically within atlasdb false positive rate set depending whether table append heavy read light mean given size tiered compaction strategy whether negative lookup expected frequent determined user schema static final double defaultleveledcompactionbloomfilterfpchance static final double defaultsizetieredcompactionbloomfilterfpchance static final double negativelookupsbloomfilterfpchance static final double negativelookupssizetieredbloomfilterfpchance far transaction concerned observe number partition small thus afford low setting thus set bloomfilterfpchance empirically observed setting bloom filter transaction table writing every timestamp one one billion contrast bloom filter transaction atlasdbs existing setting index interval cassandra partition index ondisk file store mapping partition key offset within sstable cassandra maintains partition summary sstable memory sample partition index every key store offset location mapping given key within file value may tuned smaller value require memory improve performance seek within partition index shorter transaction number partition expected small set minindexinterval maxindexinterval forcing partition index perfect compression chunk length cassandra compress sstables block disk block configurable size choosing larger block may enable better compression since similarity column value may exploited expense needing read data disk read occurs typically atlas set reduce amount transaction expect user often reading data relatively smaller working set case larger chunk size enables better compression increase proportion said working set maintained memory experimented several setting found good balance empirical evaluation cassandra table parameter ran several benchmark internal testing stack different level concurrency different hit rate transaction table benchmark ensured actually performed disk operation evaluate performance done sstableloader ingest billion timestamp pair test stack stress tool consume almost free memory outside cassandra heap avoid operating system page cache also create fresh transactionservice benchmark run circumvent applicationlevel caching following benchmark run hit rate every timestamp queried corresponded existing transaction attempted run test various configuration optimisation discussed simply transaction algorithm transaction algorithm standard atlasdb table setting bloomfilterfpchance minindexinterval maxindexinterval unspecified chunklengthkb refers explicit configuration bloomfilterfpchance refers explicit configuration minindexinterval maxindexinterval ckn refers explicit configuration chunklengthkb concurrency metric bfii bfiick bfiick result may better visualised graph example concurrent reader next set benchmark run hit rate note rare practice practice miss try roll back transaction inserting entry transaction table concurrency metric bfii bfiick bfiick determined final choice setting bfiick brought transaction read performance mostly line transaction hit rate also notice seems regression unoptimised transaction optimisation probably useful seems performance hit lower percentile hit rate test though deem bad read miss make future read value modulo race condition hit meaning unlikely steady state cell loader background cell loading atlasdb load data multigetslice cassandra endpoint codeblock java map multigetslicerequired list key required columnparent columnparent required slicepredicate predicate required consistencylevel consistencylevelconsistencylevelone throw invalidrequestexception ire unavailableexception timedoutexception slicepredicate cassandra struct allows client specify column want read column loaded key presented notice method support multiple key one predicate thus atlas try perform get collection cell rowcolumn pair first group pair column parallel dispatch request cassandra row relevant example one cell atlas would send three request column key column key column key note practice request make range predicate cassandra cell dont include timestamps latest timestamp cell existed isnt something know priori multiget multislice model work well transaction transaction cell end distributed reasonably evenly among column case thus attempting determine whether atlas value committed perform many request parallel request end many resource cassandra connection pool also incur lot overhead term scheduling network want able batch call together added another endpoint thrift interface palantirs fork cassandra provides codeblock java struct keypredicate optional binary key optional slicepredicate predicate map multigetmultislicerequired list request required columnparent columnparent required consistencylevel consistencylevelconsistencylevelone throw invalidrequestexception ire unavailableexception timedoutexception implementing endpoint cassandra side difficult may seem little wasteful may require key predicate specified transaction would likely mostly distinct improved performance still faced significant regression relative cell loader determined cassandra worker pool loading value satisfy readcommand calling thread request also allowed participate thus creating large batch would turn likely detrimental read performance even cassandra node actually able handle higher concurrency safely selective batching thus settled compromise sending large singular request inundating cassandra smaller one unlike original cellloader make batch parameter configurable two parameter cross column load batch limit may combine request different column one call merged call exceed size single query load batch limit single request never larger size expect still partition request load cell column first thereafter given column number cell least cell column exclusively take one batch batch size greater otherwise cell may combined cell column batch size guarantee cell given column batch however guarantee column returned batch first last column batch term implementation simply maintain list cell eligible crosscolumn batching partition list contiguous group size way row key included twice may possible reduce amount data sent wire cassandra possibly internal read bandwidth solving underlying binpacking problem ensure rowkey occurs consider duplicate many key want load cell many column may worth considering future binpacking npcomplete algorithm like firstfit decreasing give good approximation implemented yet overhead constant factor many case transaction expect number cell per column small consider assuming uniform distribution even single transaction read value maximum batch size probably exceed example supposing one cell partitioned column number cell column follows suppose column cell column least cell column fewer cell loaded single request column cell cell loaded three parallel request remaining column fewer cell visit column lexicographical order first batch consisting cell column cell column second batch consisting cell column cell column though note request done parallel notice optimal end sending request cell column twice incurs network serialization overhead possible combine request column single batch size thereby removing overhead however discussed problem computationally difficult general benchmarking tested selective batching cell loader original algorithm fullbatching algorithm always batch cell regardless row column tested loader general atlasdb user workload row static column row static column workload specific transaction row dynamic column important would prefer separate codepath transaction current behaviour loading query row many different column regardless table also previously observed inefficient first ran benchmark single thread aforementioned workflow test dynamic column random unlikely overlap time reported millisecond row column metric cellloader full batching cellloader static static static static static static dynamic dynamic dynamic result may better visualised graph example response time notice row test cellloader performs marginally better cellloader probably able make rpcs instead recall full batching algorithm performs worst probably owing cassandra latency one requestor thread apart worker pool executing request row test cellloader performance similar expected underlying call cassandra cluster column single rpc full batching algorithm performs poorly however row dynamic column test cellloader performance poor may make many distinct rpcs owing different column key full batching algorithm still suffers one requestor thread cellloader able divide approximately parallel rpcs performs best overall also ran benchmark concurrent reader workflow time reported millisecond row column metric cellloader full batching cellloader static static static static static static dynamic dynamic dynamic magnitude full batching perform well cellloader also much lower possibly worker pool finite size even full batching algorithm case reader contributes one requesting thread although cellloader spin many requesting thread cassandra side cassandra cluster unable actually thread work concurrently live migration coordination service consequence write performance read performance data compression operational concern backup restore cassandra dependency safe installation considered timelock paxos mechanism transaction rowtickets algorithm multiget multislice exactly future work dbkvs transaction