volume management case disruption update decided rely statefulsets manage persistentvolumes adr remains valid range action limited supported statefulset controller responsible creating reusing pvc proposed deciders cloudonks team problem statement aim document capture scenario pvc get orphaned define reuse pvc mechanism must behave document deal reuse pvc spec cluster updated inline growandshrink update complex scenario deserves adr preamble dive different usecases scenario consideration lead disruption reminder constraint raised storage class disruption pod disappear person controller deletes unavoidable hardware system software error reason classified main category external involuntary disruption hardware failure instance deleted kernel panic runtime panic example containerd crash eviction supposed happen long qos class guaranteed external voluntary disruption node hosting pod drained non exhaustive example node upgraded repaired cluster scaling pod manually deleted someone error also sometimes reboot fix problem storage class constraint storage class provide capability come reusing volume instance google persistent disk attached single availability zone regional persistent disk replicate data zone region volume backed elasticlocal storage class reused node stage worth mentioning even scheduler predicate reschedule pod node volume reused attached preserve capacity needed reschedule pod instance pod local volume node run capacity pod recreated becomes impossible reuse volume capacity freed disruption occurs either volume considered recoverable considered unrecoverable unrecoverable volume unrecoverable state reached two situation administrator know volume cant recovered must abandoned elastic operator creates new pod attempt reuse volume pod still scheduled given amount time recovering strategy possible strategy time try recover data pvc recoverable required elastic operator must delete pvc may hold copy data recoverable required state volume must recovered get missing data back online recoverable optional recoverable optional state missing data available others node instance node local volume data replicated node mandatory elastic operator wait forever best effort scenario choose wait node back online paying cost replication node mean scenario find way determine time operator wait decided new pod must created may hard find exact timeout must user configurable sane default value set based criterion like example shard size number replica still available todo check controller access pvc node cant watch node cant watch claim driver solution must able handle following case cluster suffering external involuntary disruption volume cannot recovered scenario must consider data permanently lost example local storage destroyed give way user instruct elastic operator immediately move volume recovering strategy volume recoverable required state user able forcibly reuse pvc even replica available cluster suffering external involuntary voluntary disruption volume eventually recovered elastic operator create new pod according affinity scheduler hopefully find new node data available take much time schedule pod volume moved one two recoverable state admin want plan voluntary disruption volume cannot recovered scenario administrator want definitively evacuate node data available anymore example server local storage definitively removed cluster usually done two step cordon node evict delete pod considered add finalizer pvc pvc pod deleted immediately finalizer set scheduler add finalizer create new pod migrate data delete pod pod deleted pvc deleted handle pvc deletion annotation tombstone set pvc annotation annotation elasticsearchkselasticcodelete set pvc following value graceful migrate data delete node pvc force discard data operator try reuse pvc pvc deleted elastic operator add kubectl plugin add domain specific command kubectl extended new subcommands httpskubernetesiodocstasksextendkubectlkubectlplugins example bash kubectl elastic migrate elasticsearchsampleesqlvprlqnnk default migrate data delete pod pvc kubectl elastic delete elasticsearchsampleesqlvprlqnnk default delete pod pvc try handle pod eviction pvc deletion webhook pro con pro look like simple approach kubectl delete pvcxxxxx migrate data delete pod con volume cant recovered user remove finalizer administrator uncordon node delete manually also known error prone pvc want drain pro elastic operator figure easily must try migrate data abandon volume cant recovered con admin uncordon node annotate pvc manually still error prone want drain admin must remember annotation pro provides meaningful interface con stable even plugins introduced alpha feature release reworked end user must install plugin admins still evict node manually node drained pro integrate smoothly cordon drain scenario con doesnt seem possible handle node eviction confirmed setting webhook requires privilege cluster level even possible webhooks safely migrate data eviction occurs link strimzi deleting kafka node manually kubernetes isbeingused function pvc deleted scheduled pod including unknown state