retry longrunning lock via blockingtimeoutexception note applies lock taken via legacy lock service since implemented asynchronous lock service transactional lock blocking timeouts relevant implementation atlasdb client timelock server interacting way causing timelock server experience large thread buildup running http manifested issue issue eventually root caused longrunning lock request excess jetty idle timeout server would close relevant http stream free resource consumed request eventually server thread leader node would busy handling lock request timed leader would thus respond ping resulting node proposing leadership problematic leader election cause lock lost thus inflight transaction fail concrete trace follows client acquires lock client block acquiring lock request idle timeout client connection run close http stream client retries block acquiring lock request client release request granted client longer listening minute idle timeout client expire four time client retries request lock granted request reaped request granted client listening since retry every second default service one request every minute second accumulate backlog request also observe setting idle timeout minute second solve problem though mitigate since multiple client may blocking acquiring lock time limiting lock request decided solution problem prevent pattern satisfying one following prevent idle timeout reasonably occurring ensure resource freed idle timeout trigger introduced time limit lock request allowed block call blocking timeout set lower jetty idle timeout margin thus achieving requirement even lose race still free resource shortly thus achieving requirement event lock request block longer blocking timeout interrupt requesting thread notify client sending serializableerror wrap blockingtimeoutexception upon receiving client check nature error occurring server event blockingtimeoutexception retry node reset counter number time weve failed zero timeout mechanism implemented guava timelimiter server time thus considered authoritative believe reasonable time limiting jetty handling idle timeouts thus relatively resilient clock drift relative client node timelock cluster considered significantly increase jetty idle timeout could configured recommended idle timeout timelock substantially longer expect lock request reasonably block day solution advantageous simple however current default second already longer would expect lock request block furthermore event client link failure would possible resource would unproductively allocated associated connection longer period time would also introduce dependency idle timeout http clientside would also increased account current default second convert lock service nonblocking api could changed lock api lock request return immediately regardless whether lock asked available lock asked available yet server would return token indicating request satisfied client later time poll server token ask request satisfied alternatively could investigate http websocket server push solution likely best longterm approach though involve significant change api lock service would prefer make time implement connection keepalives heartbeat close connection byte sent received idle timeout thus reset timeout sending heartbeat message client server vice versa frequency higher idle timeout would probably prefer live server since idle timeout configured serverside solution seems reasonable though appear readily supported jetty send lastgasp message lock service free resource stream close idea considered jetty free resource lock service closing http stream solution appears cleanest free resourcesbased solution including one chose implement unfortunately feature requested jetty time writing implemented yet see jetty issue client truncate individual request idle timeout server return blockingtimeoutexceptions longrunning request would client trim request appropriate length case blockindefinitely indefinitely send request suitable length example default idle timeout second client wishing block second could send lock request block second upon failure submit another request block second suitably accounting network overhead solution relatively similar implemented though requires client know aforementioned appropriate length idle timeout inappropriate timeout configured server side implement magic http code header ask client retry serializing exception serializableerror could defining specific http code andor custom header indicate blocking timeout occurred andor client retry practice nginx indicates error client ssl certificate solution would simpler serializing exception existing atlasdberrordecoder already switched code returned http response however prefer introduce custom code feasible since client unlikely understand code similar argument though perhaps weaker applies header well consequence blockforatmost behaviour implementation lock request block idle timeout blockindefinitely behave correctly however lock request block idle timeout incorrect client acquires lock repeatedly refreshes client block want block second client receives blockingtimeoutexception retries block second positive integer client receives blockingtimeoutexception retries actually behave like blockindefinitely request unless client retries correctly suitably reducing blocking duration lock request retrying implement lock service client modify lock request retrying though subject separate implementation separate adr subtlety involving source time one treat authoritative also certain authoritative source time strictly necessary exception serialization exception serialization changed previously contacting node leader would send empty response otherwise would throw feignexception underlying cause stack trace string serialize notcurrentleaderexception new blockingtimeoutexception manner compatible palantir httpremoting library throw atlasdbremoteexception including serialized information said exception also mean receiving necessarily mean one talking leader one interpret message body http response see caused said believe positive change service may unavailable reason leader fairness starvation lock request previously fair thread block acquiring lockserversync given lock thread normal circumstance barring exception interruption leader election would acquire lock still true thread synchronization level longer necessarily hold application layer since possible would timeout interrupted lock would become available would grab consequence starvation becomes possible believe acceptable lock request remain fair long none block longer idle timeout blocking longer idle timeout considered unexpected furthermore previous behaviour http lock request would never succeed far client concerned owing retry problem flagged issue client http wish avoid behaviour feature configurable configuration timelock additional configuration parameter though nonbreaking provide sensible default