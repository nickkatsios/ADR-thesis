handling total capacity requirement local volume proposed deciders operator team problem statement current dynamic provisioner local volume handle maximum storage available node mean pod get assigned node wed create persistentvolume even though physical disk might full already way currently work following single clusterwide provisioner dynamically creates persistentvolume resource matching nonbound persistentvolumeclaim resource persistentvolumes bound particular node yet pod scheduled node driver node responsible creating actual volume disk pod scheduled node also update persistentvolume nodeaffinity match node reuse step assign node yet pod scheduled node kubernetes scheduler step pod might assigned node free space way make sure pod scheduling take available disk space consideration driver pod requesting elasticlocal pvc would fit node remaining disk space scheduled node solution require particular human operator intervention performance impact kubernetes pod scheduling acceptable node affinity failure domain mapping still possible considered consider disk space multiplier ram capacity one way circumvent problem consider kubernetes node composed amount ram capacity amount disk space ration expressed ramtodisk storage multiplier scenario node kubernetes cluster respect approach ramtodisk multiplier consider node run ram hence unschedulable new pod run disk requires kubernetes cluster administrator human aware multiplier size kubernetes node accordingly user operator aware multiplier size elasticsearch node accordingly either manually map ram disk number provide ram let disk computation elastic operator one entire disk space bind smaller one node volume provisioner instead single clusterwide provisioner one provisioner per node responsible provisioning persistentvolume corresponding node let call node volume provisioner name tbd global provisioner exist anymore startup node volume provisioner inspects available disk space total creates single persistentvolume resource apiserver node affinity set node running persistentvolume cover entire available disk space persistentvolume bound persistentvolumeclaim yet far quite similar whats done static local volume provisioner except probably want consider single entire lvm disk space spanning multiple disk instead creating one per disk pvcpv binding pvc storage class elasticlocal created kubernetes persistentvolume controller automatically bind persistentvolume created node volume provisioner pvc pvc created another node storage capacity taken consideration spec specifies capacity bound pvc claiming however pvc still bound effectively wasting disk space avoid wasting disk space scenario soon bound pvc node volume provisioner get notified watching pvs created retrieving matching pvc notice pvc request available result update persistentvolume spec match stay bound pvc even though capacity changed actual volume corresponding created driver running node done current implementation left available node node volume provisioner creates new persistentvolume capacity bound pvc kubernetes persistentvolume controller get deleted node volume provisioner reclaims disk space freed updating capacity example pod deleted updated summarize node provisioner make sure always one given time waiting bound pvc kubernetes cover entire available disk space disk space available locally reflected controlled provisioner get bound pvc node provisioner resizes match pvc capacity creates new cover remaining disk space deployment node volume provisioner deployed daemonset desired node rbac permission get list persistentvolumeclaims get list create update delete persistentvolumes running either container new pod container pod driver alongside driver process binary limitation scheduling concurrency due fact node volume provisioner always single waiting bound given time representing total available disk space time kubernetes cannot assign multiple pvc underlying node first pvc bound second pvc bound new representing remaining disk space created provisioner scenario deploy pod deployment instance pvc chance pod assigned different node whereas could assigned node creation managed node volume provisioner time necessary create new covering remaining free space estimated rather small though notified pvpvc binding provisioner directly issue new thats order millisecond bestcase scenario one way mitigate could maintain one waiting bound example splitting space time space failure domain approach affect failure domain way failure domain expressed label pod relying waitforfirstconsumer volume binding mode storage class kubernetes pod scheduled node pvc get bound priority therefore given pod failure domain criterion pvcpv mapping picked security consideration provisioner pod node must readwriteupdatedelete access persistentvolume resource already case current localvolume driver mean driver required api access interfere persistentvolumes cluster storage class local containerescaping exploit could technically get access perform crud operation pvs cluster possibly get access pvs nonlocal risk kubernetes team decides forbid update persistentvolumes current design work expected since rely updating pvs proper reduced disk capacity bound possibility could happen seems rather low considering recent pvcpv expansion feature also requires updating pvc outcome todo seems way better imho pro con pro change localvolume provisioner design bother remaining disk space long kubernetes node sized accordingly con pvs node left reuse running pod taken consideration might end situation node full ram left alive reuse cannot anymore would free ram flexible easily map lot configuration apply well outside deploying elasticsearch cluster require knowledge kubernetes administrator elastic operator user pro give flexibility assign disk capacity pod additional operation required user administrator point view node responsible pvs sound like scalable design long request apiserver correctly scale con one pvc mapped node given time impact andor slow kubernetes scheduling pod claiming storage class updating pvs bound possible sound bit like hack link elastic dynamic provisioner local volume kubernetes static local volume provisioner local volume initial issue