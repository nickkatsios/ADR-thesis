eks whats proposed amazon eks running main cluster host moj service team application replaces usage kops reasoning benefit eks managed control plane master node reducing operational overhead compared kops scaling control plane node reduces risk api availability sudden increase api traffic managed node reducing operational overhead kubernetes upgrade smoother kops rolling upgrade problematic upgrade kops caused work around networking issue team see kops upgrade particularly stressful risk register open door elb ingres managed seen preferable selfmanaged nginx requires upgrade scaling etc avoid security challenge managing token exported kops export kubeconfig already run manager cluster eks gained lot insight experience eks configuration auth chosen oidc auth broker github identity provider developer service team auth github continues common sso amongst good tiein jml process see adr github identity provider auth useful broker couple important rule run login time ensures user ministryofjustice github organization staff get kubeconfig login website like grafana insert user github team oidc token claim rbac authorize user correct namespaces future azure sso growing moj there case switching adopted amongst user iam auth benefit immediately revoking access maybe could federated login github would give temporary kubecfg sync github team info iam completed auth credential issuer chosen cloud platform kuberos weve long kuberos issuing kubecfg credential user original version kuberos unmaintained pretty simple updated dependency httpsgithubcomministryofjusticecloudplatformkuberos maintain fork considered gangway similar kuberos release year kubelogin team would distribute client secret user seems odd trouble securely sharing secret overcome perceived difficulty issuing kubecfg credential requires user install software rather serverside centrally kubehook compatible eks doesnt support web hook authn dex doesnt web frontend issuing cred oidc broker completed authorization chosen existing rbac configuration well continue existing rbac configuration previous cluster completed node management chosen managed node group future experimenting fargate selfmanaged node managed node group automates various aspect node lifecycle including creating auto scaling group registration node kubernetes recycling node fargate node fully automated node least manage benefit isolation pod automatic scaling doesnt support daemonsets aim take advantage much automation possible minimize team operational overhead risk initially well managed node group looking fargate workload completed future fargate consideration pod limit quota limit fargate pod per region per aws account could issue considering currently run pod request aws raise limit currently sure scope multicluster stage separation load different aws account settle issue daemonset functionality replacement fluentbit currently log shipping elasticsearch aws provides managed version fluent bit fargate configured ship log elasticsearch prometheusnodeexporter currently export node metric prometheus fargate node managed aws therefore hidden however collect useful metric pod running fargate scraping cadvisor including cpu memory disk network support prometheus run still managed node group likely workload consider people check deployment investigated ingres cant nginx load balancer front investigated dont fargate take advantage spot instance reduced cost however fargate priority main driver engineer time cost node group chosen main node group majority workload prometheus node group well minimize number node group minimize operation overhead prometheus continue design previous cluster separate node group larger node dedicated prometheus pod prometheus resource hungry memory disk iop two large node single availability zone access volume store metric data node group may valueable spot fargate node explore future completed prometheus node group node instance type chosen rxlarge rxlarge raxlarge main node group chosen prefix configured image choosing instance type past experience memory tends limiting factor gone memory optimized range instance type latest cost efficient range without going arm processor see consideration however rxlarge current cluster well stick move raxlarge migration choice aws cni networking new cluster initially added constraint number pod per node due limitation number enis address however aws released prefix giving higher pod density per node enables avoid limit support added aws cni requires instance nitrobased range primary choice rxlarge vcpus memory fallback cloud provider run rxlarge vcpus memory raxlarge vcpus memory future might consider arm processor range wed consider added complexity crosscompiled container image chosen rxlarge rxlarge prometheus node group existing cluster rxlarge well continue add fallback rxlarge memory optimized range core rxlarge memory optimized range core rxlarge place main node group temporarily high number instance prefix backlog pod networking cni chosen aws vpc networking cni link httpsdocsawsamazoncomekslatestuserguidepodnetworkinghtml awss cni pod networking ipam cni routing pod given eni address underlying vpc rather overlay network pod traffic routed node native vpc advantage awss cni default eks native aws fully supported aws low management overhead offer good network performance concern awss cni would address every pod limit per node depending instance type number enis support calculation node instance type show change instance type cost cluster increase acceptable likely engineering cost maintaining supporting full calico networking custom node image alternative considered calico networking advantage needing address per pod associated instance limit open source however wouldnt support cloud provider networking issue maintain customized image calico installed likely change eks time frequently cause breakage networking setup installation requires recycling node good fit declarative config completed node image chosen amazon eks optimized amis link httpsdocsawsamazoncomekslatestuserguideeksoptimizedamihtml eks amis default therefore lowest maintenance cant see reason needed change launch config would give flexibility completed cluster scaling chosen manual cluster scaling future consideration autoscaling initially continue manually scaling updown node manually kops cluster proven fine since workload change slowly alert setup warn capacity low prompt team scale cluster cluster autoscaling considered soon though embrace one cloud key advantage scale updown swiftly good efficiency example usage hosted site much lower night aim get workload scale automatically cluster follow suit addition tenant nonproduction environment run night consideration autoscaler maintain spare capacity workload scale dont wait node startup take minute may require tuning tenant encouraged autoscale pod effectively horizontal pod autoscaler capitalize cluster autoscaling scaling nonprod namespaces agreement service team manual scaling place autoscaler still desired network policy enforcement chosen calico network policy enforcement policy full blown calico networking link httpsdocsprojectcalicoorggettingstartedkubernetesmanagedpubliccloudeksinstallekswithamazonvpcnetworking implement kubernetes network policy api needed isolate tenant recommended eks team previous live cluster familiar cps namespaces default networkpolicy allows incoming request pod namespace ingresscontroller namespace tenant namespaces alternative aws security group creating securitygrouppolicy resource namespace would attach security group allowing specify inbound outbound network traffic advantage similar networkpolicy security group specify aws resource like rds instance would network restriction addition auth restriction cps rds instance currently network accessible across vpc explore future defence depth completed aws calico network policy enforcement installed chosen calico typha install typha performance benefit cache request made calico felix api control plane logging chosen enable cluster logging link httpsdocsawsamazoncomekslatestuserguidecontrolplanelogshtml control plane log enabled log api authentication controller manager scheduler log cloudwatch maybe export elsewhere discussion covered adr logging completed node terminal access chosen aws system manager session manager bastion node link httpsdocsawsamazoncomprescriptiveguidancelatestpatternsinstallssmagentonamazoneksworkernodesbyusingkubernetesdaemonsethtml httpsgithubcomawscontainersroadmapissues aws system manager session manager benefit easy install daemonset auth via team member aws cred tied jml process access removed immediately leave team norm terminal command logged useful audit purpose eks best practice take advantage system manager feature future including diagnostic compliance monitoring note requires permission hostnetwork true privileged true may psp node failing boot join cluster properly live likely pod want characterize node node managed traditional method node access would ssh via bastion involves shared ssh key shared credential acceptable security practice completed implementation ticket httpsgithubcomministryofjusticecloudplatformissues runbook usage httpsrunbookscloudplatformservicejusticegovukeksnodeterminalaccesshtml podsecuritypolicies chosen default eks psp eksprivileged awsnode pod chosen apply cps existing privileged psp kubesystem namespace restricted psp rest eks come psp eksprivileged allows anything however security best practice limit ensure pod arent allowed run privileged root eks best practice say recommend scope binding privileged pod service account within particular namespace kubesystem limiting access namespace practice found awsnode pod needed already privileged restricted psps apply existing cluster completed psp deprecation psps deprecated april removed due april psps considered difficult way specify policy clear replace opa gatekeeper possibility look move psps whatever standard selected kep coming month outstanding backlog apps access aws role chosen iam role service account irsa instead kiam completed kubeiam usage migrated irsa kubeiam running new cluster chosen block access instance metadata node role completed access instance metadata blocked irsa benefit irsa kiam kubeiam kiamkubeiam require running managing daemonset container kiamkubeiam require powerful aws credential allow box assume role appropriate configuration kiamkubeiam aim provide container specific role however security concern approach kubeiam remember set defaultrole annotation set pod node boot may short window kiamkubeiam start protection instance metadata comparison irsa injects token pod avoiding concern kubeiamkiam attacker able get root node could access credential therefore aws role comparison irsa breach might bring access aws role associated service role blocking access instance metadata eks node various aws permission provided node role avoid pod permission block access implementation detail provided httpsawsgithubioawseksbestpracticessecuritydocsiamrestrictaccesstotheinstanceprofileassignedtotheworkernode achieved globalnetworkpolicy