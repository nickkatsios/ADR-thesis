automatic data scraping system deployed heroku organized follows worker dyno process job scraping task queue background scraping task populated queue via oneoff dynos scheduled run periodically via heroku scheduler example oneoff script pull data climatescapes airtable schedule initial scraping task newly added orgs background worker task queue driven heroku documentation justifies approach scalable reliable worker fetch scraping task background scraping access twitter api put result postgres database alternative pull approach scraping task populated scheduled oneoff dynos push approach considered backend maintains web interface separate dyno climatescape website via netlify function zapier pullbased approach chosen following advantage dependency netlify function zapier avoided reducing number concept developer learn environment manage hand even push approach oneoff script heroku could needed anyway schedule periodic rescraping information organization backend doesnt expose post put interface worry protection authentication form backend web interface might eventually added monitoring number scraping task queue interface could probably readonly authentication might required background worker done see also preceding message thread pushbased approach decided pullbased instead pushbased approach discussion