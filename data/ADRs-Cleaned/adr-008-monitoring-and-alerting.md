monitoring alerting amended updated reflect team paused email alert api acquired undesirable reputation application produce high volume unactionable alert frequent source govuk incident monitoring alerting approach longer appears sufficient application behaviour application volume work changed past year previous year application would reach peak sending million email day whereas sending million email day elevated volume work increased system latency impacted alert contributed incident approach monitoring application wider email alert system two fold one approach monitor email alert apis internal state see completing task within particular time frame whether high quantity work approach endtoend check verifies gmail account received email within time frame approach flawed monitoring high quantity work whether work completed time frame left email alert api vulnerable producing alert application busy yet fully operational alert unactionable support engineer alert resolve work completed little engineer speed process example application accumulated email send raise critical alert even sending fastest rate notify would allow alert therefore waste time support engineer instance occurring represent problem doesnt require intervention increase risk engineer wouldnt act alert actually represent application broken since alert cant relied upon reliably suggest application broken arent outofhours alert therefore even reflect genuine problem engineer would contacted engineer instead learns problem outofhours due type monitoring approach endtoend check alert content published travel advice publisher arrive particular gmail inbox within specified time period monitoring inbox limited set email flaw email alert api break outside office hour engineer would notified unless travel advice content published alert also monitor beyond boundary email alert system considering whether govuk publishing notify gmail working leading alert representing problem irrelevant system outside control govuk engineer gmail identifying email spam finally nature time based alert also vulnerable system busy sufficiently high volume travel advice publishing guarantee alert trigger reviewing monitoring approach helped understand reputation unactionable alert reviewing govuk incident email alert api helped identify pattern incident reflected ambiguity whether system broken felt room improvement decided system raise critical alert email alert system broken majority alert contact engineer outofhours unpacking consider email alert system comprised application email alert api email alert frontend email alert service excludes monitoring whether govuk publishing notify working independently monitored reflects condition weve formed system raise critical alert unless highly confident broken whereas warning appropriate situation decided email alert system broken represents failing lead user noticeable issue unless engineer intervenes since alert represent broken system requiring intervention suitable contact engineer outofhours otherwise user experiencing issue decided reapproach alerting mindset look replace alert vulnerable reflecting busy extend beyond boundary system proposed team working piece work paused december unclear whether future team formed complete work consequence changing approaching monitoring alerting across email alert system application order meet new alerting criterion monitoring email alert api sidekiq worker configure critical alert contact engineer outofhours individual sidekiq job fail multiple time following worker dailydigestinitiatorworker digestemailgenerationworker processcontentchangeworker processmessageworker sendemailworker weeklydigestinitiatorworker worker database store number time attempted perform action alert raised quantity failed attempt reach threshold suggestion based evidence consistent failure expect threshold worker even though sendemailworker speaks external service notify may likely fail failure rate low make consecutive failure unlikely ensure job mentioned worker actually run configure recoverlostjobsworker identify run attempt protects lost work problem sidekiq crashing expect sendemailworker digestemailgenerationworker job run quickly duration second know high volume run regularly reach volume concerned scenario consistently fail take long time engineer informed could occur could take long time sufficient retries occur alert raised retry placed back queue reflect concern contact engineer either worker completed one successful run within time period despite work existing suggested time period minute sidekiq retry cadence recovery window job updated ensure job always attempted within time period ensure dont alert work isnt attempted contact engineer outofhours determine digestrun record created within reasonable time frame suggestion hour creating trivial essential operation relies schedule lack record suggests either schedule broken code running scheduled time failed also monitor run recoverlostjobsworker metricscollectionworker succeeding least within time frame suggestion hour job continue running even system busy lack running indicates longer confidence system monitored able recover thus alert created contact engineer outofhours succeed time frame sidekiq worker dont affect user noticeable aspect email alert api emaildeletionworker wont raise critical alert failing instead raise warning sidekiq worker exhaust retries placed dead set approach chosen pragmatic technique monitor multiple worker without adding alert individual worker worker retry cadence considered alert reasonable time frame retries exhausted sidekiq default day long modify latency check email queue treating transactional email different latency check email expectation prompt delivery lack block user progressing task raise warning min critical engineer contacted outofhours latency reach min latency check made significantly sensitive latency change time warning occurs measure multiple hour compared minute intention serve alert support engineer system exceptionally busy may unhealthy alert reach critical hit hour point would indicate consistently creating email faster application send may mean system perpetually backlogged web application decided mechanism alert people outofhours email alert api web application appears broken noting currently engineer unlikely contacted event whole email alert api application becoming unavailable unless travel advice published noted wasnt particularly consistent approach across govuk apps raising outofhours alert single instance fails healthchecks felt would good take simple approach avoid surprising engineer decided appropriate place alert would application load balancer healthy instance application represents instance web application healthy request succeed alert removed remove following alert become superseded content change unprocessed hour message unprocessed hour incomplete digestruns hour monitoring email alert service email alert service listens publishing api event communicates email alert api currently alert monitored inhours system fails visibility outofhours problem intend improve consolidate existing alert contact engineer outofhours pattern failure processing event monitoring email alert frontend email alert frontend act public interface subscriber interaction email alert system web application monitored similar way email alert api web application considering whether load balancer healthy instance removal email alert monitoring email alert monitoring aforementioned endtoend check monitor whether travel advice medical safety content change result email received gmail inbox intend retire check believe new approach monitoring email alert system cover aspect check within scope email alert system broadly email alert monitoring verifies content published outofscope email alert system publishing api concern whether publishing application successfully communicated email alert api outofscope email alert system publishing application concern whether content change converted email inscope email alert system monitored identifying processcontentchangeworker failure whether email sent govukemailcheckdigitalcabinetofficegovuk inscope email alert system monitored identifying sendemailworker failure whether notify actually sends email outofscope email alert system notify concern whether gmail successfully receives email outofscope email alert system google concern completed configuring alert cover inscope scenario remove email alert monitoring govuk stack