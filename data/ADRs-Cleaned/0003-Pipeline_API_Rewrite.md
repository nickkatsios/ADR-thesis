pipeline api rewrite proposed alex black discussed sham paul adam konduit serving concept step represents one component machine learning deployment example modelstep contains neural network model pythonstep run arbitrary code executed one step chained together user via jsonyaml configuration api create konduit serving deployment execution output one step input next step output final step returned user perhaps post processing proposal proposes significant change format data passed pipeline step well proposing change userfacing api rest endpoint result change background number key class pipelinestep interface configuration step much jsonyaml user write deployment related configuring pipelinesteps pipelinesteprunner interface actually handle execution pipelinesteprunner instantiated pipelinestep inferenceexecutioner inferenceexecutionerfactory machine learning neural network model modelsteps pipelinesteprunner inferenceexecutionersteprunner inferenceexecutionersteprunner creates inferenceexecutionerfactory one implementation samediff tensorflow dlj pmml onnx kera inferenceexecutionerfactory creates initializedinferenceexecutionerconfig inferenceexecutioner internally one inferenceexecutioner implementation dlj samediff tensorflow etc inferenceexecutioner generic type outputtype executeinputtype input indarray inout dlj onnx listmapfieldname object pmml summary modelstep inferenceexecutionersteprunner xinferenceexecutionerfactory xinferenceexecutioner present highestlevel internal api pipelinestep schema pipelinesteprunner transform method based datavec follows dataframe type api ultimately everything converted datavec record end pipeline step example case neural network model record indarray model indarray record happens modelstep inferenceexecutors note datavecs record object simply listwritable internally writable object value datavec support example doublewritable ndarraywritable text imagewritable etc link also concept output data format prediction type schema type schema defined following enumsclasses inputdataformat numpy json ndj image arrow outputdataformat numpy json ndj arrow outputpredictiontype classification yolo ssd rcnn raw regression optional output adapter applies output schematype string integer long double float categorical time byte boolean ndarray image note distinct datavec columntype value minus image schema datavec schema object note inputdataformat outputpreductiontype also impact rest api endpoint format predictiontypeinputdataformat furthermore simply internal detail user understand order createconfigure pipeline example user set inputdataformat outputdataformat serving configuration link though oddly doc show setting serving input data format java servingconfig setting output config present user set inputdataformat outputdataformat value pythonsteps link user optionally specify outputpredictiontype server configuration however user set yaml config according doc servingconfig outputpredictiontype configuration calling endpoint manually via client user understand predictiontype inputdataformat issue current design multiple problem current design api verbosity pipelinestep interface method related input output typesschemas confusing type weve got datavec schema schematype inputdataformat outputdataformat outputpredictiontype thing think developer know aboutsetconfigure user pipelinesteprunner defines separate transform method object writable object writable record record usage objectobject highly ambiguous error prone note largely equivalent practice record listwritable internally basesteprunnertransformobject transformobject unwrapconvert call transformrecord internally anyway note none suitable sequence without ugly hack like embedding sequence ndarray note even added datavec sequencerecordlistlistwritable support sequence still way mix sequence nonsequence data without ugly hack sequence different length datavecbased api significant blocker adding programming language happens want pipeline step based swift javascript there datavec api communicating language vms pipeline split processesvms microservice datavecbased api add significant serialization overhead support dynamic schema optional return value example classification optionally return probability based user request writing custom java pipeline step requires user familiar datavecs apis well conversion tofrom indarray etc inputdataformat outputdataformat contain languagespecific feature numpy ndj inputdataformat outputdataformat restrictive example there niceeasy way return string arbitrary byte image segmentation model least without hack like wrapping ndarray multioutput network really supported example multitask classification regression model single predictiontype setting doesnt work invalid combination configuration easy produce yolo prediction type json arrow outputdataformat mean similarly python client able handle outputdataformat ndj link way return metadata user short issue usability performance maintainability support case current design requirement new api data format suppose redesign api handling data consequence throughout codebase change impacting pipeline pipelinestep apis yamljson configuration serving client step rest endpoint client api requirement extensible language principle allow interop language might one day deploy javascript matlab etc support efficient serializationdeserialization ideally zero copy possible suitable microservices type split pipeline step step run different processescontainers efficient monolithic deployment paying serializationdeserialization memory cost everything running jvm like suitable communication approach deployment scenario grpc mqtt iot kafka etc flexible doesnt restrict case explicitly builtin library support optional value good usability easier write maintain pipeline step json serializable nice metadata support optionally present absent support batch one recordexample object binary serializable suitable longterm storage example recording value compliance manual labelling purpose longterm storage schema evolution backward compatibility load old serialized value new library version efficient binary storage readable multiple language would also nice example write java read python proposal starting point remove datavec entirely pipelinestep pipelinesteprunner apis remove concept predefined fixed schema pipelinestep pipelinesteprunner regard current pipeline definition record record replaced something else proposed call data class thus pipeline defined data data operation two aspect consider api user developer interact writing pipeline step storage actual data structure store serialize data instance discussing number aspect apicodebase discussed api data class api proposed mapdictionarylike semantics data proposed maplike hold set keyvalue pair key string type value one predefined datatypes see data instance would hold one example batch represented array list data data listdata allows dynamic schema different schema different example batch dynamic schema example image classification one example batch inference request may request predicted class schema class string whereas another example may request class probability schema class string probability ndarray also allows optionalmissing input value data similar dataframe type design different column map entry one set predefined data type however unlike dataframe allow multiple example different schema per examplerecord value proposed one following type value type ndarray string byte image double integer int boolean collection type data nested data instance allowed possible given limitation chosen storagedata structure arraylist including multidimensional listsarrays additionally data metadata support datatypes standard type term implementation metadata nested data instance dedicated key perhaps metadata something note data type easily converted json binary format also ndarray image multiple possible json representation raw text multidimensional json array base byte ndarray ndarray base image file byte jgp png etc image client could easily specify ndarray image encoding type want request via client config input data metadata proposed data api java tojson string key list keyint string typestring datatype enum listtypestring datatype enum type list entry getters getarraystring ndarray getstringstring string getliststring datatype list getdatastring data etc default getters get present provided default value getarraystring ndarray getstringstring string getbooleanstring boolean etc put method format column name value putstring string putstring indarray putstring byte etc metadata method hasmetadata boolean getmetadata data setmetadatadata void serialization savefile void writeoutputstream void asbytes byte static method fromjsonstring data fromfilefile data fromstreaminputstream data frombytesinputstream data singletonstring object data builder databuilder data databuilderputmyvalue vbuild frommapmap data note dynamically typed language like python may simply single getstring setstring object method instead overload example custom pipelinesteprunner api implementation one method replace pipelinesteprunnertransform method java public data transformdata data indarray arr datagetarrayfeatures indarray modeloutput mycustommodeloutputarr return datasingletonoutput modeloutput example custom samediff model optional input value optional return value attention model java public data transformdata data boolean withattnweights datagetbooleanreturnweights false user requested attention weight array output mapstringindarray new hashmap phputin datagetarrayfeatures phputmask datagetarraymask string output withattnweights new stringoutput attention new stringoutput mapstringindarray map sdoutputph output return datafrommapmap storageserialization data structure data api interface multiple implementation underlying data structuresstorage java main storage format monolithic deployment scenario running one jvm interprocess communication like simple map type structure avoids paying unnecessary serializationdeserialization overhead interprocess communication persistent storage format flatbuffers protobuf advantage including multilanguage support efficient creationserialization efficient space utilization support zerocopy access array data course apis language user friendly already noted well api top underlying storage issue summary map based storage within single jvm case current konduit serving monolithic deployment flatbuffers protobuf data structure serialization ipc required enable conversion two internally ifwhen required far appears either flatbuffers protobuf fine rest endpoint mqtt endpoint grpc endpoint see protobuf flatbuffers serialization longterm storage support schema evolution schema validation present konduit serving implement degree schema validation pipeline define schema inputoutput type checked however runtime schema validation isnt useful might first appear something wrong schema validation exception runtime input data match schema step without schema validation exception runtime exception step input data key found input data consequently proposed konduit serving allow dynamic schema pipeline step may input return anything including different data key type different example individual pipeline step responsible interpreting format input konduit serving runtime schema validation beyond step check get required input post processing output configuration currently outputpredictiontype classification yolo ssd rcnn raw regression outputprediction post processing raw default setting post processing currently predictiontypeinputdataformat endpoint example get output localhostclassificationnumpy endpoint pas numpy data get processed classification data note current classifieroutputadapter return information default set classifier label maximum predicted class index integer full set probability double proposal would similar postprocessing mechanism different design consider example image classification image want return one depending user request predicted class probability array top class probability list class label following proposed outputpredictiontype removed entirely api predictiontypeinputdataformat endpoint removed entirely api keep existing output adapter refactored pipeline step additional configuration add support postprocessing subset output especially useful multitask multioutput network add support configuring default behaviour classification predicted class probability default add ability pipeline step get initial input metadata allow user request output format want pipeline step alternative would existing output adapter different type class however pipeline step may simpler user one type thing learn plus flexible example classification output adapter transform method java public data transformdata data indarray output datagetarrayoutput data inputmetadata inputmetadata method defined basepipelinesteprunner everything extends basepipelinesteprunner boolean retprobabilities inputmetadatagetbooleanreturnprobabilities configisreturnprobabilities boolean retclasses inputmetadatagetbooleanreturnclasses configisreturnclasses data ret datacreate note data interface hence cant new data ifretprobabilities retputprobabilities output ifretclasses retputclasses getclasses return ret feature note input metadata metadata come original user request available pipeline step matter many step precede deployment split separate microservicesprocesses etc default configuration step set user applicable user override configuration step directly client internally via metadata setting allows optional return value configured client impact endpoint designproposal would longer type specific endpoint one predict endpoint binary applicationoctetstream mime type inout protobufflatbuffers encoded data object would main method builtin client handle conversion tofrom data internally json applicationjson mime type inout json encoded data object secondary method user interacting konduit serving manually without one client json reason current predictiontypeinputdataformat endpoint would removed refactor model execution noted earlier current machine learning model class heirarchy complex konduit serving modelstep inferenceexecutionersteprunner xinferenceexecutionerfactory xinferenceexecutioner proposed flatten heirarchy simply level modelstep set xsteprunner class one dlj samediff tensorflow onnx etc based inferenceexecutioner utility data indarray mapstringindarray etc conversion normally change magnitude done separate given scale api change suggest time dataapi change reason simple dont want waste time getting existing heirarchy modelstep inferenceexecutionersteprunner etc working new data api remove newly refactored code right away consequence advantage considerably simplified api pipeline step fewer method complication schema datatypes etc greater flexibility extensibility performance etc support sequence possible present combination sequence nonsequence data easier development maintenance new pipeline step konduit developer user make future enhancement easier including running pipeline step different processescontainershosts nonmonolithic deployment inputoutput recording proposal satisfy requirement mentioned earlier extensible language via probufflatbuffers encoding suitable microservices type split pipeline step support efficient serializationdeserialization ideally zero copy possible efficient monolithic deployment via java mapbased data implementation without serialization cost suitable communication approach deployment scenario grpc mqtt iot kafka etc via json binary encoding flexible doesnt restrict case explicitly builtin library data flexible enough support type returned value support batch one recordexample object partial via data support optional value good usability yes imo easier write maintain pipeline step json serializable ideally yaml serializable also metadata support optionally present absent inputrequests output binary serializable suitable longterm storage via flatbuffers protobuf tbd disadvantage schema mismatch determined runtime one step output array ndarray next step expects feature double feature double however worse current design schema check trigger runtime anyway potential mismatch pipeline step mapped manually user example preprocessing step produce array name mask model expects array name inputarray maskarray probably solved combination guessing single input array ambigious even name differ extra user config data object single example may introduce ambiguity user could still sneak batch single data object via ndarray via listdouble batch double etc potential performance overhead batching data execution data indarray worse current record batch record approach however write converter utility tofrom data worse current tofrom record conversion however discussion micro service deployment remote endpoint background planned allow single konduitserving deployment spread along multiple container case wed like ship something already existing deployed service effectively becomes sort rpc call yes split multiple container something well frequently deployment situation require consider example following scenario pipeline model face detector run edge device powerful face verification model run powerful remote server idea edge device initial detection face detected pas data remote server cant andor dont want run entire pipeline one machine pipeline requires multiple docker container isolation cant embed every language ksjava like python case well split thing multiple docker container matlab swift etc come mind thing might deploy separate docker image even image run one machine situation nonlinear scaling pipeline step example first step filtering like face detection example multicore cpu machine filtering case actually make one gpu machine deployment cpu gpu gpu arent something right think inevitable well run deployment scenario current single process monolithic design wont work schema validation wouldnt possible make step queryable input output want definition time checker schema validation would quite nice guess could added another step doesnt built right thats bad idea would optional step really know inputsoutputs runtime inputsoutputs dynamic wont though example model step wont know placeholderinput name even number inputsoutputs load specific model yes let revisit metadata like input meta data available step wouldnt nice meta data addable step yes dont think covered something like mind example image loading step could return metadata original image format dimension filename etc back user matter many step present image loading step mapping model step end acknowledge naming mismatch may problem maybe easiest way instead guessing make explicit reusable mappingstep map one name another input mask feature featuremask type labeling problem yes dont see alternative unambiguous case like single array input etc