sidekiq lost job recovery sidekiq background processing email alert api includes critical task generating sending email however possible job lost code enqueue job redis fails either transient reason due bug worker process quits ungracefully processing job second issue occurred frequently due worker forcibly restarted consume much memory memorable example loss travel advice content change detected alert travel advice email lead incident conversely dont think first issue occurs frequently help support engineer cope certain kind lost work manual step recover job process content change message manual step involved checking state database indication whether job might lost however possible checking database turn false positive system running slowly worker would double check prevent accidentally duplicating work ideally issue wouldnt exist first place previously considered upgrading sidekiq pro persists job redis processed sidekiq pro would therefore prevent second issue first unrelated sidekiq also worth noting previously concern switching sidekiq pro thinking would hard practice still possible benefit could outweigh drawback decided implement new worker automatically recover work associated generating sending email resolve issue lost work codifies previously manual recovery step since little urgency around sending email decided run worker infrequently currently every half hour work thats hour old expect work processed within hour edge case recovery initiation daily weekly digest run one scheduled job lost create digestrun record normal strategy checking database wont work scenario decided separate recovery strategy thats coupled schedule initiator job strategy involves looking back previous week see digestrun record missing period decided pursue recovering lost work instead acquiring sidekiq pro would help avoid loss first place impression process pay acquire integrate sidekiq pro email alert api would take longer complete implementing solution consequence potential duplicate work mentioned possible may recover work still exists system example job thats hour old may simply delayed due unusually high backlog sidekiq queue although could check state sidekiq part finding lost work still possible race condition falsely requeue work thats lost cope modified worker idempotent worker double check previously completed however could still false positive duplicate job end running concurrently two approach cope slow job like processing content change nonblocking advisory lock mean inprogress work thats incorrectly recovered processed quickly noop fast high volume job like sending email blocking rowlevel lock inside transaction rowlevel lock faster lock part select update statement already execute fetch email record better common case duplicate work even duplicate work exists occupy worker short time original job completes lock released making job idempotent mean system behave correctly long term short term still possible recovery generate alarming amount noop work queue system running slowly particular concern job send email queue latency exceed one hour many thousand fresh job queue sidekiquniquejobs prevent snowball effect scenario competing sidekiq retries creating recovery worker realised would competition retries setup job default sidekiq retries faster recovery make sense continue however recovery worker could still generate relatively large backlog retrying work job perpetually failing due bug tried mitigate limiting number retries job could also consider unique job approach email isnt enough long term also plan improve alert system intervene good time work appears perpetually failing delay processing lost work worst case could take hour recover piece lost work something could tune necessary transactional email sent within minute plan consider part work improving alert system also plan reconsider sidekiq pro lightweight solution speedy recovery recovery worker fallback