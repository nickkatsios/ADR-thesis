# 2. Expose terraform outputs from Product infrastructure to Application pipelines
Date: 2017-07-07

## Status 
Accepted

## Context 

We raised [spike-575](https://contino.tpondemand.com/entity/575) because we wanted to explore the alternatives to expose outputs from the product infrastructure pipelines to the application pipelines. This is necessary because we know that application pipelines will need to consume artificats that will be dinamically generated by other external pipelines (i.e. published urls which are outputs by Product Infrastructure Pipelines).

## Decision 

For this spike we looked at both, Azure’s KeyVault and Hashicorp’s Vault, as key/value stores for making Terraform outputs available for other applications to consume in a secure manner.

Both offerings are very similar however there are a couple of differences worth noting here. The first one is that KeyVault is a hosted cloud native service and key part of the Azure offering, whereas Vault will require additional (although not significant) infrastructure to be provisioned in order to run this service.

The second main difference is around the Terraform support currently available. Although we have Terraform providers covering both services the resource capabilities are more extensive for the Hashicorp Vault provider. Only the Hashicorp Vault provider allow us to populate the store with secrets using terraform code (`azurerm_key_vault` only allow us to create a keyvault); this pretty much reduces the scope of this spike to Hashicorp Vault. Despite the fact that it is possible to populate KeyVault with outputs, any approach will require us to perform actions outside of the current Terraform workflow adding nothing but complexity (i.e. runnint the terraform output command and parsing stdout).

Storing output in Vault as part of any terraform workflow can be simple. The code below shows a very simple and basic example illustrating how this can be accomplished:

```code
.
├── modulea
│   └── output.tf
├── terraform.tfstate
├── terraform.tfstate.backup
└── vault.tf

1 directory, 4 files
portable:vaultpoc dan$ cat vault.tf
module "testmodule" {
  source = "./modulea"
}

resource "vault_generic_secret" "example" {
  path = "secret/foo1"

  data_json = <<EOT
{
  "foo":   "${module.testmodule.test}",
}
EOT

}
portable:vaultpoc dan$ cat modulea/output.tf
output  "test" {
  value = "testing"
}
```

Likewise, it is also very easy to consume the outputs since Vault (and KeyVault as the matter of fact) is a RESTful service after all, providing a good level of abstraction which means any http client can retrieve values as shown below:

```code
portable:vaultpoc dan$ curl -H "X-Vault-Token: 86aa6a1f-cf36-59ff-abd1-ab2624100dec" -X GET http://127.0.0.1:8200/v1/secret/foo1 | jq
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   198  100   198    0     0  34226      0 --:--:-- --:--:-- --:--:-- 39600
{
  "request_id": "8518b07f-bce0-dd61-f2a9-20ca8da1f212",
  "lease_id": "",
  "renewable": false,
  "lease_duration": 2764800,
  "data": {
    "foo": "testing",
  },
  "wrap_info": null,
  "warnings": null,
  "auth": null
}

portable:vaultpoc dan$ curl -H "X-Vault-Token: 86aa6a1f-cf36-59ff-abd1-ab2624100dec" -X GET http://127.0.0.1:8200/v1/secret/foo1 | jq -r .data.foo
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   198  100   198    0     0  40268      0 --:--:-- --:--:-- --:--:-- 49500
testing
```

Regarding naming conventions for secret storage there are many options available but we recommend to start by mirroring the agreed conventions for TF state detailed in the [general terraform guidelines](https://github.com/contino/moj-infrastructure-guidelines#storing-state) i.e. 
```code
<statestorebackend>/<project>/<env>/<output key>
```
Similarly there are many approaches when it comes to access policies for Vault, it is a very extensive topic and well documented by [hashicorp](https://www.vaultproject.io/docs/concepts/policies.html). A good starting point, however, would be - again - to mirror the policies the Terraform/Jenkins user have against the Azure storage containers used to store the state, which at the time of this writing are pretty much matching the sudo level in Vault. With time and when we start considering more complex use cases, Vault access policies are flexible enough and provide and very good level of granularity for us to start writing more comprehensive policies.

Finally, it is important to understand the general guidelines when writing Terraform code but in particular, when writing Vault related resources, please do not forget the following:

> **Important** All data provided in the resource configuration will be written in cleartext to state and plan files generated by Terraform, and will appear in the console output when Terraform runs. Protect these artifacts accordingly. See the main provider documentation for more details.

One might think that the above can be mitigated by setting the sensitive attribute in the output to true, but during our testing (specifically when running the terraform plan command) we were hitting the following limitation of sensitive outputs:

> **Sensitivity is not tracked internally, so if the output is interpolated in another module into a resource, the value will be displayed.**

## Consequences
By having TF outputs stored in Vault it becomes easier for application teams to consume this information in a safe a secure manner. At the same time we are reducing complexity by not mixing pipelines goals somewhat establishing some separation of concerns (i.e. no need to run Terraform utilities or code within the application pipeline).
