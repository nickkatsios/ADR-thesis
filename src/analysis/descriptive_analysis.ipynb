{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Library/Frameworks/Python.framework/Versions/3.12/lib/python312.zip', '/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12', '/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/lib-dynload', '', '/Users/nikolakis/Library/Python/3.12/lib/python/site-packages', '/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages', '/Users/nikolakis/Projects/ADR-thesis/src/scrapping', '/Users/nikolakis/Projects/ADR-thesis/src/scrapping']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Add the parent directory of 'scrapping' to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join(current_dir, '..', 'scrapping')))\n",
    "\n",
    "from text_cleaner import read_and_clean_adrs\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from markdown2 import markdown\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "print(sys.path)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Path to the ADR directory\n",
    "adr_directory = \"../../data/ADRs-Updated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of ADR files in metadata: 6362\n"
     ]
    }
   ],
   "source": [
    "# Iterate over all files in the directory\n",
    "metadata_path = \"../../data/ADR-Study-Dataset-Metadata/repositories\"\n",
    "total_adr_files = 0\n",
    "for filename in os.listdir(metadata_path):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(metadata_path, filename)\n",
    "        # Open and read the JSON file\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            # Add the value of numAdrFiles to the total\n",
    "            total_adr_files += data.get('numAdrFiles', 0)\n",
    "\n",
    "# Print the total number of ADR files\n",
    "print(f'Total number of ADR files in metadata: {total_adr_files}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of ADR files in directory: 5368\n"
     ]
    }
   ],
   "source": [
    "# count total number of files in the ADR directory\n",
    "total_adr_files = 0\n",
    "for filename in os.listdir(adr_directory):\n",
    "    if filename.endswith('.md'):\n",
    "        total_adr_files += 1\n",
    "print(f'Total number of ADR files in directory: {total_adr_files}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(markdown_content):\n",
    "    # Convert Markdown to HTML\n",
    "    html_content = markdown(markdown_content)\n",
    "    # Parse HTML to text\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    text = soup.get_text()\n",
    "    # Remove non-alphabetic characters and lower the case\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text).lower()\n",
    "    # Tokenize, remove stop words, and lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "    # remove words with less than 3 characters\n",
    "    tokens = [word for word in tokens if len(word) > 2]\n",
    "\n",
    "    # remove common terms\n",
    "    common_terms = [\"context\", \"decision\", \"status\", \"consequences\",\n",
    "                     \"motivation\", \"options\", \"option\", \"alternatives\", \"alternative\"]\n",
    "    \n",
    "    tokens = [word for word in tokens if word not in common_terms]\n",
    "\n",
    "    return ' '.join(tokens), tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most common terms in adr names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words in ADR file names:\n",
      "use: 900\n",
      "api: 163\n",
      "data: 116\n",
      "test: 110\n",
      "service: 105\n",
      "architecture: 99\n",
      "file: 88\n",
      "component: 80\n",
      "code: 74\n",
      "testing: 73\n",
      "application: 70\n",
      "framework: 67\n",
      "module: 66\n",
      "create: 62\n",
      "decision: 61\n",
      "user: 60\n",
      "support: 59\n",
      "store: 58\n",
      "version: 58\n",
      "frontend: 57\n",
      "record: 57\n",
      "add: 56\n",
      "database: 56\n",
      "react: 54\n",
      "dependency: 54\n",
      "structure: 54\n",
      "management: 54\n",
      "client: 53\n",
      "docker: 52\n",
      "aws: 52\n",
      "event: 51\n",
      "package: 50\n",
      "app: 50\n",
      "remove: 50\n",
      "library: 49\n",
      "implement: 49\n",
      "server: 49\n",
      "model: 47\n",
      "storage: 47\n",
      "using: 46\n",
      "configuration: 45\n",
      "language: 44\n",
      "integration: 44\n",
      "build: 44\n",
      "config: 42\n",
      "project: 42\n",
      "authentication: 41\n",
      "container: 40\n",
      "source: 40\n",
      "type: 40\n"
     ]
    }
   ],
   "source": [
    "file_name_word_count = {}\n",
    "for file_name in os.listdir(adr_directory):\n",
    "    words = file_name.split('-')\n",
    "    words = [word.replace('.md', '') for word in words]\n",
    "    # convert to lowercase\n",
    "    words = [word.lower() for word in words]\n",
    "    # remove the numbers\n",
    "    words = [word for word in words if not word.isdigit()]\n",
    "    # remove the word ADR\n",
    "    words = [word for word in words if word.lower() != 'adr']\n",
    "    # keep words longer than 2 characters\n",
    "    words = [word for word in words if len(word) > 2]\n",
    "    # use limatizer\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    for word in words:\n",
    "        if word in file_name_word_count:\n",
    "            file_name_word_count[word] += 1\n",
    "        else:\n",
    "            file_name_word_count[word] = 1\n",
    "\n",
    "# Top 10 words in ADR file names\n",
    "print(\"Top 10 words in ADR file names:\")\n",
    "for word, count in Counter(file_name_word_count).most_common(50):\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average clean tokens (words) per ADR: 212.4236214605067"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most frequent words inside adrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('use', 9933), ('need', 7712), ('data', 6911), ('user', 6667), ('service', 6332), ('new', 5297), ('code', 4958), ('would', 4925), ('consequence', 4836), ('using', 4815), ('api', 4752), ('change', 4633), ('application', 4588), ('file', 4503), ('date', 4360), ('type', 3842), ('component', 3827), ('used', 3823), ('one', 3769), ('also', 3718), ('accepted', 3625), ('time', 3579), ('version', 3528), ('test', 3408), ('support', 3372), ('make', 3157), ('project', 2904), ('may', 2853), ('message', 2797), ('example', 2785), ('adr', 2772), ('client', 2759), ('module', 2737), ('request', 2724), ('system', 2687), ('work', 2684), ('configuration', 2562), ('like', 2549), ('feature', 2520), ('set', 2485), ('could', 2475), ('way', 2458), ('value', 2435), ('create', 2421), ('key', 2387), ('implementation', 2375), ('case', 2353), ('error', 2336), ('different', 2322), ('name', 2269)]\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "for file_name in os.listdir(adr_directory):\n",
    "    file_path = os.path.join(adr_directory, file_name)\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        preprocessed_text, words = clean_text(text)\n",
    "        all_words.extend(words)\n",
    "\n",
    "word_freq = Counter(all_words)\n",
    "print(word_freq.most_common(50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
