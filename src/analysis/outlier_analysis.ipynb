{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "# Add the parent directory of 'scrapping' to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join(current_dir, '..', 'scrapping')))\n",
    "from text_cleaner import read_and_clean_adrs\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from markdown2 import markdown\n",
    "import openai\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import MaximalMarginalRelevance, OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "import datamapplot\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Path to the ADR directory\n",
    "adr_directory = \"../../data/ADRs-Updated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "I have a topic that contains the following documents: \n",
    "[DOCUMENTS]\n",
    "The topic is described by the following keywords: [KEYWORDS]\n",
    "\n",
    "Based on the information above, extract a short topic label in the following format:\n",
    "topic: <topic label>\n",
    "the topic label must be at most 5 words long strictly, it must not contain specific names of technologies or programming languages. The documents are related to software development. They are architectural\n",
    "decision records (ADRs) that describe the decisions made in the development of software systems. The keywords are extracted from the documents.\n",
    "The topic labels should be general enough but will need to differantiate between\n",
    "decision categories such as component decisions, programming language or framework decisions,\n",
    "security, performance, scalability, infrastructure, deployment, testing, formating, standands, etc.\n",
    "\"\"\"\n",
    "\n",
    "# GPT as representation model ~0.10$ per run\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPEN_AI_API_KEY\"))\n",
    "open_ai_repr_model = OpenAI(client, model=\"gpt-3.5-turbo\", chat=True, prompt=prompt, tokenizer=\"vectorizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main representation of a topic\n",
    "# Use a KeyBERT-like model to fine-tune the topic representations\n",
    "# The algorithm follows KeyBERT but does some optimization in order to speed up inference.\n",
    "keybert_repr = KeyBERTInspired()\n",
    "\n",
    "# Add all models together to be run in a single `fit`\n",
    "representation_model = {\n",
    "   \"Main\": open_ai_repr_model,\n",
    "   \"Keywords\":  keybert_repr,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of embeddings\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "outlier_embeddings = embedding_model.encode(outlier_documents, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "# A higher min_cluster_size will generate fewer topics and a lower min_cluster_size will generate more topics.\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "umap_model = UMAP(n_neighbors=10, n_components=5, min_dist=0.0, metric='cosine', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_topic_1 = BERTopic(\n",
    "                        # hyperparameters \n",
    "                        language=\"english\", # language of the documents\n",
    "                        # nr_topics=10, # number of topics to output (this reduces topic AFTER they have been discovered)\n",
    "                        top_n_words=10, # number of top words per topic\n",
    "                        n_gram_range=(1, 2), # number of words per n-gram (n_grams are phrases of n words)\n",
    "                        min_topic_size=12,  # minimum number of data points per topic (more = less topics)\n",
    "                        # models and embeddings\n",
    "                        umap_model=umap_model, \n",
    "                        hdbscan_model=hdbscan_model,\n",
    "                        representation_model=representation_model,\n",
    "                        embedding_model=embedding_model,\n",
    "                        # other\n",
    "                        calculate_probabilities=True, # calculate the probs of a document belonging to a topic (slows down training, use when good results are found)\n",
    "                        )\n",
    "topics, probs = bert_topic_1.fit_transform(outlier_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_topic_1.visualize_documents(outlier_documents, embeddings=outlier_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
