Topic,Count,Name,Representation,Keywords,Representative_Docs
0,770,Frontend component architecture,['Library component distribution'],"['react', 'component', 'typescript', 'npm', 'framework', 'frontend', 'dependency', 'design', 'development', 'angular']","['create new react application setting redesign project deciders lauren zugai orchard danny coates wil clouser problem statement setting refers widely accountsfirefoxcomsettings page user manage firefox account setting redesign project refers project centered around giving page fresh user interface change fairly significant require lot component shuffling functionality refactoring see prd fxa engineering team desire migrate backbone mustache react adr address question around level action take regarding desire setting redesign project conversion react done later time change significant enough justify propelling react considered existing framework backbone templating library mustache make change file take approach creating new component react modify existing backbonemustache file take ground approach create new setting react application outcome chosen take ground approach create new setting react application overall goal setting redesign project offer flexibility expand setting functionality new feature already planned could argued heavy refactoring later would needed would take much time recreating page react approach allows opportunity set storybook content server review test implementation implement change approach also simplifies workflow testing entire redesign mitigate risk avoiding big bang surprise replacement implementing deploying launching smaller piece mvp functionality along way new frontend route serving react application keeping backbone setting live may tedious safety net losing integrity wisdom earned original system pro con existing framework backbone templating library mustache make change file description backbone mustache setting redesign project conversion addition react done separate project pro allows reuse existing fxacontentserver component level many change satisfied mustachecss update requires initial setup change production much quickly con significant functionality change functionality refactor create project refactored later react note set well future feature request come later year would forced continue either way react conversion backlog would continue grow integration main must done carefully testing change may complicated take approach creating new component react modify existing backbonemustache file refactor component react component description combination react backbonemustache setting redesign project component already exist modify backbonemustache create new component react possibly begin backbonetoreact integration make sense full conversion react done separate project approach suggests fxa new component created react pro reuse existing fxacontentserver component level many mustachecss update requires initial setup upfront refactoring change production much quickly allows piecemeal react setting resulting future refactoring avoiding taboo ground remake approach allows creation react component could elsewhere fxa ecosystem con full benefit react application wont realized many piece converted many popular backbonetoreact integration guide combination react backbone approach like embedding react component backbone view wrapping backbone app react andor keeping data synchronized redux store backbone preferable move away interlinked dependency two thus would require refactoring later package like nestedreact offer convergence layer two additional dependency would want remove later integration main must done carefully testing change may complicated take ground approach create new setting react application description create new react application setting page approach incremental viewbyview build pro propels new framework choice reduces future refactoring effort setting doesnt manage different integration like signinsignup flow seems like reasonable place begin conversion regardless redesign allows fresh start audit html semantics architecture accessibility practice component built allows test auditing scrutiny potentially remove superfluous test add lack may allow fresh take metric setting offer opportunity set storybook fxa viewbyview build deploy change somewhere folk see progress request tweak feature built potentially opportunity review extract model api code existing setting app shared module management github easy change directly main feature flagging branch held isolation main necessary testing also likely easier con ground remake generally viewed taboo changing large chunk frontend codebase doesnt come without set risk approach take much longer rollout previous link setting redesign project prd backbone react adr react backbone mustache nestedreact', 'publish component statusaccepted proposed able pull component apps one npm package pro con singlepackage approach versus one package per component one package per component pro minimum dependency pulled component dont download package anything arent pull fix one component arent unintentionally changing behavior hundred component con track dependency per component also install whole style guide app run one component package installed developer needed publishing tricky especially interdependency lerna semanticrelease may help typical lerna repo structure may ideal style guide app single package exporting component pro install single package get access app much simpler publish multiple package easier people working style guide repo understand con every time bump dependency version component package youll pull potentially changed version every component app extra work required ensure bundled javascript include component app style guide complication style guide release cycle relate release cycle component within style guide represent multiple version component change complexity setting maintaining understanding onepackagepercomponent approach even help tool like lerna strong con assume single package preference let look might mitigate con apprach first con pulling unwanted component change major cause lot headache lot people thing avoid every component change change appearance component way exported new component whole component folder copied folder way app pull updated package see change change import button reactioncommercecomponentsv import button reactioncommercecomponentsv react component library import react component library component component render passed prop child every component jest snapshot testing give clue developer reviewer component appearance may changed part review process deciding whether change visual whether require splitting new version component considered breaking change possibility automating even automated screen shot comparison approach also answer question style guide show version component since version remain repo versioned folder name theyll naturally appear second con single package avoided following suggestion apps component package set tree shaking solution avoid bundling component import', 'adr isolating behavior custom clements vanilla javascript stage approved adopted superseded adr related document april custom element example may custom element experiment may react custom element october issue exploring pro con reusing com react november experiment porting prc web component november usage custom element iie project november styling custom element styled component june discussion slack skatejs ssr november replace dialog detailsdialog january guideline authoring behavior february example vanilla behavior accompanying hook providing behavior via custom element throughout last year folk design infrastructure web system platform team discussed idea custom element behavior primer react main goal custom element primer react able author behavior reuse framework several experiment conducted listed assumption deduplication highest priority attempt deduplication must weighed change maintainer developer customer experience finding developer experience regression custom element rendering subtrees shadowdom requires polyfills asyet implemented specification mean primer react accumulate added complexity implement custom element shadowdom implementing custom element primer react require division client side server side code custom element executed browser environment currently primer react isomorphic code executed anywhere react includes nodejs server runtimes well client side insurmountable mean primer react accumulate added complexity likely surfaced user possible add server side library enable custom element rendered server add complexity antithetical usage pattern custom element writing cannot style custom element styledcomponents mean component want custom element get behavior also want style component must another wrapper div apply style bug styledcomponents fixed next release incompatibility react tool github custom element detailsdialog detailsmenu make assumption dom tree example detailsdialog expects detail element wrap custom element assumption determine whether click happening inside outside dialog close dialog click happened outside dialog make sense case nice way enforcing proper usage detail element break react portal often ensure menu displayed correctly case parent overflow hidden applied incompatible zindex extensibility building behavior react hook give ability provide thing like state state change hook consumer component allows user build additional behavior component based state variable provided component consumer custom element would require listening event document reacting certainly doable foundational principle react reacting change dom change react state organizational overhead githubs custom element managed different repos introduces maintenance overhead youd npm link developing want test change presentational component instead making change seeing update instantly npm link usually doesnt work well hot module reloading either youd draft publish release library every time want update behavior behavior shared githubcom primer react youd careful testing environment make sure change dont create regression greatly widens engineer keep mind every time change made reacting change take bit time well orchestrate release custom element primer react opposed behavior already present primer react versioned lockstep engineer want contribute primer react component build new component behavior would familiar custom element react two different paradigm switch two custom element web component api progress slower react due change needing whatwg standard process risk switching custom element behavior spend extra time building behavior react already built custom element library currently behaviorscomponents listed custom element documentation site several already implemented react either primer react doctocat react application github upstreamed detailsdialog detailsmenu clipboardcopy textexpander autocomplete tasklist via drag drop hook tabcontainer textexpander decide invest react github wasted time could spent building custom element seems unlikely seems clear consensus continue build highly interactive product react library abandoned becomes obsolete risk technology may seems highly unlikely near term also possibility custom element track record demonstrates deprecation web apis extremely rare long deprecation path behavior githubcom custom element behavior primer react diverge leading inconsistent experience probably biggest risk face moving custom element isnt necessarily best solution explore way detecting divergence integration test providing behavior vanilla javascript simpler method isolating component behavior implement vanilla javascript typescript way shared react component web component type consumer would hook vanilla behavior component case strategy straightforward behavior made dependency dom easy isolate consume various framework behavior effect interactionsevents shared state component style difficult isolate manner interaction event many user interaction rely dom event click keypress focus reacts event system native dom event system react implement syntheticevent wrap native event working syntheticevents native event simultaneously significant additional complexity maintainer consumer however vanilla javascript must operate native event make isolating behavior automatically hook event listener dom element difficult achieve resulting simultaneous usage native event syntheticevent potential degrade maintainer consumer developer experience primer react shared state countless way manage state web application react ecosystem state management strategy library addition primitive construct state management since standard state management pattern vanilla javascript introducing pattern would add new layer complexity component behavior api component style since primer react styledcomponents manage style behavior affect style styledcomponents vanilla javascript behavior affect style add complexity introducing second mechanism applying style since able styledcomponents custom element due challenge listed priority listed assumption section investing time building behavior custom element primer react library instead spend time expanding coverage react hook focus finding approach making sure implementation behavior different stack consistent integration test vanilla javascript behavior behavior implemented vanilla javascript without introducing additional complexity primer react consumer case possible behavior implemented dependency except dom consumed within react hook provide functionality primer react general portion behavior affect rely user interaction event shared state style kept react hook part behavior implemented isolation concept built dependency react library']"
1,276, Architecture Decision Records (ADRs),['Architecture Decision Records (ADRs)'],"['architectural record', 'architecture record', 'documentation', 'changelog', 'record architecture', 'record architectural', 'example description', 'document', 'architectural', 'structure']","['documenting architecture architecture agile project described defined differently made done project begin agile method opposed documentation valueless documentation document assist team value kept large document never kept small modular document least chance updated nobody ever read large document either developer least one project specification document larger byte total source code size document large open read update bite sized piece easier stakeholder consume one hardest thing track life project motivation behind certain new person coming project may perplexed baffled delighted infuriated past without understanding rationale consequence person two choice blindly accept response may still valid may good however changed really revisited project accumulates many without understanding development team becomes afraid change anything project collapse weight blindly change may reversed hand changing without understanding motivation consequence could mean damaging project overall value without realizing supported nonfunctional requirement hasnt tested yet better avoid either blind acceptance blind reversal keep collection record architecturally significant affect structure nonfunctional characteristic dependency interface construction technique architecture record short text file format similar alexandrian pattern though necessarily pattern share characteristic balancing force record describes set force single response force note central piece specific force may appear multiple adrs keep adrs project repository docarchadrnnnmd lightweight text formatting language like markdown textile adrs numbered sequentially monotonically number reused reversed keep old one around mark superseded still relevant know longer format part document easy digest format part title document name short noun phrase example adr deployment ruby rail adr ldap multitenant integration section describes force play including technological political social project local force probably tension called language section valueneutral simply describing fact section describes response force stated full sentence active voice may proposed project stakeholder havent agreed yet agreed later adr change revers may marked deprecated superseded reference replacement consequence section describes resulting applying consequence listed positive one particular may positive negative neutral consequence affect team project future whole document one two page long write adr conversation future developer requires good writing style full sentence organized paragraph bullet acceptable visual style excuse writing sentence fragment bullet kill people even powerpoint bullet consequence one adr describes one significant specific project something effect rest project run consequence one adr likely become subsequent adrs also similar alexander idea pattern language largescale response create space smaller scale fit developer project stakeholder see adrs even team composition change time motivation behind previous visible everyone present future nobody left scratching head understand thinking time change old clear change project', 'record architectural architecture agile project described defined differently made done project begin agile method opposed documentation valueless documentation document assist team value kept large document never kept small modular document least chance updated nobody ever read large document either developer least one project specification document larger byte total source code size document large open read update bite sized piece easier stakeholder consume one hardest thing track life project motivation behind certain new person coming project may perplexed baffled delighted infuriated past without understanding rationale consequence person two choice blindly accept response may still valid may good however changed really revisited project accumulates many without understanding development team becomes afraid change anything project collapse weight blindly change may reversed hand changing without understanding motivation consequence could mean damaging project overall value without realizing supported nonfunctional requirement hasnt tested yet better avoid either blind acceptance blind reversal keep collection record architecturally significant affect structure nonfunctional characteristic dependency interface construction technique architecture record short text file format similar alexandrian pattern though necessarily pattern share characteristic balancing force record describes set force single response force note central piece specific force may appear multiple adrs keep adrs project repository docarchadrnnnmd lightweight text formatting language like markdown textile however git repo project record document google drive adrs numbered sequentially monotonically number reused reversed keep old one around mark superseded still relevant know longer format part document easy digest format part section usage title document name short noun phrase example adr deployment ruby rail adr ldap multitenant integration section describes force play including technological political social project local force probably tension called language section valueneutral simply describing fact section describes response force stated full sentence active voice may proposed project stakeholder havent agreed yet agreed later adr change revers may marked deprecated superseded reference replacement consequence section describes resulting applying consequence listed positive one particular may positive negative neutral consequence affect team project future whole document one two page long write adr conversation future developer requires good writing style full sentence organized paragraph bullet acceptable visual style excuse writing sentence fragment bullet kill people even powerpoint bullet consequence one adr describes one significant specific project something effect rest project run consequence one adr likely become subsequent adrs also similar alexander idea pattern language largescale response create space smaller scale fit developer project stakeholder see adrs even team composition change time motivation behind previous visible everyone present future nobody left scratching head understand thinking time change old clear change project taken httpthinkrelevancecomblogdocumentingarchitecturedecisions reference', 'architectural record statusaccepted architecture agile project described defined differently made done project begin agile method opposed documentation valueless documentation document assist team value kept large document never kept small modular document least chance updated nobody ever read large document either developer least one project specification document larger byte total source code size document large open read update bite sized piece easier stakeholder consume one hardest thing track life project motivation behind certain new person coming project may perplexed baffled delighted infuriated past without understanding rationale consequence person two choice blindly accept response may still valid may good however changed really revisited project accumulates many without understanding development team becomes afraid change anything project collapse weight blindly change may reversed hand changing without understanding motivation consequence could mean damaging project overall value without realizing supported nonfunctional requirement hasnt tested yet better avoid either blind acceptance blind reversal keep collection record architecturally significant affect structure nonfunctional characteristic dependency interface construction technique architecture record short text file format similar alexandrian pattern though necessarily pattern share characteristic balancing force record describes set force single response force note central piece specific force may appear multiple adrs keep adrs project repository docarchadrnnnmd lightweight text formatting language like markdown textile adrs numbered sequentially monotonically number reused reversed keep old one around mark superseded still relevant know longer format part document easy digest format part title document name short noun phrase example adr deployment ruby rail adr ldap multitenant integration section describes force play including technological political social project local force probably tension called language section valueneutral simply describing fact section describes response force stated full sentence active voice may proposed project stakeholder havent agreed yet agreed later adr change revers may marked deprecated superseded reference replacement consequence section describes resulting applying consequence listed positive one particular may positive negative neutral consequence affect team project future whole document one two page long write adr conversation future developer requires good writing style full sentence organized paragraph bullet acceptable visual style excuse writing sentence fragment bullet kill people even powerpoint bullet consequence one adr describes one significant specific project something effect rest project run consequence one adr likely become subsequent adrs also similar alexander idea pattern language largescale response create space smaller scale fit developer project stakeholder see adrs even team composition change time motivation behind previous visible everyone present future nobody left scratching head understand thinking time change old clear change project']"
2,248, User Authentication Architecture,['User Authentication Architecture'],"['access control', 'authorization', 'oauth', 'authentication', 'openid', 'organization', 'authenticate', 'access token', 'auth', 'security']","['determine new authentication system open apparel registry application currently firebase user authentication new user sign form react client creates record user firebase firebase turn generates user unique source identifier uid legacy oar api connects uploaded facility data legacy api also collates uid api key via keyschema keycontroller apparent aim enabling user create userspecific api key communicating api directly variety reason weve decided replace firebase auth new authentication system previously decided replace legacy api written nodejs restify mongodb new api written python django rest framework postgis change would compel rewrite apis authentication keygenerating mechanism entirely even going keep firebase however since neither civic apps operation much experience firebase production system weve decided replace rather adapt adr explains choice two authentication replace firebase djangorestauth auth ultimately recommends djangorestauth django rest framework django rest auth complement django rest framework djangorestauth add standardized set url endpoint operation simplify single page apps djangos user model djangorestauths documentation enumerates following feature user registration activation loginlogout retrieveupdate django user model password change password reset via email social medium authentication civic apps previously djangorestauth pwd stormdrain marking pwd stormwater connect also known garp similar stack react single page application talking django rest framework api azavea development team also djangorestauth project similar stack temperate project instance djangorestauth angular spa pro familiarity since weve djangorestauth familiar library garp implement registration account activation password reset feature ideally would mean wont spend much time poring djangorestauth documentation source code order set django user model token authentication djangorestauth make django user model also experience model watershed opentreemap model also enables considering drf token authentication generating api key django user model also enables configure permission class python code rather elsewhere django admin since djangorestauth work top djangos user model enables make django admin available administrative task related user anticipate mutiple different type user django admin site offer way user management via web gui without create one react thirdparty dashboard con email legacy oar api confirmation password reset email handled entirely firebase handling via djangorestauth compel set email backend likely amazon note however likely already configuring application boilerplate djangorestauth garp seemed entail writing lot boilerplate code general seems like djangopython code footprint wasnt large write lot react code related signup login password reset form along associated redux action error handling validation likewise well write django template email different kind email djangorestauth generate incidentally react application currently full suite signup signin password reset user profile form already implemented case would adapt component redux action djangorestauth rather firebase django user model djangorestauth rather auth require application thicker user model store user logins password potentially risky store data since application userbase likely international scope likely gdpr consequence social login djangorestauth social login capability feature weve documentation indicates support twitter facebook auth auth thirdparty identity platform provides user signup signin management along addons like email multifactor authentication two selling point ability abstract away many painful part dealing user management vast array integration enable work single page apps mobile apps apis even machinetomachine application like command line tool civic apps previously auth small clientonly react application raster foundry extensive experience auth pro reduced boilerplate auth provides nearly dropin signup signin password reset auth site tutorial react project general idea react app would include authservice class couple required route clicking button sign login would move user auth sign via user interface mean react apps auth implement form component redux apparatus handling signin auth also provides mechanism whereby redirect would internal url say httpexamplecom httpauthexamplecom social login along usernamepassword combination auth box support logging via google extended support numerous identity provider including github weibo configuring new identity provider typically entail setting new application provider site appears work automatically afterwards thin django user model since auth presumably would store data related user logins profile permission resulting django user model could thin essentially would gather whatever auth provides together api key token foreign key group also roughly legacy apis keyschema designed pricing general auth seems would free monthly active user number price start grow linearly tried auth pricing wizard appeared maus cost per month per month rate seem completely reasonable certain auth would save lot developer time con integration django rest framework api along guide auth react auth django rest framework auth documentation site also guide integrating auth application designed api single page app roughly similar wed want build guide fairly straightforward follow also seems indiciate there quite bit complexity manage general idea user signed react app would get jwt would send api api turn would decode token determine whether user correct set permission whatever request differential permission set configuring set role auth web api endpoint get decorator indicate permission require permission class auth dashboard auth handle differential access user type via authorization extension enables auth dashboard create different role permission user default stored auths webtask storage since provides storage there numerical limit combination user role wed set bucket store permission documentation note roughly equivalent group user user member group group user user member group addition user management would take place auth dashboard mean people user management would given auth login api key generation although seems like authbased system may also allow drfs token authentication api key generation workflow weve set raster foundry prediction api application api token main raster foundry application created band unfamiliarity firebase civic apps comparatively unfamiliar auth production civic apps auth small backendless react application case small set user known advance mean signup mechanism across azavea raster foundry seems auth extensively prediction api application drf api react spa auth roughly similar wed antipate creating application also seems small known set user raster foundry main application also auth team encountered friction early trying adapt change auth made api djangorestauth familiar enough djangorestauth work setting involve typing rather thinking problem fact react already includes custom signup login etc form component suggests itll bit cumbersome would starting scratch building clientonly react application talked number third party service think itd make sense integrate auth however since django backend seems leverage capability django includes user management django admin email like interested trying auth smaller project committing larger one consequence result choosing djangorestauth authentication well plan include djangorestauth project register proper url possibly also adjust initial migration create new user model well also adjust react application form work new auth system decide retain existing firebase user data well also determine migrate alternatively could also plan require user sign new account new api', 'permission architecture organization adr superseded adr concerning implementation access control rulesacrs basic implementation organization essence two organization root organization default public organization feature initially envisioned change priority lack data pushed full implementation road point would better understanding usage pattern requirement find point target feature set concept target feature set introduces redefines administrative concept platform platform top level unit administration platform provide administrative control number organization platform owner group owner administer underlying organization application platform largely invisible user given administrative control platform platform implemented allow largescale organization raster foundry foundational architecture case whether public infrastructure raster foundry raster foundry deployed private cloud platform level control make managing large number organization realistic organization organization represent group user within platform organization two type setting one group setting administered platform owner featureflags upload limit another set adjustable organization owner logo api key organization belongs one platform user member one organization team team represent group user within across organization team belongs one organization user member number team mechanic able determine user access entity taking account multiple factor including entity ownership team membership organization membership role within organization role within platform explicitly granted access scenario consider following scenario mechanism test accesscontrol method member organization want allow user organization view project allowing anyone outside organization access admin organization able change organization setting regular user within organization admins organization allowed view adjust setting admin platform able change platform level setting user platform admins cannot view change setting member team want allow user team able view edit analysis allowing anyone outside team access user organization want another user organization able view project ensuring user organization access user team want user team able view analysis current landscape existing control current implementation far complete existing piece api could kept place requires full understanding current implementation userorganization relationship user organization field allows relationship organization user currently permission check method isinrootorganization isinrootorsameorganizationas user role user role field field limited three value user viewer admin field actually way entity ownership visibility entity api owner apart organization user owner basis permission check many case method like isinrootorowner entity api also visibility field field determines visibility particular entity relative owning user organization three possible value public entity visible user organization organization entity visible user within organization owner private entity visible owner entity also assumed admin user view entity regardless visibility setting access control method attempting understand various method would applied important note case method conjunction case one method implement another method additionally many standard method mostly described theoretical term implementation detail seen scope terminology helpful object resource entity protected unauthorized access action think crud raster foundry object project analysis subject entity attempting access perform action object case user could also machineuser attribute property entity environmental condition object subject attribute owner name timestamps environmental attribute could timeofday dayofweek even current location subject mandatory access control mac mac access control method policy applied subject object based security attribute determine access allowed resource owner control policy attribute mandatory access control security policy centrally controlled security policy administrator user ability override policy example grant access file would otherwise restricted mandatory access control wikipedia many mac implementation focused accesscontrol compatible government classified information system strict control mac provides necessary case security attribute subject could subject clearance level object security attribute would classification level policy would indicate subject topsecret clearance read object topsecret classification level contrived example mac might look like raster foundry nonadmin user create analysis never delete analysis admin user create analysis nonadmin user view analysis discretionary access control dac dac similar mac policy applied subject object determine attribute satisfy access requirement dac differs mac object owner elect modify attribute object thereby grant access user hence discretionary portion name example would unix file mode ability user chmod adjust attribute file rolebased access control rbac rbac role assigned user determine access rbac also enforce mac dac user never assigned permission directly instead permission granted user determined assigned role rbac role intended define user permission term specific operation organizational meaning raster foundry operation might adding band datasource rather defining user permission term object editing datasource typical rbac data model role granted permission permission map operation object mean pool object expand permission granted role also expand example project created permission projectget would created added admin role adding permission admin role avoids directly grant permission large number user role access control list acl identitybased access control ibac acls list identity allowed access specific resource resource acl attempted access requires ensuring subject identity included object acl acl implementation differ complexity case entry acl also contains specific operation example entry may indicate user may allowed read object another entry indicates user write object sense user access accumulated acl attributebased access control abac abac considered successor rbac seen policy based similar mac far flexible various attribute source compute access right additionally unlike mac abac prohibit resource owner control permission example timeofday could attribute creating accesspolicy user create deposit abacs flexibility implement mac dac rbac acls case rbac evaluated subject attribute would role acl subject attribute might object attribute would acl abac policy often expressed xacml xacml wikipedia requirement abac accesscontrol analysis contextual knowledge back look scenario previously listed see best might implemented scenario member organization want allow user organization view project allowing anyone outside organization access would best implemented dac abac subject case someone trying access project person creating project subject attribute organization object attribute visibility organization organization policy member organization read access project organization mac suitable user control visibility object weve already implemented attribute determine access scenario admin organization able change organization setting regular user within organization admins organization allowed view adjust setting could implemented method rbac make sense would assume role specific organization exists admins subject would role view change setting scenario admin platform able change platform level setting user platform admins cannot view change setting rbac make sense scenario well assuming role platform exists scenario member team want allow user team able view edit analysis allowing anyone outside team access rbac would involve role member team permission view edit specific analysis would added teammember role specific team thing certainly begin get convoluted level abac may better point note involve implementing acl within abac note subject still person attempting access subject attribute teammembership object attribute visibility private acl team teamidview team teamid edit policy creator object access private entity acl team entry would check subject teammembership grant specific permission match scenario user organization want another user organization able view project ensuring user organization access rbac would messy would involve new role specifically project adding needed permission role granting user role instead seems like clear case acl via abac would mean adding single user project acl view permission subject attribute userid object attribute visibility private acl user useridview policy creator object access private entity acl user entry would check subject userid grant specific permission match scenario user team want user team able view analysis scenario would handled similarly way scenario different acl subject attribute teammembership object attribute visibility private acl team teamidview policy creator object access private entity acl team entry would check subject teammembership grant specific permission match proposed access control approach scenario make thing clear accesscontrol system concept acls accesscontrol system concept role almost anything rbac risk roleexplosion real rbac implemented emulate acl functionality unavoidable addition administrative overhead rbac would significant would mostlikely necessary present interface user largely obscures underlying accesscontrol system hand implementing accesscontrol system acls would allow accesscontrol system closely parallel presented user propose aclcentric approach take implementation detail accesscontrol computation within process completing action object accesscontrol computation determine subject necessary permission complete action akka authorize directive authorize akka http per object policy set seems like would good approach policy method leverage request subject object return boolean indicate access allowed would also beneficial unit test policy well objectlevel policysets whole possible issue accesscontrol logic api level code rather level could incur large read define apply policy subject object acl abac perspective treat acls object level attribute architectural perspective acl table would enable query better leverage acls improve performance list query possible schema table could uuid objecttype enum object type project analysis etc objectid uuid pointing specific object subjecttype enum possible control level public platform organization team user subjectid uuid pointing specific subject nullable public would allowedactions field could handled several way limited enum action type could multiple row fully represent access rule one row read one write etc bit mask could represent possible permission single column separate boolean column action acls operate subject level relate specific permission specific user propose implement multilevel acl reflects userorganization structure mean allowing acl entry reference one following platform organization team user acls first accesscontrol check done would override policy acls would also effectively eliminate visibility field present resource acl would allow defining object visibility finegrained control private organzation public issue dont currently approach replicating public though allowing access platform level may sufficient question storing acl entity make sense separate table json make sense better type subject level necessary action missing eliminate visibility field resource instead acl exclusively replicate public visibility setting platform level acl entry equivalent negative permission example specifically denying access certain person team role handle role subject attribute would best accomplished via relationship table entry could constructed like uuid user uuid user role applies grouptype platform organization team groupid uuid specific group role admin member enums rather isadmin field leaf room role could also done separate table type rather combining one table question comment currently think user belong one organization user would move different organization would create another account may problematic approach currently authenticate via email would account cant support think better name type typeid descriptive eliminate organization role column user lean towards yes sample implementation sample implementation make several change add table platform team usergrouproles accesscontrols add enums grouptype grouprole objecttype subjecttype accesscontrolaction add platformid column organization remove role organizationid user seed database testable user orgs team rule existing project project test owned user platform organization team user assigned platform organization team role user platform organization team user platform organization team user platform organization user platform organization test following access control rule applied project set viewable organization project set viewable team project set viewable user project set viewable platform project set viewable platform well test looking project show user here expected user able view project project project user able view project project user able view project project user able view project following sql allow proposed solution tested sql create table platform uuid primary key null name text null alter table organization add platformid uuid reference platformsid create table team uuid primary key null organizationid uuid reference organizationsid null name text null create type grouptype enum platform organization team create type grouprole enum admin member create table usergrouproles uuid primary key null userid varchar reference usersid null grouptype grouptype null groupid uuid null grouprole grouprole null create type objecttype enum project scene create type subjecttype enum platform organization team user create type accesscontrolaction enum view edit create table accesscontrols uuid primary key null objecttype objecttype null objectid uuid null subjecttype subjecttype null subjectid text allowedaction accesscontrolaction null alter table user drop column role drop column organizationid create platform insert platform value eadaecebbefbadbe platform create platform insert platform value ddaedacbddcddb platform create organization platform insert organization value cbcbddcfbca organization eadaecebbefbadbe insert organization value fbbaef organization eadaecebbefbadbe create organization platform insert organization value dbcabfebdcc organization ddaedacbddcddb create team organization insert team value decaaffcceef cbcbddcfbca team create team organization insert team value bafdbadcac cbcbddcfbca team create team organization insert team value eebcebdfcbfe fbbaef team inserting fixture user insert user createdat modifiedat value bcbcdacde ecbcefbabfecfaf addbcdbec fccceaedefc make user admin platform insert usergrouproles value eddceccebee authadfbbcaebcfc platform eadaecebbefbadbe admin make user admin organization organization directly connected platform prototype insert usergrouproles value bbafbafbfdfdeaf authadfbbcaebcfc organization cbcbddcfbca admin make user admin team insert usergrouproles value edefcdfe authadfbbcaebcfc team decaaffcceef admin make user member platform insert usergrouproles value cbdbdaacfede bcbcdacde platform eadaecebbefbadbe member make user member organization insert usergrouproles value febfaafc bcbcdacde organization cbcbddcfbca member make user member team insert usergrouproles value dbadfbbbcedcff bcbcdacde team decaaffcceef member make user member platform insert usergrouproles value fffeaeffd ecbcefbabfecfaf platform eadaecebbefbadbe member make user member organization insert usergrouproles value ecfaaffc ecbcefbabfecfaf organization cbcbddcfbca member make user member team insert usergrouproles value edacdaebe ecbcefbabfecfaf team bafdbadcac member make user member platform insert usergrouproles value dabfcdbdeabeffead addbcdbec platform eadaecebbefbadbe member make user member organization insert usergrouproles value abdbcadfbbcad addbcdbec organization dbcabfebdcc member make user member platform insert usergrouproles value ecfabbcbdefdddb fccceaedefc platform ddaedacbddcddb member make user member organization insert usergrouproles value bbcaeeeaeee fccceaedefc organization fbbaef member insert accesscontrols value allow project viewed platform aabbffdc project cccdceddbebbcad platform eadaecebbefbadbe view allow project viewed organization caabdbacf project bafcabbacbaca organization cbcbddcfbca view allow project viewed team fdceadbaccebbdd project eecbfdadaeeb team decaaffcceef view allow project viewed platform dfbfbbeaeabfdefc project bccadedbafed platform ddaedacbddcddb view allow project viewed user dffbdcfecdeff project eecbfdadaeeb user addbcdbec view update project set name project idbafcabbacbaca update project set name project ideecbfdadaeeb update project set name project idcccdceddbebbcad update project set name project idbccadedbafed following sql template listing specific object user access take three parameter objecttype actiontype userid sql doesnt currently handle ownership since want display applied rule sql select distinct pid aclallowedaction aclsubjecttype aclsubjectid aclallowedaction pname usergrouproles ugr join accesscontrols acl aclobjecttype objecttype aclallowedaction actiontype aclsubjecttype aclsubjecttypetext ugrgrouptypetext aclsubjectidtext ugrgroupidtext aclsubjecttype user aclsubjectid ugruserid join project pidtext aclobjectidtext ugruserid userid test objecttype action type always project view respectively change userid see project user would able view', 'authenticationuser management raster foundry support following requirement regarding user authentication registration ability support oauth sso authentication beyond simple usernamepassword basic user management necessary resetting password deactivating user grouping user organization object layer tool user etc may limited scope user organization role within organization user able generate client token programmatic access revoked past entire user management workflow handled via web framework django extension user registration usually done adhoc basis utilizing extending thirdparty addons could executed successfully case issue first thirdparty addons needed rely additional integration like saml underdeveloped required either significant extension diving code lack documentation second new application required starting almost scratch inclusion additional service basic functionality instance user registration passwordmanagement required setting email serversservices asynchronous worker send email broker manage asynchronous worker third problem weve experienced past mixing authentication technique provided django session token create unintended difficulty isolating authentication problem another case would thirdparty service streamlines user management registration service mentioned often stormpath auth amazon cognito service benefit disadvantage documentation stormpath auth good documentation slight edge going auth number sdks especially frontend backend frameworkslanguages stormpaths documentation slightly harder navigate auths starter application example made putting together simple application much easier documentation cognito fairly dense difficult determine service terminology documentation unique amazon whereas stormpath auth share common language much literature already exists token authentication user management multitenancy handling multiple tenant raster foundry requirement stormpath auth good documentation handle respective system amazon cognito documentation isnt clear however another would continue manage roleslevels database would still able leverage service managing user password would also able easily control database constraint organization user storing organization role user metadata service seems somewhat risky data service get sync data modeled database third party integration stormpath auth fair amount thirdparty integration almost draw instance google facebook twitter etc auth also number integration service make attractive instance integrates well papertrail forward loginlogout information keep log event pricing little difficult compare pricing amongst service different metric determine price auths plan determined number monthly active user set feature provides free plan support active user basic feature satisfy requirement development possibly even early phase released product feature may influence switch paid plan social login free plan limited two customized email template basic plan start month active user pricing start add however pricing scale based number user correlate revenue may important stormpath pricing based number api call feature free plan feature auth free plan limited api call probably relevant user signing time month next plan month comparable feature paid plan auth api call user closest auth plan price signing time month amazon cognito far cheapest though fraction feature auth stormpath cognito free monthly user additional user reduced pricing user raster foundry thirdparty service auth manage user authentication better documentation sdks third party integration suggest reduce development time provide full set login user registration feature application additionally provides capability write custom javascript code execute authentication process utilizing auth two token type access token signed json web token jwt authenticate request api client application web native etc refresh token longlived revokable user generated token programmatic api access json web token jwt type token see rfc compact urlsafe mean representing claim transferred two party claim jwt encoded json object payload json web signature jws structure plaintext json web encryption jwe structure enabling claim digitally signed integrity protected message authentication code mac andor encrypted three part jwt header description type token algorithm sign body data includes token issuer user identifier audience expiration time issue time additional information wish include signature hmac signed shared secret data jwt verification jwt server achieved via verifying signature shared secret case mean additional call database limited compared traditional token authentication method time jwt long properly signed expired server extend expiration time resign expiration time jwt adjusted based user activity log user soon instance percent user make api call every minute percent make one least every minute would likely want set expiry little longer minute depending want handle token revocation andor forcing login period time likely want application level timeout token cannot reissued indefinitely additionally user decides reset password log completely may want set timestamp database check issue time ensure token issued timestamp refresh token refresh token opaque string get new jwt auth indefinite unless revoked tied client application token allow user set programmatic access application complicate sdkscommand line client somewhat client handle jwt expiration gracefully following diagram auth illustrates usage operate consequence choosing auth consequence first set frontend application auth authentication user signup second library web server validate signature jwt many third make encode user role organization database andor auth lastly come determination appropriate expiration time jwt risk involved adopting auth handle user management largest risk decide auth appropriate solution come plan exporting user support claim provide export user database request yet selfservice way likelihood coming determination like auth significant user low still possibility api call involved migrating another provider like stormpath seem prohibitive']"
3,225, Test Strategy Evolution,['Test Strategy Evolution'],"['test suite', 'testing framework', 'unit testing', 'unit test', 'testing', 'acceptance test', 'development', 'test run', 'test test', 'test']","['title follow test pyramid area productoperations tag test structure performance flakiness since beginning development shopware weve tried test much possible effort went writing integration endtoend test led two main issue performance test integration test degree slow nature perform lot step assert required condition test suite grown lifetime shopware take hour realtime executed serial flakiness test involve lot moving part test deterministic behave little differently every execution lead flakiness sometimes hard reproduce depends performanceload machine executing test flakiness combination number test performance complex test matrix merge train led distrust test suite caused lot hassle especially many merge request merged frustrating pipeline fail due flakiness commit following best practice test pyramid testing structure currently reversed pyramid many unit test test code base get closer ideal well cut test covered jest test better implemented php integration testsapi test consequence coverage decrease bug might slip might caught deleted test likely test deleted test feature whole mostly admin module crud operation test pyramid write unit test well delete test cover thing tested jest php integration test well write jest test basic stuff covered deleted test mostly crud stuff well add test actually test important feature end end quality well add high quality test well test thoroughly merging least time performance well refactor test require database reset test case well also reconsider moving playwright well reduce disable test retries fight performance creep well move test quarantine fast possible', 'separate integration package integration test user story write server side easi application decide structure testing suite complex computationally expensive access resource like call network access apis become cumbersome slow set run test include flip side mocking resource cause poor test coverage mimic production case important create testing strategy balance coverage runtime adr focus setting strategy balance resource driven integration test wide coverage unit test codebase important understand application layer pattern described truss engineering playbook driver purpose adr well focus breaking unit integration test wont try define content detail integration test likely happen endpointhandler level run production case unit test happen across codebase mock outside package focus individual function case considered dont distinguish unit integration test run together together effectively nothing integration test unit test reside testing suite run based integration test suite unit test flag testingshort case distinguish integration test testingshort running unit test short command line provides way separate execution different test however package dependency still linked move integration test separate package integration test would live integration package test could developed run separately still utilize short break apart execution would allow also break apart package dependency unit test would minimize import mock avoiding wide package surface integration test would pull production dependency mirror server setup outcome chosen move integration test separate package moving dependency wiring separate package allows unit test stay decoupled dependency may integration package existing package based production package structure route server example handler service model required pull package mimic interface dependency based production structure layered package approach push achieve integration test become synonymous wiring server dependency pro con dont distinguish unit integration test run together together early develop easy nothing lends towards bad unit testing habit including testing dependency integral test target writing unit test slow focusing api surface single responsibility principle testing suite quickly become slow clear coverage function integration unit test count coverage statistically unclear mock import run integration suite handler flag testingshort long test disabled developer workflow integration test live close unit counterpart increasing developer awareness test run functionpackage suite setupteardown happen suite level short flagging would required within individual test suite setup word short logic would sparse across test clear coverage function integration unit test count coverage statistically unclear mock import move integration test separate package integration test dependency clearly marked easily tested separate unit test dependency longer imported unit test short flagging happens suitepackage level test also run per directory dependency structure mirror production easier write unit test wider test surface optimize performance testing suite type benchmark easily testing suite slow requires developer awareness might moot current instruction testing service ignore integration test', 'adr test scope changelog initial draft add precision integration test add precision test proposed partially implemented abstract recent work sdk aimed breaking apart monolithic root module highlighted shortcoming inconsistency testing paradigm adr clarifies common language talking test scope proposes ideal state test scope adr module refactoring express desire sdk composed many independently versioned module adr app wiring offer methodology breaking apart intermodule dependency dependency injection described epic separate sdk module standalone module module dependency particularly complected test phase simapp key test fixture setting running test clear successful completion phase epic require resolution dependency problem epic unit testing module via mock thought gordian knot could unwound mocking dependency test phase module seeing refactors complete rewrite test suite discussion began around fate existing integration test one perspective ought thrown another integration test utility place sdks testing story another point confusion current state cli test suite xauth example code called integration test reality function end end test starting tendermint node full application epic rewrite simplify cli test identifies ideal state cli test mock address place end end test may sdk identify three scope testing unit integration end end seek define boundary shortcoming real imposed ideal state sdk unit test unit test exercise code contained single module xbank package client isolation rest code base within identify two level unit test illustrative journey definition lean heavily bdd book formulation section illustrative test exercise atomic part module isolation case might fixture setupmocking part module test exercise whole module function dependency mocked journey almost like integration test exercise many thing together still mock example journey illustrative test depinjects bdd style test show rapidly build many illustrative case demonstrating behavioral rule without much code maintaining high level readability example depinject table driven test example bank keeper test mock implementation accountkeeper supplied keeper constructor limitation certain module tightly coupled beyond test phase recent dependency report bank auth found total usage auth bank production code test tight coupling may suggest either module merged refactoring required abstract reference core type tying module together could also indicate module tested together integration test beyond mocked unit test case setting test case module many mocked dependency quite cumbersome resulting test may show mocking framework work expected rather working functional test interdependent module behavior integration test integration test define exercise relationship arbitrary number module andor application subsystem wiring integration test provided depinject helper code start running application section running application may tested certain input different phase application life cycle expected produce invariant output without much concern component internals type black box testing larger scope unit testing example clientgrpcquerytesttestgrpcquery test misplaced client test life cycle least runtime bank progress startup genesis query time also exercise fitness client query server without putting byte wire queryservicetesthelper example xevidence keeper integration test start application composed module keeper integration test suite one test suite exercise handleequivocationevidence contains many interaction staking keeper example integration suite app configuration may also specified via golang yaml statically dynamically limitation setting particular input state may challenging since application starting zero state may addressed good test fixture abstraction testing test may also brittle larger refactors could impact application initialization unexpected way harder understand error could also seen benefit indeed sdks current integration test helpful tracking logic error earlier stage appwiring refactors simulation simulation also called generative testing special case integration test deterministically random module operation executed running simapp building block chain specified height reached specific assertion made state transition resulting module operation error halt fail simulation since crisis included simapp simulation run endblockers end block module invariant violation also fail simulation module must implement appmodulesimulationweightedoperations define simulation operation note module implement may indicate gap current simulation test coverage module returning simulation operation auth evidence mint params separate binary runsim responsible kicking test managing life cycle limitation may take long time run minute per simulation timeouts sometimes occur apparent success without indication useful error message provided requiring developer run simulation locally reproduce test end end test exercise entire system understand close approximation production environment practical presently test located testsee rely testutilnetwork start inprocess tendermint node application built minimally possible exercise desired functionality sdk application required module test application developer advised application test limitation general limitation end end test orchestration compute cost scaffolding required start run prodlike environment process take much longer start run unit integration test global lock present tendermint code cause stateful startingstopping sometimes hang fail intermittently run environment scope test complected command line interface testing accept test scope identify following point scope app type mock unit none yes integration integration helper simulation minimal app minimal app valid sdk application developer test application full application instead minimal app unit test module must mocked unit test coverage illustrative test outnumber journey unit test unit test outnumber integration test unit test must introduce additional dependency beyond already present production code module unit test introduction per epic unit testing module via mock result near complete rewrite integration test suite test suite retained moved testsintegration accept resulting test logic duplication recommend improving unit test suite addition illustrative test integration test integration test shall located testsintegration even introduce extra module dependency help limit scope complexity recommended smallest possible number module application startup dont depend simapp integration test outnumber test simulation simulation shall minimal application usually via app wiring located xmodulenamesimulation test existing test shall migrated integration test removing dependency test network inprocess tendermint node ensure lose test coverage rest runner shall transition process tendermint runner powered docker via dockertest test exercising full network upgrade shall written cli testing aspect existing test shall rewritten network mocking demonstrated consequence positive test coverage increased test organization improved reduced dependency graph size module simapp removed dependency module intermodule dependency introduced test code removed reduced run time transitioning away process tendermint negative test logic duplication unit integration test transition test written dockertest may bit worse neutral discovery required transition dockertest discussion may useful test suite could run integration mode mocked tendermint fixture real tendermint many node integration fixture could quicker run fixures could battle hardening poc xgov completed progress unit test demonstrating bdd rejected observing strength bdd specification readability con cognitive load writing maintaining current consensus reserve bdd place sdk complex rule module interaction demonstrated straightforward low level test case continue rely table test level network mocking integration test still worked formalized']"
4,345, Release Process Standardization,['Release Process Standardization'],"['versioning', 'release note', 'repository', 'pull request', 'git', 'github', 'version', 'github action', 'workflow', 'deployment']","['new release process updating release process branch give flexibility term fixing issue release working fine happening current release process skip one version issue inside creating confusion putting maven central invalid version long discussion decided update branch release new process first order standardize project usi primis decided dev branch continuous development master branch release new version software process described stepbystep dev master dev branch create feature branch usually associated ticket work specific task developer finished task creates pull request feature branch back dev branch github everybody merged dev branch team want new release software user create release note prepare dev branch master branch everybody release note developer merge update version dev next snapshot snapshot snapshot master release software master branch developer update version software release candidate snapshot software corresponding new version pushed remote master branch trigger build release plan snowy everything going fine developer promote artifact scooby wwwdev release wwwdev developer send email biosamplesusers mailing list announce new software release ask people test error found developer make hotfix master update version software release various environment till wwwdev software wwwdev week everybody happy time make final release update version final version tag current commit github git tag push code tag remote repository release environment one snowy scooby wwwdev www release production done send email biosamplesannouncements mailin list announcing new release merge master branch back dev make sure dev also contains hotfixes done master consequence make possible avoid issue version software actually released production give flexibility team continue development dev branch release also make possible release faster way dev branch developer continue work without issue', 'release process based gitflow discussion httpsgithubcomesipfedscienceonschemaorgissues gitflowinspired release model track proposed change implementation associated architectural record people review guidance document online helpful explicit versioning release guidance document particular github user normally see master branch therefore reflect current stable release documentation rather confusing people inprogress proposed change yet released consequently gitflowinspired release model master branch always reflects current stable release develop branch merging finished proposal prepared release feature branch creating change implement specific proposal reflected architectural record change require formal via adr spelling correction grammatical rewording etc maintainer commit change directly develop branch contributor pull request directly develop branch feature branch really focused managing proposal discussion review adr maintainer make judgement call whether adr needed might convert contributed pull request feature branch determine adr needed release workflow propose described detail contributingmd document highlight master branch github repository always reflects current release develop branch development work extend guideline adrs minor change adr associated pull request develop branch reference issue number adr template merged develop branch approved good implementation adr develop branch merged master tagged create new release approved release pull request based separate feature branch appropriately named feature include issue addressed feature featurereleaseworkflow discussion proposed adr occur associated github issue agreement reached proposed change adr updated feature branch merged develop branch set feature targeted release complete review finished develop branch merged master branch tagged release release tag follow semantic versioning create associated release github allows file downloaded consequence people easily follow current stable release guideline github page community easily propose change pull request development branch minor change dont require happen easily directly develop clear linkage github issue adr process release process development core committers somewhat complicated people understand branching model doesnt affect thirdparty contributor contributor might accidentally submit pull request master branch retargeted maintainer develop branch merged', 'rapid release moving towards monorepo proposed deciders sync team bendk tarikeshaq lougeniac ddurst mhammond release management rvandermeulen release engineering jlorenzo firefox android csadilek firefox nish firefox desktop sylvestre dec feedback deadline jan problem statement application service currently long cycle time code change make release main adhoc schedule averaging release month consumer application take release also adhoc basis firefoxandroid take release fairly frequently firefoxios frequently desktop infrequently mean code change take relatively long time released public example release taken firefoxandroid new commits release ranged day old two main reason long cycle current release process requires good deal developer involvement normal release take couple hour developer monitor progress perform several manual step along way unreleased code breaking change developer also merge consumer application release ideal since developer often one wrote worse dev caused breaking change forgot create create leading multiple devs coordinating release release main also require extra work creating branch version backporting fix making patch release etc applicationservices team application team generally put low priority updating applicationservices version generally focus benefit waiting take release rather cost mozilla generally favor rapid release model long cycle time firefox desktop adopted rapid release decade ago mobile application similar release cadence current work move service towards rapid release although cost rapid release company weve decided willing accept application service consider moving towards rapid release model reasoning led team adopt rapid release also applies furthermore mobile team working increase release cadence even may depend applicationservices also increasing finally process moving towards single monorepo android code potential add code andor move mozcentral monorepo would require application service code taken consumer application monorepo soon merged reducing amount time release application take release brings closer monorepo world driver want get change merged released consumer application faster want simplify pushing new code application especially urgent bug fix want move towards future monorepo world mozilla moving towards rapid release mobile code willing accept overhead upgrading applicationservices frequently application repository considered keep current release process move rapid release process change appservices frequently merged consumer application phase update branching model release process jan feb update branch match firefoxandroid work main branch current nightly build releasesvn previous release automate nightly release main branch see discussion bottom document could work note regardless choose longer semverstyle version also following semver semantics releasing breaking change nightly cycle without bumping major version manually handle patch release releasev branch update version number firefoxversionn create github release branch avoid unreleased code branch release soon new code committed branch code always uplifted branch code merged beta branch already merged main released nightly code merged releasevn branch merged released releasevn branch continue maintain changelog new versioning main keep rolling log update header firefoxversion replaces current release process push change consumer create repository bump applicationservices version one release issue merging revert merged application revert breaking change applicationservices wait next nightly release merge implement script manually run monthly firefox merge script create releasevn branch main update version number final release version change vna create github release branch bump major version number main start new changelog header phase automate merges applicationservices consumer application march may create system running android desktop inprogress applicationservice branch needed confidence androidiosdesktop code resolve breaking change ready merge breaking change main branch every morning consumer application including mobile apps desktop create take current applicationservices code main set automerge system automatically merge phase merge issue application back change application get merge start merges morning set alert get sent syncbots test suite fails mean would identify test failed assign developer resolve issue expectation would developer fix thing time next nightly merge note currently automation android leverage existing code much possible perform process whenever new applicationservices version published releasevn branch applicationservices change breaking consumer corresponding consumer repos reviewed approved accidentally merge change applicationservices ready back change outcome move rapid release process change appservices frequently merged consumer application pro con keep current release process good doesnt require change workflow bad reason enumerated top document move rapid release process change appservices frequently merged consumer application good code get merged released frequently good dont spend time cutting release good uplift done much quicker good release process automated bad implement new automation bad breaking change cause automatic nightly merge fail application mean developer get relatively urgent task resolving thing however mitigated policy merging change sure taken consumer application good avoids current state affair main branch ready taken application mean current code getting tested complicated get change application including bugfixes bad breaking change require corresponding approved team merged applicationservices good thing work monorepo world adjusting workflow prepares future give insight may help guide monorepo migration good version number align closely rest mozilla good make easier know applicationservices version application current firefox release always latest applicationservicesx release mobile apps desktop good align better firefox process may make easier mozilla tool later example buildhub product detail nonnormative discussion impact team application team android desktop phase applicationservices version number style change may increase amount applicationservice version upgrade approve well want make sure application applicationservices release match release avoiding fenix applicationservices androidios team small increase desktop larger phase applicationservices approve especially start eventually able arrange get automerged release management handle tagging releasing version bumping applicationservices end nightly cycle handle patch release release nightly branch release engineering support applicationservices team developing automation believe proposed change simple enough applicationservices team handle nightly release adr mention making nightly release main branch two basic approach create new release create new version number github tag github release kotlinswift package etc update single release name like firefoxversiona nightlies dont change dont create github tag release republish existing kotlinswift package rather create new one plan follow second approach firefoxversiona version number follows team manage release main benefit first fit better existing tooling expect replace new tooling']"
5,467, data structure transformation proposal,['data structure transformation proposal'],"['database', 'schema', 'sql', 'metadata', 'implementation', 'data', 'information', 'structure', 'migration', 'json']","['adr supporting nosql backends jean cochrane open source fellow summer grout currently tightly coupled postgresql dependency postgres powerful database engine also difficult deploy potentially serious barrier adoption grout make grout even easier would like support nosql backends however nosql backend feasible must provide support key grout function storing geospatial data querying geospatial data querying complex json data including nested search find value baz infoo bar baz compound rule foo bar baz optional jsonschema support addition nosql backends considered based much value bring project including easy backend configure deploy cheap free document look different nosql backends including mongodb couchdb google cloud firestore aws dynamodb jsonbin eliminating cannot support basic requirement grout evaluate cloud provider finally consider value nosql backend would bring project backends mongodb mongodb opensource nonrelational database store data jsonlike format translates json object document organized collection here score requirement geospatial data mongodb store arbitrary geojson meaning point polygon supported geospatial query mongodb support geospatial querying intersection within near query query make geojson object comparison operation querying json run query nested field support condition support logical condition jsonschema support mongodb support jsonschema validating create update operation draft supported limitation documentation doesnt describe limitation however couchdb couchdb opensource nonrelational database similar mongodb focus supporting highlydistributed database data stored jsonlike document database queryable rest api geospatial support provided official plugin geocouch documentation great distributed github new tagged release since geospatial query geocouch save point return bounding box request querying json query capability powerful quite esoteric thirdparty javascript api pouchdb available jsonschema support external reference possible define validation function create update operation could hack together schema solution nothing come box google cloud firestore google cloud firestore proprietary nonrelational database service provided google data model similar mongodb object stored document inside collection currently beta geospatial support firestore type latlng coordinate presumably theyre stored webmercator there much information firestore support polygon querying geospatial data seems possible cant find documentation blog post issue querying json firestore serious limitation cannot query across multiple collection cannot support range filter multiple field cannot support logical query reccommends performing multiple query merging clientside may work case jsonschema support result search define validation pattern security rule firestores idiomatic security rule syntax aws dynamodb dynamodb proprietary nonrelational database provided service amazon web service support keyvalue documentbased data storage object stored item row queried primary key designed easily integrate aws product geospatial support dynamodb support geohashes point geometry support polygon geospatial query relies thirdparty java library sparsely documented allows searching coordinate within radius box querying json limitation query search item attached primary key query retrieve maximum data raise question image true aws form query doc slog decipher result confident fully understand capability jsonschema support schema validation whatsoever jsonbin jsonbin jsonstoreasaservice allows developer easily store retrieve json data rest api sense like serverless json datastore requires configuration builtin permission token authentication system main focus simplicity easeofuse geospatial support since jsonbin store document pure json geojson inherently supported geospatial query querying product roadmap currently supported querying json querying product roadmap currently supported jsonschema support mention validation api documentation provider based research summarized mongodb backend considered support project requirement considered cloud provider mongodb two main cloud provider mongodb mongodb atlas hosting service provided mongodb mlab thirdparty service mlab read argument favor mlab see mlabs account better mongodb atlas pro free nonexpiring sandbox account storage shared hosting variable ram integrates aws quick easy set con provided thirdparty lag behind current release mongodb however available sandbox account july mongodb atlas read argument favor mongodb atlas see mongodb atlas account better mlab pro provided mongodb team integrates aws slightly complicated set still easy relative hosting database integrates mongodb stitch serverless backendasaservice mongodb application con free tier come major limitation idle instance may terminated backup max throughput writessec data transfer limit gbweek value two main goal supporting nosql backend include making grout easier configure deploy abstracting data model could applied backends future ill look goal detail making grout easier configure deploy pro mongodb slightly easier configure postgres however still requires configuration developer con database server still must deployed interact database mean developer must still deploy database database server addition frontend mongodb offer serverless framework mongodb stitch expose api queried client side serverless pattern like would substantial improvement existing development framework moving direction would require grout nosql backend tightly coupled certain provider mongodb stitch would introduce another layer integration would slow development process overall score mediumlow abstracting data model pro mongodb would let represent data model jsonschema validate backend would big win generalizability con jsonschema widely supported among nosql provider according research overall score medium based research recommend move forward nosql backend build top mongodb promising work done many nonrelational database mongodb currently nosql database satisfies project requirement mongodb satisifes project requirement however still uncertain whether value proposition strong enough prioritize work seems mongodb present strong value proposition backend component eliminated completely infrastructure provisioning otherwise deploying nosql database requires essentially amount work mongodb technically possible atlas stitch serverless stack however choosing stack require integrating two separate service making work much complex introducing undesirable coupling application review', 'architecture record database migration general arachnes philosophy embrace concept immutability reproducibility rather changing something replace something new usually simplifies mental model reduces number variable reducing way thing wrong one area approach cant work administering change production database database must stable existence across time cant throw away data every time want make change yet change database happen data model change new field added entity relationship refactored challenge provide way provide measured safe reproducible change across time also compatible arachnes target defining describing relevant part application including data model therefore schema configuration compounding challenge build system define concrete schema different type database based common data model chimera described adr prior art several system already exist best known probably rail active record migration oriented around making schema change relational database another solution interest liquibase system reifies database change data explicitly applies relation database scenario variety user story accomodate example include new developer project want create local database work current head codebase local development responsible production deployment project team new software version ready requires new field added database new code run want set staging environment exact mirror current production system fellow developer merging branch different feature made different change data model sure compatible merge recognize made mistake earlier development stored currency value floating point number create new column database fixedpoint type copy existing value rounding logic youve agreed domain expert chimera explicitly define concept migration reify migration entity configuration migration represents atomic set change schema database given database instance either migration logically applied hasnt migration unique expressed namespacequalified keywords every migration one parent migration except single special initial migration parent migration may applied database unless parent already applied migration also signature signature checksum actual content migration applied database whether txdata datomic string sql ddl json string etc ensure migration changed already applied persistent database adapter responsible exposing implementation migration accompanying config dsl appropriate database type chimera adapter must additionally satisfy two runtime operation hasmigration take signature particular migration return true migration successfully applied database implies database must migration aware store idssignatures migration already applied migrate given specific migration run migration record migration applied migration type four basic type migration native migration instance migration type directly implemented database adapter specific type example native migration sql database would implemented primarily via sql string native migration adapter appropriate type chimera migration define migration chimera entityattribute data model abstract work multiple different type adapter chimera migration supported chimera adapter sentinel migration coordinate manual change existing database code requires always fail automatically apply existing database database admin must add migration record explicitly perform manual migration task note actually implementing deferred needed structure usage migration may one parent migration form directed acyclic graph appropriate combine well arachnes composability model module may define sequence migration build data model extending module branch point build data model share structure module may also depend upon chain migration specified two dependent module indicate requires configuration chimera database component may depend number migration component migration ancestor form database definition represent complete schema concrete database instance far chimera concerned database component started connects underlying data store verifies specifies migration applied fails start guarantee safety arachne system given application simply start compatible specified database parallel migration create opportunity problem two migration dependency relatinship parallel migration operation incompatible would yield different result depending order applied operation conflict applying database could result error nondeterministic behavior parallel migration chimera migration arachne aware internal structure detect conflict refuse start run migration actually touch database unfortunately arachne cannot detect conflicting parallel migration migration type responsibility application developer ensure parallel migration logically isolate coexist database without conflict therefore advisable general public module chimera migration addition making broadly compatible possible also make tractable application author avoid conflicting parallel migration since worry create chimera migration entity type one drawback chimera migration cannot see full entity type defined one place reading config dsl script cannot avoided real living application entity defined time many different migration application grows chimera migration contains fragment full data model however pose usability problem developer machine consumption many reason developer module view query entity type model point time snapshot rather series incremental change support case chimera module creates flat entity type model database rolling individual chimera entity definition form single full data structure graph canonical entity model render schema diagram user queried module applying migration invoke adapter migrate function defined since different team wish different way possibility include application call migrate every time started advisable database excellent support transactional atomic migration scenario developer worry deploying code devops team manually invoke migrate function new configuration prior deployment continuousdeployment setup server could run battery test clone production database invoke migrate automatically pas development team inspect set migration generate set native sql txdata statement handoff dedicated dba team review commit prior deployment database without migration every application want chimera migration system situation migration may good fit include prefer manage database schema working existing database predates arachne work database administered separate team however still may wish utilize chimera entity model leverage module define chimera migration support chimera allows configuration designate database component assertonly assertonly database never migration applied require database track concept migration instead inspect chimera entity model rolling declared migration assert database already compatible schema installed everything start normal component fails start course schema chimera expects likely exact match present database accomodate chimera adapter defines set override configuration entity accompanying dsl user apply override change behavior mapping chimera query store data note chimera override incompatible actually running migration assertonly database migration rollback generalized rollback migration intractable given variety database chimera intends support one following strategy instead development testing constantly creating throwing away new database back database running migration cant afford downtime data loss associated restoring backup manually revert change unwanted migration proposed consequence user define data model configuration data model automatically reflected database data model change explicitly modeled across time migration entity type schema element represented arachne apps configuration given configuration database built migration reliably reproduced configuration migration contain entire perfectly reproducible history database migration optional chimera data model existing database', 'publishing time change history content published govuk metadata indicate history content comprised time indicate first iteration content published firstpublishedat time content last changed meaningful way publicupdatedat change history record change note publishing time meaningful change convention first item change history govuk content provided publishing application without publisher input item change note first published time content initially published govuk set metadata field content publisher relied publishing api automatically setting time field changenote model model store record major change published govuk collate build change history meant content publisher needed store firstpublishedat publicupdatedat full change history needed store change note edition concern continued usage publishing api time change history system would lead discrepancy migrating content whitehall content publisher led review system problem relate migration others broader problem identified accurate time whitehall migrated content content migrated whitehall content publisher want time associated content remain whitehall complicates passing two time value represent first publishing time aforementioned firstpublishedat field deprecated time value firstpublicat preferentially firstpublishedat rendering content content publisher populate firstpublicat field content item notably older one predate whitehall sending firstpublishedat value different value time consequence matching migrated content would show different first publishing time migrating change note whitehall whitehall doesnt make publishing api changenote model build change history instead whitehall creates change history content includes updating content publishing api consequence data stored changenote model publishing api whitehall content cannot relied upon guarantee accuracy also expectation data accurate many piece whitehall content created changenote model introduced publishing api content publisher migrate change history whitehall content would ensure entire change history sync publishing api removingediting change note relatively common task govuk line support engineer requested remove modify change note published piece content resolve whitehall developer would modify past edition piece content either remove amend change note content publisher ideal modify past edition edition represents snapshot content published govuk edits invalidate accuracy snapshot backdating content backdating feature content publisher support backdating first edition piece content subsequent one allowed whitehall publishing api mean change initial item change history content thus would cause change history inconsistent firstpublishedat time consequence publisher make mistake backdating forget backdate content publisher cannot rectify without creating new document requesting support developer approach considered correct whitehall data publishing api considered audit syncing exercise time change history whitehall publishing api would involve either large data modification exercise creating new endpoint publishing api allow importing data positive approach could mean content publisher could continue rely publishing api time change history management without needing data stored content publisher negative would guarantee data would remain accurate work complete whitehall continues operate would easy check whether data still accurate migration content publisher store also didnt allow improvement removingediting change history allowing backdating past first edition store time change history content publisher sync publishing api considered changing content publisher similar approach whitehall canonical source publishing time change history time content publisher content updated publishing api would sent time change history approach offer greater degree control data allows content publisher enhance backdating feature provide support change history removalediting also offer higher degree compatibility whitehall migrating content due similar model negative increase content publisher responsibility storing time change history also mean prior publishing content content publisher would update timestamps publishing api add additional api call adapt publishing api support altering change history considered modifying publishing api allow altering change history would involve change note particular value could identify request positive allows change note treated separate concept content compared approach allows reduction duplicate data shared different edition content negative requires additional development complexity publishing api potential migration needed existing content also presented number complication whether change history would part draftlive workflow since content already concern decided take approach storing time change history content publisher felt pragmatic approach didnt require additional development publishing api compatible whitehall approach change history increased control offered also consistent potential future feature allowing change history modified via user interface concluded recommended approach change history publishing api ultimately flawed reconsidered recommendation concern publishing application exception specialist publisher publishing api syncing target rather store publishing history reversal unrealised intention publishing api canonical store content history change history lone outlier user visible data govuk publishing application cannot fix case issue publishing api changenote model designed appendonly therefore incompatible ability change time content marked first published change history disagree value resolve mistake consequence content publisher store information needed present firstpublishedat publicupdatedat change history publishing api content publisher considered canonical source data data reflected non user editable data firstpublishedat document model publishedat edition model reflects time content publisher published content addition existing changenote field metadatarevision also changehistory field store json representation previous change history entry content changehistory field store collection timestamps note include fixed first published change note automatically appended start change history time based either firstpublishedat backdating value also include current change note edition become aspect change history content published update type publishing time fixed noted changenote changehistory field risk confusion developer however seemed preferable complicating changehistory field data yet historical initially mean edit item change history via rake task developer perform provide mean add edit delete item eventually may choose build web edit expand integritychecker system content publisher verify time change history sent publishing api match already stored whitehall content ensure remain consistent migration publishing api previously deprecated changehistory field favour changenote field remove deprecation reflect changehistory field still valid approach active']"
6,165, Module Communication Decisions,['Module Communication Decisions'],"['cosmos sdk', 'sdk', 'implement', 'protocol', 'module', 'implementation', 'node', 'interface', 'proposal', 'cosmos']","['adr core module api changelog first draft first draft update partially implemented abstract new core api proposed way develop cosmossdk application eventually replace existing appmodule sdkcontext framework set core service extension interface core api aim simpler extensible stable current framework enable deterministic event query support event listener adr protobufbased intermodule communication client historically module exposed functionality framework via appmodule appmodulebasic interface following shortcoming appmodule appmodulebasic defined registered counterintuitive apps implement full interface even part dont although workarounds interface method depend heavily unstable third party dependency particular comet legacy required method littered interface far long order interact state machine module needed combination thing get store key app call method sdkcontext contains full set capability available module isolating state machine functionality sdkcontext set functionality available module tightly coupled type change upstream dependency comet new functionality desired alternate store type change impact sdkcontext consumer basically module also module receive contextcontext convert sdkcontexts nonergonomic unwrapping function breaking change interface one imposed thirdparty dependency like comet side effect forcing module ecosystem update lockstep mean almost impossible version module run different version sdk different version another module lockstep coupling slows overall development within ecosystem cause update component delayed longer would thing stable loosely coupled core api proposes set core apis module rely interact state machine expose functionality designed principled way tight coupling dependency unrelated functionality minimized eliminated apis longterm stability guarantee sdk framework extensible safe straightforward way design principle core api follows everything module want interact state machine service service coordinate state via contextcontext dont try recreate bag variable approach sdkcontext independent service isolated independent package minimal apis minimal dependency core api minimalistic designed longterm support lts runtime module implement core service defined core api handle module functionality exposed core extension interface noncore andor nonlts service exposed specific version runtime module module following design principle includes functionality interacts specific nonstable version third party dependency comet core api doesnt implement functionality defines type stable api compatibility guideline followed httpsgodevblogmodulecompatibility runtime module module implement core functionality composing abci app currently handled baseapp modulemanager runtime module implement core api intentionally separate core api order enable parallel version fork runtime module possible sdks current tightly coupled baseapp design still allowing high degree composability compatibility module built core api dont know anything version runtime baseapp comet order compatible module core mainline sdk could easily composed forked version runtime pattern design intended enable matrix compatible dependency version ideally given version module compatible multiple version runtime module compatible module allow dependency selectively updated based battletesting conservative project may want update dependency slower fast moving project core service following core service defined core api valid runtime module implementation provide implementation service module via dependency injection manual wiring individual service described bundled convenient appmoduleservice bundle service simplicity module declare dependency single service store service store service defined cosmossdkiocorestore package generic storekvstore interface current sdk kvstore interface store key refactored store service instead expecting know store invert pattern allow retrieving store generic three store service three type currently supported store regular kvstore memory transient type kvstoreservice interface openkvstorecontextcontext kvstore type memorystoreservice interface openmemorystorecontextcontext kvstore type transientstoreservice interface opentransientstorecontextcontext kvstore module service like func msgserver sendctx contextcontext msg typesmsgsend typesmsgsendresponse error store kkvstoresvcopenkvstorectx current runtime module implementation module explicitly name store key rather runtime module choose appropriate name module request type store dependency injection manual constructor event service event service defined cosmossdkiocoreevent package event service allows module emit typed legacy untyped event package event type service interface emitprotoevent emits event represented protobuf message described adr caller assume event may included consensus event must emitted deterministically adding removing changing event considered statemachine breaking emitprotoeventctx contextcontext event protoifacemessagev error emitkvevent emits event based event kvpair attribute event part consensus adding removing changing event statemachine breaking change emitkveventctx contextcontext eventtype string attrs kveventattribute error emitprotoeventnonconsensus emits event represented protobuf message described adr without including blockchain consensus event part consensus adding removing changing event statemachine breaking change emitprotoeventnonconsensusctx contextcontext event protoifacemessagev error typed event emitted emitproto assumed part blockchain consensus whether part block app hash left runtime specify event emitted emitkvevent emitprotoeventnonconsensus considered part consensus cannot observed module clientside add event patch release method logger logger cosmossdkiolog must supplied depinject made available module via depinjectin module follow current pattern sdk adding module name type moduleinputs struct depinjectin logger loglogger func providemodulein moduleinputs moduleoutputs keeper keepernewkeeper inlogger func newkeeperlogger loglogger keeper return keeper logger loggerwithlogmodulekey xtypesmodulename core appmodule extension interface module provide core service runtime module via extension interface built top cosmossdkiocoreappmoduleappmodule tag interface tag interface requires two empty method allow depinject identify implementers depinjectonepermodule type app module implementation type appmodule interface depinjectonepermoduletype isappmodule dummy method tag struct implementing appmodule isappmodule core extension interface defined cosmossdkiocore supported valid runtime implementation msgserver queryserver registration msgserver queryserver registration done implementing hasservices extension interface type hasservices interface appmodule registerservicesgrpcserviceregistrar cosmosmsgvservice protobuf required msg service serviceregitrar register msg query service genesis genesis handler function defaultgenesis validategenesis initgenesis exportgenesis specified genesissource genesistarget interface abstract genesis source may single json object collection json object efficiently streamed genesissource source genesis data json format may abstract single json object separate file field json object streamed module open separate ioreadcloser field required field represent array efficiently streamed data field function return nil nil important caller close reader done type genesissource funcfield string ioreadcloser error genesistarget target writing genesis data json format may abstract single json object json separate file streamed module open separate iowritecloser field prefer writing field array possible support efficient iteration important caller closer writer check error done expected stream json data written writer type genesistarget funcfield string iowritecloser error genesis object given module expected conform semantics json object field json object read written separately support streaming genesis orm collection support streaming genesis module framework generally write manual genesis code support genesis module implement hasgenesis extension interface type hasgenesis interface appmodule defaultgenesis writes default genesis module target defaultgenesisgenesistarget error validategenesis validates genesis data read source validategenesisgenesissource error initgenesis initializes module state genesis source initgenesiscontextcontext genesissource error exportgenesis export module state genesis target exportgenesiscontextcontext genesistarget error pre blocker module functionality run beginblock implement haspreblocker interface type haspreblocker interface appmodule preblockcontextcontext error begin end blocker module functionality run transaction begin blocker transaction end blocker implement hasbeginblocker andor hasendblocker interface type hasbeginblocker interface appmodule beginblockcontextcontext error type hasendblocker interface appmodule endblockcontextcontext error beginblock endblock method take contextcontext module dont comet information blockinfo eliminate dependency specific comet version module comet block header andor return validator update specific version runtime module provide specific functionality interacting specific version comet supported order beginblock endblock initgenesis send back validator update retrieve full comet block header runtime module specific version comet could provide service like type validatorupdateservice interface setvalidatorupdatescontextcontext abcivalidatorupdate header service defines way get header information block information generalized implementation type service interface headerinfocontextcontext info type info struct height int height return height block hash byte hash return hash block header time timetime time return time block chainid string chainid return chain block comet service provides way get comet specific information type service interface getcometinfocontextcontext info type cometinfo struct evidence abcimisbehavior misbehavior return misbehavior block validatorshash return hash validators comet hash next validators validatorshash byte proposeraddress byte proposeraddress return address block proposer decidedlastcommit abcicommitinfo decidedlastcommit return last commit info user would like provide module information would implement another service like type rollkit interface know type change comet level also limited set module actually functionality intentionally kept core keep core limited necessary minimal set stable apis remaining part appmodule current appmodule framework handle number additional concern arent addressed core api include gas block header upgrade registration gogo proto amino interface type cobra query command grpc gateway crisis module invariant simulation additional appmodule extension interface either inside outside core specified handle concern case gogo proto amino interface registration generally happen early possible initialization adr app wiring protobuf type registration happens dependency injection although could alternatively done dedicated provider grpc gateway registration probably handled runtime module core api shouldnt depend grpc gateway type already older version possible framework registration automatically future runtime module probably provide sort specific type registration type grpcgatewayinfo struct handler grpcgatewayhandler type grpcgatewayhandler funcctx contextcontext mux runtimeservemux client queryclient error module return provider func providegrpcgateway grpcgatewayinfo return grpcgatewayinfo handler handler typesregisterqueryhandlerclient crisis module invariant simulation subject potential redesign managed type defined crisis simulation module respectively extension interface cli command provided via cosmossdkioclientv module autocli framework example usage example setting hypothetical foo module orm state management genesis type keeper struct ormmoduledb evtsrv eventservice func keeper registerservicesr grpcserviceregistrar foovregistermsgserverr foovregisterqueryserverr func keeper beginblockcontextcontext error return nil func provideappconfig foomodulevmodule evtsvc eventeventservice ormmoduledb keeper appmoduleappmodule keeperdb evtsvc evtsvc return runtime compatibility version core module define static integer var cosmossdkiocoreruntimecompatibilityversion minor version indicator core module accessible runtime correct runtime module implementation check compatibility version return error current runtimecompatibilityversion higher version core api runtime version support new feature added core module api runtime module required support version incremented runtime module initial runtime module simply created within existing githubcomcosmoscosmossdk module runtime package module small wrapper around existing baseapp sdkcontext module manager follow cosmos sdks existing based versioning move semantic versioning well runtime modularity new officially supported runtime module created cosmossdkioruntime prefix supported consensus engine semanticallyversioned module created runtime implementation consensus engine example cosmossdkioruntimecomet cosmossdkioruntimecometv cosmossdkioruntimerollkit etc runtime module attempt semantically versioned even underlying consensus engine also runtime module also first class cosmos sdk module protobuf module config type new semantically versioned module config type created runtime module correspondence module module config type practice followed every semantically versioned cosmos sdk module described adr app wiring currently githubcomcosmoscosmossdkruntime protobuf config type cosmosappruntimevalphamodule standalone comet runtime dedicated protobuf module config type cosmosruntimecometvmodule release comet runtime cosmossdkioruntimecometv corresponding cosmosruntimecometvmodule protobuf type order make easier support different consensus engine support core module functionality described adr common module created shared runtime component easiest runtime component share initially probably messagequery router intermodule client service register event router common runtime module created initially cosmossdkioruntimecommon module new architecture implemented main dependency cosmos sdk module would cosmossdkiocore module able supported consensus engine extent explicitly depend consensus engine specific functionality comet block header app developer would able choose consensus engine want importing corresponding runtime module current baseapp would refactored cosmossdkioruntimecomet module router infrastructure baseapp would refactored cosmossdkioruntimecommon support adr eventually dependency githubcomcosmoscosmossdk would longer required short module would depend primarily cosmossdkiocore cosmossdkioruntimeconsensusengine would implement cosmossdkiocore functionality consensus engine additional piece would resolved part architecture runtimes relate server likely would make sense modularize current server architecture runtime even based consensus engine besides comet mean eventually comet runtime would encapsulate logic starting comet abci app testing mock implementation service provided core allow unit testing module without needing depend particular version runtime mock service allow test observe service behavior provide nonproduction implementation instance memory store mock store integration testing mock runtime implementation provided allows composing different app module together testing without dependency runtime comet consequence backwards compatibility early version runtime module aim support much possible module built existing appmodulesdkcontext framework core api widely adopted later runtime version may choose drop support support core api plus runtime module specific apis like specific version comet core module strive remain semantic version long possible follow design principle allow strong longterm support lts older version sdk support module built core adaptor convert wrap core appmodule implementation implementation appmodule conform version sdks semantics well providing service implementation wrapping sdkcontext positive better api encapsulation separation concern stable apis framework extensibility deterministic event query event listener intermodule msg query execution support explicit support forking merging module version including runtime negative neutral module refactored api replacement appmodule functionality still defined followup type registration command invariant simulation take additional design work discussion gas block header upgrade registration gogo proto amino interface type cobra query command grpc gateway crisis module invariant simulation reference adr protobufbased intermodule communication adr app wiring adr orm adr public key address keeping module compatible', 'adr semver compatible sdk module changelog first draft draft abstract order move cosmos sdk system decoupled semantically versioned module composed different combination staking bank distribution reassess organize api surface module avoid problem semantic import versioning circular dependency adr explores various approach take addressing issue fair amount desire community semantic versioning sdk significant movement splitting sdk module standalone module ideally allow ecosystem move faster wont waiting dependency update synchronously instance could version core sdk compatible latest release cosmwasm well different version staking sort setup would allow early adopter aggressively integrate new version allowing conservative user selective version theyre ready order achieve solve following problem way semantic import versioning siv work moving siv naively actually make harder achieve goal circular dependency module broken actually release many module sdk independently pernicious minor version incompatibility introduced correctly evolving protobuf schema without correct unknown field filtering note following discussion assumes proto file versioning state machine versioning module distinct proto file maintained nonbreaking way something like buf breaking ensure change backwards compatible proto file version get bumped much frequently might maintain cosmosbankv many version bank module state machine state machine breaking change common ideally wed want semantically version module xbankv xbankv etc problem semantic import versioning compatibility consider module foo defines following msgdosomething weve released state machine module examplecomfoo protobuf package foov message msgdosomething string sender uint amount service msg dosomethingmsgdosomething return msgdosomethingresponse consider make revision module add new condition field msgdosomething also add new validation rule amount requiring nonzero following semantic versioning release next state machine version foo examplecomfoov protobuf revision package foov message msgdosomething string sender amount must nonzero integer uint amount condition optional condition thing since revision condition condition approaching naively would generate protobuf type initial version foo examplecomfootypes would generate protobuf type second version examplecomfoovtypes let say module bar talk foo keeper interface foo provides type fookeeper interface dosomethingmsgdosomething error scenario backward compatibility newer foo older bar imagine chain foo bar want upgrade foov bar module upgraded foov case chain able upgrade foov bar upgraded reference examplecomfootypesmsgdosomething examplecomfoovtypesmsgdosomething even bar usage msgdosomething changed upgrade impossible without change examplecomfootypesmsgdosomething examplecomfoovtypesmsgdosomething fundamentally different incompatible structs type system scenario forward compatibility older foo newer bar let consider reverse scenario bar upgrade foov changing msgdosomething reference examplecomfoovtypesmsgdosomething release barv change chain want chain however decided think change foov risky itd prefer stay initial version foo scenario impossible upgrade barv without upgrading foov even barv would worked fine foo changing import path msgdosomething meaning barv doesnt actually new feature foov way semantic import versioning work locked either foo bar foov barv cannot foo barv foov bar type system doesnt allow even version module otherwise compatible naive mitigation naive approach fixing would regenerate protobuf type examplecomfoovtypes instead update examplecomfootypes reflect change needed adding condition requiring amount nonzero could release patch examplecomfootypes update foov change state machine breaking requires changing validatebasic method reject case amount zero add condition field rejected based adr unknown field filtering adding change patch actually incorrect based semantic versioning chain want stay foo importing change incorrect problem circular dependency none approach allow foo bar separate module reason foo bar depend different way instance cant foo import bartypes bar import footypes several case circular module dependency sdk staking distribution slashing legitimate state machine perspective without separating api type somehow would way independently semantically version module without mitigation problem handling minor version incompatibility imagine solve first two problem scenario barv want msgdosomethingcondition foov support barv work foo set condition nonnil value foo silently ignore field resulting silent logic possibly dangerous logic error barv able check whether foo dynamically could choose condition foov available even barv able perform check however know always performing check properly without sort frameworklevel unknown field filtering hard know whether pernicious hard detect bug getting app clientserver layer adr intermodule communication may needed solution approach separate api state machine module one solution first proposed httpsgithubcomcosmoscosmossdkdiscussions isolate protobuf generated code separate module state machine module would mean could state machine module foo foov could type api module say fooapi fooapi module would perpetually accept nonbreaking change would allow module compatible either foo foov long intermodule api depends type fooapi would also allow module foo bar depend could depend fooapi barapi without foo directly depending bar vice versa similar naive mitigation described except separate type separate module could break circular module dependency problem naive solution otherwise could rectify removing state machine breaking code api module validatebasic interface method embedding correct file descriptor unknown field filtering binary migrate interface method api type handler solve remove interface implementation generated type instead handler approach essentially mean given type sort resolver allows resolve interface implementation type sdkmsg authzauthorization example func keeper dosomethingmsg msgdosomething error var validatebasichandler validatebasichandler err kresolverresolvevalidatebasichandler msg err nil return err err validatebasichandlervalidatebasic case method sdkmsg could replace declarative annotation instance getsigners already replaced protobuf annotation cosmosmsgvsigner future may consider sort protobuf validation framework like httpsgithubcombufbuildprotocgenvalidate cosmosspecific replace validatebasic pinned filedescriptors solve state machine module must able specify version protobuf file built instance api module foo upgrade foov original foo module still copy original protobuf file built adr unknown field filtering reject msgdosomething condition set simplest way may embed protobuf filedescriptors module filedescriptors runtime rather one built fooapi may different buf build embed build script probably come solution embedding filedescriptors module fairly straightforward potential limitation generated code one challenge approach place heavy restriction api module requires state machine breaking code api module would generated protobuf file probably control code generation done risk aware instance code generation orm future could contain optimization state machine breaking would either ensure carefully optimization arent actually state machine breaking generated code separate generated code api module state machine module mitigation potentially viable api module approach require extra level care avoid sort issue minor version incompatibility approach little address potential minor version incompatibility requisite unknown field filtering likely sort clientserver routing layer check adr intermodule communication required make sure done properly could allow module perform runtime check given msgclient func keeper callfoo error kintermoduleclientminorrevisionkfoomsgclient kfoomsgclientdosomethingmsgdosomethingcondition else unknown field filtering adr router would protoreflect api ensure field unknown receiving module set could result undesirable performance hit depending complex logic approach change generated code alternate approach solving versioning problem change protobuf code generated move module mostly completely direction intermodule communication described adr paradigm module could generate type internally including api type module talk module via clientserver boundary instance bar talk foo could generate version msgdosomething barinternalfoovmsgdosomething pas intermodule router would somehow convert version foo foointernalmsgdosomething currently two generated structs protobuf type cannot exist binary without special build flag see httpsdevelopersgooglecomprotocolbuffersdocsreferencegofaqfixnamespaceconflict relatively simple mitigation issue would set protobuf code register protobuf type globally generated internal package require module register type manually applevel level protobuf registry similar module already interfaceregistry amino codec module adr message passing naive nonperformant solution converting barinternalfoovmsgdosomething foointernalmsgdosomething would marshaling unmarshaling adr router would break needed expose protobuf type keeper interface whole point try keep type internal dont end import version incompatibility weve described however issue minor version incompatibility unknown field filtering sticking keeper paradigm instead adr may unviable begin performant solution could maybe adapted work keeper interface would expose getters setter generated type internally store data memory buffer could passed one implementation another zerocopy way example imagine protobuf api getters setter exposed msgsend type msgsend interface protomessage getfromaddress string gettoaddress string getamount vbetacoin setfromaddressstring settoaddressstring setamountvbetacoin func newmsgsend msgsend return msgsendimplmemorybuffers hood msgsend could implemented based raw memory buffer way capn proto flatbuffers could convert one version msgsend another without serialization zerocopy approach would added benefit allowing zerocopy message passing module written language rust accessed ffi could also make unknown field filtering intermodule communication simpler require new field added sequential order checking field set also wouldnt issue state machine breaking code generated type generated code state machine would actually live state machine module depending interface type protobuf anys language however may still desirable take handler approach described approach either way type implementing interface would still registered interfaceregistry would way retrieve via global registry order simplify access module adr public api module maybe even one remotely generated buf could client module instead requiring generate client type internally big downside approach requires big change people protobuf type would substantial rewrite protobuf code generator new generated code however could still made compatible googlegolangorgprotobufreflectprotoreflect api order work standard golang protobuf tooling possible naive approach marshalingunmarshaling adr router acceptable intermediate solution change code generator seen complex however since module would likely migrate adr anyway approach might better approach dont address issue solution seen complex also decide anything explicit enable better module version compatibility break circular dependency case developer confronted issue described require dependency update sync attempt adhoc potentially hacky solution one approach ditch semantic import versioning siv altogether people commented siv changing import path foov foov etc restrictive optional golang maintainer disagree officially support semantic import versioning could however take contrarian perspective get flexibility xbased versioning basically forever module version compatibility could achieved gomod replace directive pin dependency specific compatible version instance knew foo compatible bar could replace directive gomod stick version foo bar want would work long author foo bar avoid incompatible breaking change module developer choose semantic import versioning attempt naive solution described would also special tag replace directive make sure module pinned correct version note however adhoc approach would vulnerable minor version compatibility issue described unless unknown field filtering properly addressed approach avoid protobuf generated code public apis approach would avoid protobuf generated code public module apis would help avoid discrepancy state machine version client api version module module boundary would mean wouldnt intermodule message passing based adr rather stick existing keeper approach take one step avoiding protobuf generated code keeper interface method approach fookeeperdosomething method wouldnt generated msgdosomething struct come protobuf api instead positional parameter order foov support foov keeper would simply implement keeper apis dosomething method could additional condition parameter wouldnt present would danger client accidentally setting isnt available approach would avoid challenge around minor version incompatibility existing module keeper api would get new field added protobuf file taking approach however would likely require making protobuf generated code internal order prevent leaking keeper api mean would still modify protobuf code generator register internal code global registry would still manually register protobuf filedescriptors probably true scenario may however possible avoid needing refactor interface method generated type handler also approach doesnt address would done scenario module still want message router either way probably still want way pas message one module another router safely even case like xgov xauthz cosmwasm etc would still require thing outlined approach although could advise module prefer keeper communicating module biggest downside approach probably requires strict refactoring keeper interface avoid generated code leaking api may result case duplicate type already defined proto file write method converting golang protobuf version may end lot unnecessary boilerplate may discourage module actually adopting achieving effective version compatibility approach although heavy handed initially aim provide system adopted give developer version compatibility free minimal boilerplate approach may able provide straightforward system since requires golang api defined alongside protobuf api way requires duplication differing set design principle protobuf apis encourage additive change golang apis would forbid downside approach clear roadmap supporting module language like rust doesnt get closer proper object capability security one goal adr adr done properly anyway set case latest draft proposal alignment adopting adr addition framework core replacement keeper paradigm entirely adr intermodule router accommodate variation approach given following rule client type server type pas directly client server zerocopy generated code wrapper still defined pas memory buffer one wrapper marshalunmarshal type client server approach allow maximal correctness enable clear path enabling module within language possibly executed within wasm minor api revision declare minor api revision proto file propose following guideline already documented cosmosappvalpha module proto package revised initial version considered revision include package comment proto file containing test revision start comment line current revision number field message etc added version beyond initial revision add comment start comment line form since revision nonzero revision added advised correspondence state machine module versioned set proto file versioned either buf module api module buf schema registry version buf module always corresponds package revision patch release documentation comment updated okay include proto package named etc versioned buf module cosmosbankv long proto package consist single api intended served single sdk module introspecting minor api revision order module introspect minor api revision peer module propose adding following method cosmossdkiocoreintermoduleclient servicerevisionctx contextcontext servicename string uint module could service name statically generated grpc code generator intermoduleclientservicerevisionctx bankvbetamsgservicedescservicename future may decide extend code generator protobuf service add field client type check concisely package bankvbeta type msgclient interface sendcontextcontext msgsend msgsendresponse error servicerevisioncontextcontext uint unknown field filtering correctly perform unknown field filtering intermodule router one following protoreflect api message support gogo proto message marshal existing codecunknownproto code zerocopy message simple check highest set field number assuming require field adding consecutively increasing order filedescriptor registration single binary may contain different version generated protobuf code cannot rely global protobuf registry contain correct filedescriptors appconfig module configuration written protobuf would like load filedescriptors module loading module provide way register filedescriptors module registration time instantiation propose following cosmossdkiocoreappmoduleoption constructor various case filedescriptors may packaged package appmodule googlegolangorgprotobuf compatible generated code protofilesbankvbetafilecosmosbankvbetamoduleproto func protofilesfile protoreflectfiledescriptor gogo proto generated code func gzippedprotofilesfile byte buf build generated pinned file descriptor func protoimageprotoimage byte approach allows support several way protobuf file might generated proto file generated internally module protofiles api module approach pinned file descriptor protoimage gogo proto gzippedprotofiles module dependency declaration one risk adr dependency called runtime present loaded set sdk module also want module way define minimum dependency api revision require therefore module declare set dependency upfront dependency could defined module instantiated ideally know dependency instantiation statically look app config determine whether set module example bar requires foo revision able know creating app config two version bar foo propose defining dependency proto module config object interface registration also define interface method defined type serialized googleprotobufanys light desire support module language may want think solution accommodate language plugins described briefly adr testing order ensure module indeed multiple version dependency plan provide specialized unit integration testing infrastructure automatically test multiple version dependency unit testing unit test conducted inside sdk module mocking dependency full adr scenario mean interaction module done via intermodule router mocking dependency mean mocking msg query server implementation provide test runner fixture make streamlined key thing test runner test compatibility test combination dependency api revision done taking file descriptor dependency parsing comment determine revision various element added created synthetic file descriptor revision subtracting element added later proposed api unit test runner fixture package moduletesting import testing cosmossdkiocoreintermodule cosmossdkiodepinject googlegolangorggrpc googlegolangorgprotobufproto googlegolangorgprotobufreflectprotodesc type testfixture interface contextcontext intermoduleclient making call module testing beginblock endblock type unittestfixture interface testfixture grpcserviceregistrar registering mock service implementation type unittestconfig struct moduleconfig protomessage module config object depinjectconfig depinjectconfig optional additional depinject config dependencyfiledescriptors protodescfiledescriptorproto optional dependency file descriptor instead global registry run run test function combination dependency api revision func cfg unittestconfig runt testingt funct testingt unittestfixture example testing bar calling foo take advantage conditional service revision expected mock argument func testbart testingt unittestconfigmoduleconfig foomodulevmodulerunt func testingt moduletestingunittestfixture ctrl gomocknewcontrollert mockfoomsgserver footestutilnewmockmsgserver foovregistermsgserverf mockfoomsgserver barmsgclient barvnewmsgclientf fservicerevisionfoovmsgservicedescservicename mockfoomsgserverexpectdosomethinggomockany foovmsgdosomething condition condition expected revision returnfoovmsgdosomethingresponse nil else mockfoomsgserverexpectdosomethinggomockany foovmsgdosomethingreturnfoovmsgdosomethingresponse nil err barmsgclientcallfoof msgcallfoo unit test runner would make sure dependency mock return argument invalid service revision tested ensure module dont incorrectly depend functionality present given revision integration testing integration test runner fixture would also provided instead mock would test actual module dependency various combination proposed api type integrationtestfixture interface testfixture type integrationtestconfig struct moduleconfig protomessage module config object dependencymatrix mapstringprotomessage dependent module configs run run test function combination dependency module func cfg integationtestconfig runt testingt func testingt integrationtestfixture example foo bar func testbarintegrationt testingt integrationtestconfig moduleconfig barmodulevmodule dependencymatrix mapstringprotomessage runtime protomessage test two version runtime runtimevmodule runtimevmodule foo protomessage test three version foo foomodulevmodule foomodulevmodule foomodulevmodule runt func testingt moduletestingintegrationtestfixture barmsgclient barvnewmsgclientf err barmsgclientcallfoof msgcallfoo unlike unit test integration test actually pull module dependency module written without direct dependency module golang concept development dependency integration test written separate module examplecombarvtest paradigm semantic versioning possible build single module import version bar version runtime test together six various combination dependency consequence backwards compatibility module migrate fully adr compatible existing module keeper paradigm temporary workaround may create wrapper type emulate current keeper interface minimize migration overhead positive able deliver interoperable semantically versioned module dramatically increase ability cosmos sdk ecosystem iterate new feature possible write cosmos sdk module language near future negative module refactored somewhat dramatically neutral cosmossdkiocoreappconfig framework play central role term module defined likely generally good thing mean additional change user wanting stick predepinject way wiring module depinject somewhat needed maybe even obviated full adr approach adopt core api proposed httpsgithubcomcosmoscosmossdkpull module would probably always instantiate method providemoduleappmoduleservice appmoduleappmodule error complex wiring keeper dependency scenario dependency injection may much case discussion described considered draft mode pending final buyin team key stakeholder key outstanding discussion adopt direction module client introspect dependency module api revision module determine minor dependency module api revision requirement module appropriately test compatibility different dependency version register resolve interface implementation module register protobuf file descriptor depending approach take generated code api module approach may still viable supported strategy would pinned file descriptor reference httpsgithubcomcosmoscosmossdkdiscussions httpsgithubcomcosmoscosmossdkdiscussions httpsgithubcomcosmoscosmossdkdiscussions httpsgithubcomcosmoscosmossdkpull httpsgithubcomcosmoscosmossdkissues adr adr', 'adr ibc relayer rust changelog first draft configuration update definition definition specific document may consistent ibc specification ibc transaction transaction includes ibc datagrams including packet constructed relayer sent physical network chain according chain rule example tendermint chain broadcasttxcommit request sent tendermint rpc server ibc datagram element transaction payload sent relayer includes client connection channel ibc packet data multiple ibc datagrams may included ibc transaction ibc packet particular type ibc datagram includes application packet commitment proof onchain ibc client ibc client client code running chain typically light client verification related functionality relayer light client full light client functionality including connecting least one provider full node storing verifying header etc source chain chain relayer read data fill ibc datagram destination chain chain relayer submits transaction include ibc datagram chain connection protocol initiating chain msgconnectionopeninit initially processed eventually msgconnectionopenack chain msgconnectionopentry msgconnectionopenconfirm processed similar channel handshake protocol relayer offchain process responsible relaying ibc datagrams two chain scanning state submitting transaction ibc architecture module directly sending message networking infrastructure instead create store data retrieved relayer build ibc datagrams document provides initial rust implementation specification relayer interconnects cosmossdk tendermint chain diagram show high level view relayer interaction source destination chain next section detail different interaction assumption dependency section cover assumption dependency chain ibc implementation first implementation focus tested cosmossdk tendermint chain addition functionality required relayer outside scope document availability implementation considered data availability relayer monitor chain state determine packet forwarding required relayer must able retrieve data within time bound referred data availability data legibility ibc protocol defines minimal data set must made available relayers correct operation protocol relayer expects data legible data serialized according ibc specification format includes consensus state client connection channel packet information auxiliary state structure necessary construct proof inclusion exclusion particular keyvalue pair state query functionality ibc host state machine must expose interface inspecting state cosmostendermint chain mean ibc module chain correctly implement respond query ibcmodulesrust implementation query currently exist cosmossdk implemented rust full requirement detailed section relayer query relayer ability send rpchttp abci query receive reply tendermintcosmossdk abci rust abci rust implementation ibcmodulesrust identifier validation required ibcmodulesrust requires rust type query response merkleproofsrust candidate implementation query response include proof included ibc transaction relayer may validated tbd ibc message relayer creates transaction include ibc message manage client connection channel send application packet destination chain message must defined ibc rust implementation ibcmodulesrust ibc logging system ibc packet data timeouts stored directly chain state storage presumed expensive instead committed succinct cryptographic commitment commitment stored consequence ibc requires host state machine must provide event logging system log data course transaction execution log must queryable relayers read ibc packet data timeouts logging system must provide following function ibcmodulesgo emitlogentry emitting log entry called state machine transaction execution type emitlogentry topic string data byte void example emitlogentrysendpacket sequence packetsequence data packetdata timeout packettimeout ibcmodulesgo querybytopic querying past log matching given topic type querybytopic height uint topic string array byte keyring relay process must access account token destination chain sufficient balance pay transaction fee account key information must stored managed securely keyring implementation required crud key operation keyringrust investigation existing rust implementation needed hwchenkeyring chain transaction signing relayer must create chain specific signed transaction cosmostxrust first release cosmossdk transaction signing required one possible implementation iqlusions sdtx crate implementation ibc routing module default ibc handler receiver call pattern module must individually call ibc handler order bind port start handshake accept handshake send receive packet etc provides flexibility module imposes extra work part relayer process track state multiple module ibc specification describes ibc routing module route packet simplify task relayers routing module accepts external datagrams call ibc handler deal handshake packet relay routing module keep lookup table module look call module packet received external relayers ever relay packet routing module ibcroutingmodulego initial version relayer assumes chain implement routing module batching relayer may batch ibc datagrams single transaction supported destination chain allowed configuration case relayer amortise overhead cost signature check fee payment initial version relayer assumes batching supported chain may later included configuration file relayer requirement correct relayer must rconfigstart read parse validate configuration file upon start configure specified chain path rtransport access networking protocol tcpip udpip quicip physical transport required read state one blockchain machine submit data another rprovider maintain transport connection least one full node per chain rquery query ibc data source destination chain rlightclient run light client source chain ribcclient create update ibc client destination chain raccounts account destination chain sufficient balance pay transaction fee rtransact create sign forward ibc datagram transaction rrelay perform correct relaying required message according ibc subprotocol constraint rrestart resume correct functionality restarts rupgrade resume correct functionality upgrade rproofs perform proof verification done destination chain forward message proof verification fails relayer may rconfigcli provide way change configuration runtime rbisection perform bisection optimize transaction cost computation destination chain rrelayprio filter order transaction based criterion accordance fee payment model implementation initial implementation heavily borrow relayer implementation naive algorithm relaying message structure configuration file similar one see gorelayer configuration wip upon start relayer read configuration file includes global per chain parameter file format toml example configuration file toml global loglevel error mode modeclients enabled true refresh true misbehaviour true modeconnections enabled false modechannels enabled false modepackets enabled true clearinterval clearonstart true txconfirmation true chain chaina rpcaddr httplocalhost grpcaddr httplocalhost websocketaddr wslocalhostwebsocket rpctimeout accountprefix cosmos keyname testkey storeprefix ibc clientids cla cla gas gasadjustement gasprice stake trustingperiod chain chainb rpcaddr httplocalhost grpcaddr httplocalhost websocketaddr wslocalhostwebsocket rpctimeout accountprefix cosmos keyname testkey storeprefix ibc clientids clb gas gasadjustement gasprice stake trustingperiod main section configuration file global relaying done periodically frequency dictated thetimeoutparameter thestrategyparameter configures relayer run particular relaying algorithm chain chain level information including account key name gas information trusting period etc source destination chain must listed path connectionsconnectionspaths relayer may configured relay application port number connection channel unidirectional bidirectional mode initialization relayer performs initialization based content configuration file file parsed semantically validated chain connection port channel relaying enabled stored config structure rust pub struct config pub global globalconfig pub chain vec pub connection pub struct globalconfig valid log level defined tracing httpsdocsrstracingcoretracingcorestructlevelhtml pub loglevel string pub struct chainconfig pub chainid pub rpcaddr tendermintrpcurl pub websocketaddr tendermintrpcurl pub grpcaddr tendermintrpcurl pub rpctimeout duration pub accountprefix string pub keyname string pub clientids vec pub gas pub trustingperiod duration pub struct connection pub src source pub dest destination pub path port direction bidirectional pub struct connectionend pub clientid string pub connectionid connection client pub enum direction unidirectional bidirectional pub struct relaypath pub srcport default source port pub destport default dest port pub srcchannel default source port pub destchannel default dest port pub direction direction default bidirectional alloptionfields withnonevalues mean value fordirection default bidirectional nonoption field mandatory must appear configuration file relayer started invalid configuration file error displayed realyer process exit relayer command validate validate configuration file relayer configfile config validate command verifies specified configuration file par semantically correct start start relayer relayer configfile start command performs validation described start relayer query query performed relaying also available cli relayer configfile query client state chain clientid chainheight proofrequired command query full client state clientid chain height without proof depending proofrequired flag default height latest state proofrequired true relayer configfile query client consensus chain clientid consensusheight chainheight proofrequired command query consensus state clientid height consensusheight chain height without proof depending proofrequired flag default height latest state proofrequired true relayer query relayer query chain state order build ibc message expected chain type provides implementation query initial rust relayer implementation tested cosmossdktendermint chain ibcmodules functionality rust required handler function query crate available relayer tendermint query abcirequestquery rpchttp retrieve data format public provable state query parameter response chain independent also defined crate following query required querystoreprefixchain return commitment prefix chain return chain specific byte ibc tendermint queryallclientstateschain return ibc light client instantiated chain queryclientconsensusstatechain clientid height return consensus state proof light client given height height else return latest height queryconnectionschain return connection created chain queryclientconnectionschain clientid return connection associated light client added relayer concurrency architecture following thread spawned execute within relayer process one tendermint full light client thread per configured configured source chain example path enabled two light client thread one one thread download light client header block header commits verify store trusted header per chain store one thread main relaying functionality aka relay thread one thread relay notification source chain generate ibc event relay thread figure show interaction last two thread start communication channel relay notification thread established notification thread register ibc event relay thread creates ibc datagrams configuration triggered event client msgcreateclient msgupdateclient connection channel msgconnopeninit msgchannopeninit sent chain initiate connection channel handshake required wait event notification thread notification thread query source chain latest height sends ibc event relay thread wait notification event related connection channel packet relay thread query client state destination state source chain information collected previous step relay thread creates buffer message destined destination notification thread receives ibc notification sends relay thread step initial version single relay thread configured path temporary thread may created source destination query required future version may create multiple relay thread one possibility create one destination chain responsible relaying path thread pool selecting available thread relaying given destination notification thread route ibc event proper thread multiple notification thread per source also considered relayer algorithm relayer algorithm described relayer algorithm described ibc specification relayer implementation section describes detail really thread algorithm rust implementation input ibc event event interest described appendix high level event source chain relayer query client connection channel andor packet related state source destination chain creates new datagrams needed batch multiple datagrams single transaction sign submits transaction destination proof relayer must include proof datagrams required ibc handler two type proof proof local state source chain example proof correct connection state proofinit prooftry proofack included connection handshake datagrams connopentry message includes proofinit obtained chain connection init state certain local counterparty identifier message specific section detail proof chain ibc client clb updated consensus state height stored chain proof verified chain consensus state stored client proofheight note proof check require handler recreate state expected chain verify proof work store prefix added prefix proof path standardized currently query endpoint cosmossdktendermint initial relayer version includes per chain store prefix configuration verification requires presence consensus state client height proofheight light client message initialization relayer light client created destination chain already present successful relay ibc packet ibc client must instantiated source destination chain potentially different relayers client creation permissionless relayer may create client already present rust let msg msgcreateclientnewclientid header trustingperiod bondingperiod signer relayer run light client thread periodically retrieves verifies header relay thread stored header update aclient chain new header required rust let msg msgupdateclientnewclientid header signer possible relay thread recent trusted header case would mechanism signal client thread retrieve header since relayer must pay transaction including msgclientcreate msgclientupdate incentive optimization example light client implementation tendermint support bisection relayer may choose send skipping header aclient periodically required new ibc datagrams ibc client consensus state relayer light client state chain state number ibc datagrams contain proof obtained chain height proof verified commitment root tendermint client included client consensus state tendermint chain application hash applying transaction block included block height relayer therefore ensure consensus state proofheight exists chain one proposal shown described rest section relayer creates light client update required processing different ibc event let last consensus state client ibc event connection channel packet received includes height let event occurred according proposal relayer get latest consensus state height client let maxhx query item height get proof height wait block height received evblock get minimal set header light client verifies send zero msgupdateclient datagrams msgxx transaction transaction successful msgx failed consume evx msgx fails nothing done another relayer must submitted first else raise event one already effect new query made since consensus state exists msgx sent connection message relayer query source destination chain relaying path order determine connection handshake datagrams sent destination chain connection query following structure pertain connection query detailed ibcmodulesrustadr structure shown reference rust pub struct counterparty pub clientid clientid pub connectionid connectionid pub prefix commitmentroot pub struct connectionend pub state connectionstate pub connectionid pub clientid clientid pub counterparty counterparty pub version vec pub enum connectionstate uninit init tryopen open connectionresponse defines query response connection includes proof height proof retrieved pub struct connectionresponse pub connection connectionend pub proof pub proofpath commitmentpath pub proofheight height connection relaying figure show four connection handshake message type created relay cycle see relayer box four action message query light grey arrow expected state shown example connection open state tryopen relayer send transaction including connopenconfirm datagram processed state connection change tryopen open msgconnectionopeninit msgconnectionopeninit message initialize connection done relay thread start loading configuration includes connection information entering event loop section assumed message relayed rust pub struct msgconnectionopeninit pub connectionid connectionid connatob pub clientid clientid clb pub counterparty counterparty clientid cla connectionid connbtoa prefix bstore pub signer accaddress comment show value field diagram relayer creates forward message explicitly configured connection information see connectionssrc connectionsdestsections configuration file order create msgconnectionopeninit relayer recreates connectionend configuration stored step create connectionend path rust let connectiona getconfiguredconnectiona query connection state chain already exist continue next event rust let existinga ibcqueryconnectionchaina connectiona existingastate uninit continue create message rust let initmsg msgconnectionopeninit connectionid connectionaconnectionid clientid connectionaclientid counterparty counterparty clientid connectionacounterpartyclientid connectionid connectionacounterpartyconnectionid prefix configbstoreprefix signer configasigner send initmsg transaction msgconnectionopentry msgconnectionopentry defines message sent relayer try open connection section assumed relayed rust pub struct msgconnectionopentry pub connectionid connectionid connbtoa pub clientid clientid cla pub counterparty counterparty clientid clb connectionid connatob prefix astore pub counterpartyversions vecstring pub proofinit commitmentproof proof connatob connection end stored chain pub proofconsensus commitmentproof proof proofheight client stored consensus state consensusheight pub proofheight height height relayer retrieved proofinit pub consensusheight height pub signer accaddress comment show value field diagram note proofheight height chain relayer created proofinit diagram consensusheight latest height chain chain stored client clb time relayer queried client diagram relayer creates msgconnectionopentry relay path ibc event notification received step let connatob connection identifier ahx height event occurred cla client query last client state height rust let haprime ibcqueryclientstatechainb height create updateclientmsgs cla chain required higher latest height cla rust let maxhx haprime let header getminimalseth haprime let clientmsgs updateclientmsgscla header signer send clientmsgs query latest height wait rust todo query connection proof chain proper state continue next event rust let queryresponse ibcqueryconnectionwithproofchaina connatob queryresponseconnectionstate init continue let connectiona queryresponseconnection let proofinit queryresponseproof let proofheight queryresponseproofheight assertproofheight query consensus state stored client clb rust let consensusresponse ibcqueryconsensuswithproofchaina connectionaclientid let proofconsensus consensusresponseproof let consensusheight consensusresponseproofheight create msgconnectionopentry message information collected rust let trymsg msgconnectionopentry connectionid connbtoa clientid cla counterparty counterparty clientid connectionaclientid connectionid connatob prefix configastoreprefix proofinit proofconsensus proofheight consensusheight signer configbsigner send trymsg msgconnectionopentry processed message handler check consensusheight valid smaller equal chain current height within trusting period client cla verifies proofconsensus consensus state consensusheight client cla verifies proofinit connectionendobject expects present proofheight relayer may also perform verification submitting transaction msgconnectionopenack wip updated correct query sequence msgconnectionopenack defines message sent relayer chain acknowledge change connection state tryopen chain rust pub struct msgconnectionopenack pub connectionid connectionid connatob pub prooftry commitmentproof proof connbtoa chain tryopen state pub proofconsensus commitmentproof proof proofheight client stored consensus state consensusheight pub proofheight height height relayer retrieved prooftry pub consensusheight height pub version string pub signer accaddress comment show value field diagram note proofheight height chain relayer created prooftry diagram consensusheight latest height chain chain stored client cla time relayer queried client diagram relayer creates msgconnectionopenack relay path ibc event notification received chain scanned step let connbtoa connection identifier query connection proof chain proper state continue next event rust let queryresponse ibcqueryconnectionwithproofchainb connbtoa queryresponseconnectionstate tryopen continue let connectionb queryresponseconnection let prooftry queryresponseproof let proofheight queryresponseproofheight query connection chain validate state rust let connatob connectionbcounterpartyconnectionid let connectiona ibcqueryconnectionchaina connatob connectionastate init connectionastate tryopen continue create updateclientmsg clb chain required proofheight higher latest height clb rust let clientmsg msgupdateclientnewconnectionaclientid header signer query consensus state stored client cla rust let consensusresponse ibcqueryconsensuswithproofchainb connectionbclientid let proofconsensus consensusresponseproof let consensusheight consensusresponseproofheight create msgconnectionopenack message information collected rust let ackmsg msgconnectionopenack connectionid connatob prooftry proofconsensus proofheight consensusheight signer configasigner send clientmsg ackmsg transaction msgconnectionopenconfirm wip updated correct query sequence msgconnectionopenconfirm defines message sent relayer chain confirm opening connection chain rust pub struct msgconnectionopenconfirm pub connectionid connectionid connbtoa pub proofconfirm commitmentproof proof connatob chain open state pub proofheight height height relayer retrieved proofconfirm pub signer accaddress relayer creates msgconnectionopenconfirm relay path ibc event notification received chain scanned step let connatob connection identifier query connection proof chain proper state continue next event rust let queryresponse ibcqueryconnectionwithproofchaina connatob queryresponseconnectionstate open continue let connectiona queryresponseconnection let proofconfirm queryresponseproof let proofheight queryresponseproofheight query connection chain validate state rust let connbtoa connectionacounterpartyconnectionid let connectionb ibcqueryconnectionchainb connbtoa connectionbstate init connectionbstate tryopen continue create updateclientmsg cla chain required proofheight higher latest height cla rust let clientmsg msgupdateclientnewconnectionbclientid header configbsigner create msgconnectionopenconfirm message information collected rust let confirmmsg msgconnectionopenack connectionid connbtoa proofconfirm proofheight signer configbsigner send clientmsg confirmmsg transaction channel wip channel handshake message relayed similar way connection one addition check state underlying connection performed packet timeouts acknowledgment wip application packet stored chain state cryptographic commitment stored relayer query chain logging system get packet data given source port channel result query includes among others source port channel identifier sequence number create packet commitment path state query get packet commitment interrelayer coordination multiple relayers may run parallel expected relay disjoint path could case may submit transaction chain case first transaction succeeds subsequent fail causing loss fee ideally coordination would place avoid scope document relayer restarts upgrade section explains detail proposed solution including implementation detail also describe affect corollary item may changed part proposed change large please also indicate way change maximize ease review optimal split thing separate may proposed hasnt agreed upon yet agreed upon later adr change revers may marked deprecated superseded reference replacement deprecatedproposedaccepted consequence section describes consequence applying consequence summarized positive one positive negative neutral appendix ibc event input relay thread described createclient clientid clienttype updateclient clientid clienttype connectionopeninit connectionid clientid counterpartyconnectionid counterpartyclientid connectionopentry connectionid clientid counterpartyconnectionid counterpartyclientid connectionopenack connectionid connectionopenconfirm connectionid channelopeninit portid channelid counterpartyportid counterpartychannelid connectionid channelopentry portid channelid counterpartyportid counterpartychannelid connectionid channelopenack portid channelid channelopenconfirm portid channelid channelcloseinit portid channelid channelcloseconfirm portid channelid sendpacket packetdata string packettimeoutheight string packettimeouttimestamp string packetsequence string packetsrcport packetsrcchannel packetdstport packetdstchannel recvpacket packetdata string packetack string packettimeoutheight string packettimeouttimestamp string packetsequence string packetsrcport packetsrcchannel packetdstport packetdstchannel reference relevant comment issue led article referenced made given design choice link reference link']"
7,209, API Documentation Strategy,['API Documentation Strategy'],"['api management', 'apis', 'api', 'openapi', 'schema', 'endpoint', 'json', 'metadata', 'odata', 'specification']","['adrapidocumentation title api documentation granary standalone deployable open source application user primary way interacting granary rest api rather least foreseeable future focus put larger burden usual quality api documentation wed like evaluate different way keep api documentation uptodate strategy want know general pro con find doc drifted api document different version api time contender evaluated three software solution rely codedocumentation generation also process solution three library considered rho tapir guardrail software solution created small repository manual api spec maintenance successful way weve done past specfirst development pattern pattern added endpoint api spec implemented spec change merged drifted away time eventually wound checklist item pull request template specified api spec updated long time spec wasnt valid swagger found tried create doc site part spec incorrect found user attempted spec api interaction though problem well find doc drifted api way answered question spec make sure happy path least correctly documented generated python client yelp bravado library interact api without error manual process relied heavily python client repository different strategy could consider would generate scala client hosted version spec swaggerhub ensure drop data model generated client place existing data model testing would require investing software development time tooling another rely upcoming panrec feature parse generated client datamodel existing datamodel ensure agree strategy ensure data model correct without detecting whether weve moved route thats consistency check beyond weve done still leaf lot room get spec exactly right way make spend potential support time triaging issue responding help request spec maintenance document different version api time strategy ive come manual maintenance version lot copying pasting supposing route exists vmodels another route exists vmodels dont know openapi share thing two endpoint later change like adding new response type think would manually written place sound like headache tapir change generate doc tapir doc served localhostapihellodocsyaml put directly swagger editor tapir library separating description apis implementation interpreting description different output example endpoint description interpreted documentation yaml string server given function map input described endpoint output described endpoint endpoint tapir explicitly encode input type output type error endpointi map input type output type returning error type stream type far needed stream type anything tapir make easy add input endpoint chain call endpoint add output add metadata name description worst thing happened tapir accidentally wound unreachable route seems like tapir http interface want mount service onto path routerv new vapi new vapi instead include path component endpoint description tested serving doc algebraic data type adding authenticated route make sure understood path work straightforward adt response correctly encoded oneof default response left authentication function instead thats primarily consequence extremely simplified endpoint dont know encode specific error cant anything discriminate response return tapir endpoint also interpreted client test feature find doc drifted api doc cannot drift api doc server interpretation endpoint document different version api time think separating endpoint component authentication doc mention defining auth input first shared many endpoint believe could something similar version input scala object endpoint val endpointinv val endpointinv val scenesendpointv vinscenes val scenesendpointv vinscenes versioned collection endpoint could served version prefix rho change generate doc rho doc served localhostapihelloswaggerjson put directly swagger editor rho library http ecosystem automatically generating swagger documentation routing dsl route parameter description combined routing logic create rhoroutesf transformed normal httproutesf rhomiddleware also serf api documentation json configurable endpoint worst part rho keep number odd operator head example capturing query parameter specifying response type adding description binding route function business logic possible something wed get time look write rho generates swagger openapi specification json access oneof keyword describing response might one several different schema generated json included one error json schema generated endpoint returning circe json type missing referred route find doc drifted api doc cannot drift api rhomiddleware creates doc route actually document different version api time service serve doc mounting service http router also mount documentation service guardrail guardrail http support currently documented investigate library tapir automatically generating api documentation adt support straightforward api input output auth extractor auth flatten learning curve well stable correct reference point api documentation user setting deployment refer call beginning readme hopefully save answer entire category question consequence first route added api slightly difficult theyll include writing api route new library first time readme updated point location api documentation', 'expose version data collection api problem statement extend current api come part collaboration term service didnt read tosdr whose web application adapted obtain data public open term archive collection instead tosdr server database term annotated tosdr currently taken original web document stored tosdr server database open term archive provides significantly higher quality source term example cleaning content related term combining document scattered across several page normalising pdf document markdown allow transition historical tosdrs crawler database publicly accessible open term archive collection tosdr requires access content version datasets satisfactory produced weekly basis whereas workflow supporting annotation justadded term required tosdr rfc outline possible implementation api providing data proposed solution solution base url collection hostapiversion endpoint get versionserviceidtermstypedatemd return markdown content version term applicable given parameter parameter type description serviceid urlencoded string service termstype urlencoded string name term type urlencoded iso datetime string time version requested note full time required simple avoid ambiguity day version changed timezone difference client server required match exactly fetch version version fetched periodic interval version returned one applicable provided get latest version available simply current parameter return http markdown content body version found http found json content error version found requesteddate anterior first available version http bad request json content error requested version future future example get versionopentermsarchiveprivacypolicytaazmd markdown http privacy policy last updated september privacy policy explains condition open term archive ota site collect personal information privacy policy change time course also record change document solution base url collection hostapiversion endpoint get versionserviceidtermstypedate return json object containing version content valid given along metadata parameter parameter type description serviceid urlencoded string service termstype urlencoded string name term type urlencoded iso datetime string time version requested note full time required simple avoid ambiguity day version changed timezone difference client server required match exactly fetch version version fetched periodic interval version returned one applicable provided get latest version available simply current parameter return http json object containing markdown content body version found http found json content error version found requesteddate anterior first available version http bad request json content error requested version future future example get versionopentermsarchiveprivacypolicytaaz json fetchdate snapshotsids cdacfbcdeffbccacdfb cdacfbcdeffbccacdfb content nprivacy policynnlast updated september nthis privacy policy explains condition open term archive ota site collect personal information privacy policy change time course also record change documentshttpsgithubcomopentermsarchivedemoversionstreemainopentermsarchive solution solution aim allowing caching providing metadata standard http method redirecting canonical version name instead replying applicable content enable proxy server user agent leverage etags cache actual content might several hundred also enables client know time record version base url collection hostapiversion endpoint get versionserviceidtermstypedatemd identifies markdown content version term applicable given provides content parameter parameter type description serviceid urlencoded string service termstype urlencoded string name term type urlencoded iso datetime string time version requested note full time required simple avoid ambiguity day version changed timezone difference client server get latest version available simply current parameter return http markdown content body version found exact http content version applicable location field response giving exact http found json content error version found requesteddate anterior first available version http bad request json content error requested version future future example get versionopentermsarchiveprivacypolicytaazmd text http location hostversionopentermsarchiveprivacypolicytaazmd get versionopentermsarchiveprivacypolicytaazmd markdown http privacy policy last updated september privacy policy explains condition open term archive ota site collect personal information privacy policy change time course also record change document solution solution combination aiming keeping best part ease content access metadata extensibility performance capability added benefit possibly split implementation two part made clear proposal base url collection hostapiversion endpoint json contenttype get versionserviceidtermstypedate return content version term applicable given metadata json format parameter parameter type description serviceid urlencoded string service termstype urlencoded string name term type urlencoded iso datetime string time version requested note full time required simple avoid ambiguity day version changed timezone difference client server get latest version available simply current parameter return http json object body version found exact containing metadata content property markdown content jsonescaped http content version applicable location field response giving exact http found json content error version found requesteddate anterior first available version http bad request json content error requested requesteddate future version exist future example get versionopentermsarchiveprivacypolicytaaz http location hostversionopentermsarchiveprivacypolicytaaz get versionopentermsarchiveprivacypolicytaaz json http etag cdacfbcdeffbccacdfb fetchdate snapshotsids cdacfbcdeffbccacdfb cdacfbcdeffbccacdfb content nprivacy policynnlast updated september nthis privacy policy explains condition open term archive ota site collect personal information privacy policy change time course also record change document get versionopentermsarchiveprivacypolicytaaz ifnonematch cdacfbcdeffbccacdfb json http endpoint markdown contenttype get versionserviceidtermstypedatemd return content version term applicable given markdown format parameter parameter type description serviceid urlencoded string service termstype urlencoded string name term type urlencoded iso datetime string time version requested note full time required simple avoid ambiguity day version changed timezone difference client server get latest version available simply current parameter return http markdown content body version found exact http content version applicable location field response giving exact http found markdown content errornnno version found requesteddate anterior first available version http bad request json content errornnrequested requesteddate future version exist future example get versionopentermsarchiveprivacypolicytaazmd http location hostversionopentermsarchiveprivacypolicytaazmd get versionopentermsarchiveprivacypolicytaazmd markdown http etag cdacfbcdeffbccacdfb privacy policy last updated september privacy policy explains condition open term archive ota site collect personal information privacy policy change time course also record change document get versionopentermsarchiveprivacypolicytaazmd ifnonematch cdacfbcdeffbccacdfb markdown http solution solution solution without premature performance optimization introduced solution base url collection hostapiversion endpoint json contenttype get versionserviceidtermstypedate return content version term applicable given metadata json format parameter parameter type description serviceid urlencoded string service termstype urlencoded string name term type urlencoded iso datetime string time version requested note full time required simple avoid ambiguity day version changed timezone difference client server get latest version available simply current parameter return http json object body version applicable containing metadata content property markdown content jsonescaped http found json content error version found requesteddate anterior first available version http bad request json content error requesteddate valid iso time valid iso time http range satisfiable json content error requested requesteddate future version exist future example get versionopentermsarchiveprivacypolicytaaz json http fetchdate snapshotsids cdacfbcdeffbccacdfb cdacfbcdeffbccacdfb content nprivacy policynnlast updated september nthis privacy policy explains condition open term archive ota site collect personal information privacy policy change time course also record change document endpoint markdown contenttype get versionserviceidtermstypedatemd return content version term applicable given markdown format parameter parameter type description serviceid urlencoded string service termstype urlencoded string name term type urlencoded iso datetime string time version requested note full time required simple avoid ambiguity day version changed timezone difference client server get latest version available simply current parameter return http markdown content body version applicable http found markdown content errornnno version found requesteddate anterior first available version http bad request markdown content errornnrequested requesteddate valid iso time valid iso time http range satisfiable markdown content errornnrequested requesteddate future version exist future example get versionopentermsarchiveprivacypolicytaazmd markdown http privacy policy last updated september privacy policy explains condition open term archive ota site collect personal information privacy policy change time course also record change document outcome consulting community solution retained integrates content accessibility solution metadata extensibility solution also avoids introducing early performance optimisation stage', 'swagger annotation generate maintain konduitserving client proposed sham azeem discussed paul dub alex black considering inference two endpoint responsible take input prediction applicationjson predictiontypeinputdataformat json input multipartformdata predictiontypeinputdataformat multipart input file given two endpoint done ton work creating python client right proper documentation example maintenance planning making apis adaptable since planning apis multiple language python java others might get difficult maintain document separately future swagger annotation generate document client apis swagger annotation quick easy way generate openapi specification source code annotation apply class method argument way easier get rid client apis maintenance generation documentation packaging different language work refactoring konduitserving source code class contain apis different type verticles example java path producesmediatypeapplicationjson public class inferenceapi get pathconfig public inferenceconfiguration getconfig return new inferenceconfiguration post pathpredictiontypeinputdataformat consumesmediatypemultipartformdata operationsummary get inference result multipart data tag inference description send multipart data inference input name name input transformation process model input corresponding file containing data input response apiresponsedescription batch output data responsecode content contentschema schemaoneof classifieroutputclass regressionoutputclass detectedobjectsbatchclass manydetectedobjectsclass public batchoutput predictpathparampredictiontype outputpredictiontype predictiontype pathparaminputdataformat inputdataformat inputdataformat parameterdescription array file upload file multipartinput return new classifieroutput require similar refactoring verticles respective router currently following class refactored based detail pipelineroutedefiner memmaproutedefiner converterinferenceverticle clusteredverticle would look end api look like class generate api specification look like yaml openapi info title konduit serving rest api description restful api various operation inside konduitserving contact name konduit url httpskonduitaicontact email hellokonduitai license name apache url httpsgithubcomkonduitaikonduitservingblobmasterlicense version snapshot externaldocs description online documentation url httpsservingosskonduitai tag name inference description tag grouping inference server operation name convert description tag grouping converter operation name memmap description tag grouping memory mapping operation path config get operationid getconfig response default description default response content applicationjson schema ref componentsschemasinferenceconfiguration predictiontypeinputdataformat post tag inference summary get inference result multipart data description send multipart data inference input name name input transformation process model input corresponding file containing data input operationid predict parameter name predictiontype path required true schema type string enum classification yolo ssd rcnn raw regression name inputdataformat path required true schema type string enum numpy json ndj image arrow requestbody description array file upload content multipartformdata schema type array item type string format binary response description batch output data content applicationjson schema oneof ref componentsschemasclassifieroutput ref componentsschemasregressionoutput ref componentsschemasdetectedobjectsbatch ref componentsschemasmanydetectedobjects component schema inferenceconfiguration type object property step type array item ref componentsschemaspipelinestep servingconfig ref componentsschemasservingconfig memmapconfig ref componentsschemasmemmapconfig memmapconfig type object property arraypath type string unkvectorpath type string initialmemmapsize type integer format int workspacename type string pipelinestep type object property input ref componentsschemaspipelinestep output ref componentsschemaspipelinestep outputcolumnnames type object additionalproperties type array item type string inputcolumnnames type object additionalproperties type array item type string inputschemas type object additionalproperties type array item type string enum string integer long double float categorical time byte boolean ndarray image outputschemas type object additionalproperties type array item type string enum string integer long double float categorical time byte boolean ndarray image outputnames type array item type string inputnames type array item type string servingconfig type object property httpport type integer format int listenhost type string outputdataformat type string enum numpy json ndj arrow uploadsdirectory type string logtimings type boolean includemetrics type boolean metrictypes type array item type string enum classloader jvmmemory jvmgc processor jvmthread loggingmetrics native gpu classifieroutput type object property type array item type integer format int probability type array item type array item type number format double label type array item type string batchid type string regressionoutput type object property value type array item type array item type number format double batchid type string detectedobjectsbatch type object property centerx type number format float centery type number format float width type number format float height type number format float predictedclassnumbers type array item type integer format int predictedclasses type array item type string confidence type array item type number format float batchid type string manydetectedobjects type object property detectedobjectsbatches type array item ref componentsschemasdetectedobjectsbatch batchid type string writeonly true batchoutput type object property batchid type string writeonly true yaml client generated openapigenerator example bash java jar openapigeneratorclijar generate openapiyaml python pythonapiclient command generate python client related doc api python similar process language well consequence advantage documentation maintenance easier ability create client language thats supported openapigenerator getting rid konduitserving codegen generating python client disadvantage required code refactoring take time strictly adhering openapi allows apis defined well lose bit flexibility since generated apis might lacking well write small layer wrapper apis top generated code doc take additional time discussion going integrate generated endpoint api doc gitbook gitbook doesnt way integrate open api specification like readme opensource showcase rest apis way generate static html page update another swaggerui gitbook define endpoint apis documented well look much gitbooks functionality publish document first look mentioned restriction well face usage swaggerannotations one thing able freely define function overload since well make sure fit together nicely swagger annotation also generated apis different language able adapt api framework cant kind object since wont make sense openapi example mapstring file wont work sending multipart request swagger expects file instead small thing well wary also well make sure generated apis easily extendable going wrapper layer top generated client want numpyindarray type client apis there way specify openapi generator custom api layer top generated client take care wrapper layer would easier maintain compared fully maintained api client package specific language']"
8,136, Monitoring and Metric Management,['Monitoring and Metric Management'],"['cloudwatch', 'monitoring', 'analytics', 'telemetry', 'api', 'logging', 'metric', 'endpoint', 'aws', 'dashboard']","['log cache clock issue issue motivating influence constrains cli previously traffic controller retrieve application log implemented longlived websocket connection log streamed client migration log cache needed preserve experience streaming log built api timestamped log envelope retrieved via http request two main question needed answer oldest log shown application may log previous push already present log cache relate current staging operation would want show log current staging operation newest log shown log cache eventually consistent recent log may order incomplete would want show log settled log log cache long enough converge also needed answer question regarding log recent want define range settled log show want show everything currently log cache change proposing agreed implement determine oldest log shown peek latest log within log cache application read point determine newest log shown show log envelope timestamp two second old log recent always show log currently log cache regardless whether settled peeking latest log initial cli implementation log cache started reading log cache offset based current client clock time flawed incorrectly configured client clock would result unexpected behaviour client clock ahead server either log would shown would lengthy delay log would shown client clock behind server log relating previous operation might shown attempt decouple client clock time instead peek timestamp latest log envelope application timestamp starting point envelope present application continue retry envelope become available delaying output new log default log cache client return log envelope timestamps second old see code filtering mechanism known walkdelay configurable testing found default one second sufficient allow log cache settle resulted log loss foundation multiple log cache node multiple log cache node may ingesting event application single log cache node host cache given application possible see newer log envelope timestamp move timestamp cursor forward received earlier log envelope httpswwwpivotaltrackercomstoryshowcomments decided increase walkdelay second give log cache time settle multiple log cache node foundation note issue filed log cache client increase default walk delay httpsgithubcomcloudfoundrygologcacheissues log recent behavior decided implement log recent single request log cache instead multiple pas walk implementation better performance mean walkdelay latest log returned request may settled three considered address nothing simply return log currently log cache render log two second old wait two second showing log point command started decided would likely break cat automated tooling run log recent command due lag log appearing poor made command two second slower would also missing log emitted command started see comment information consequence becomes easier difficult risk introduced change mitigated peeking latest log peeking latest envelope application remove dependency client clock making tolerant situation client clock closely synchronised server component generates log responsible assigning timestamp still potential clock drift within foundation cause unexpected behavior code slightly involved seems like good tradeoff delaying output new log momentarily delaying output new log envelope user advantage delaying able output consistent view log walkdelay implemented within log cache client comparison log envelope timestamps client clock therefore vulnerable misconfiguration client clock weve considered strategy removing dependency client clock decided wait feedback user attempting make reliable situation issue filed log cache client httpsgithubcomcloudfoundrygologcacheissues log recent behavior recent log returned command may settled may discrepancy output two consecutive run command', 'managed prometheus proposed proposal amazon managed service prometheus amp monitoring cloud platform instead selfmanaged instance prometheus monitoring metric good operational practice good observability includes monitoring achieved regular checking metric health number container running timeseries data collected shown graph indicator dashboard evaluated rule trigger alert operator typical operator include become familiar typical quantity resource consumed software alerted deteriorating health fix becomes incident alerted incident able react quickly user flag incident getting ataglance overview problem exist incident understand went wrong help review action taken response reviewing longterm pattern health choice prometheus prometheus cloud platform monitoring metric since established prometheus number year happy functionality prometheus remains popular choice industry open source large community recommended cncf commercial proprietary team investigated commercial monitoring solution datadog splunk honeycomb tend several related thing including metric monitoring dashboard alerting logging application performance monitoring dont offer functionality particular nice monitoring logging nicely integrated managed service would reduce operation architect future scaling concern offshoring log data circumstance personal data leak log cost varied suggestion even managed service wed still retain prometheus concern user config code whether easy deploy open source would cost migration user config would migrating existing prometheusalertmanagergrafana syntax overall happy stick prometheus prometheus prometheus setup monitor whole cloud platform including tenant container tenant aws resource kubernetes cluster kubeprometheus prometheus configured store worth data enough support case data also sent thanos efficiently store year metric data make available query promql syntax alertmanager prometheus data evaluating alert rule concern hosting prometheus currently prometheus container run smoothly recent month performance resolved serious performance issue alert rule taking long evaluate prometheus data however successfully alleviated increasing disk iop remaining concern custom node group single prometheus instance monitoring entire platform consumes lot resource weve put dedicated node full resource memory node mean custom node group bit extra management overhead scalability scaling vertical way ideal scaling smooth eventually well hit limit cpumemoryiops shard see also address management overhead managed cloud service generally preferred selfmanaged cost tends amortized large customer base far cheaper inhouse staff people ops skill premium management overhead prometheus kubeprometheus high availability single instance prometheus simply weve got round choosing implementing arrangement yet risk period outage dont collect metric data although impact case likely disruptive value fixing addressing concern thanos take load prometheus suggested could reduce load prometheus retaining say log shift much possible work thanos however since latest data want running query alert rule clear close realtime thanos kept thanos rule unreliable would likely reduce load prometheus may temporary might simply shift scalability concern onto thanos sharding could splitshard prometheus instance perhaps dividing two tenant platform multicluster could one prometheus instance per cluster appears relatively straightforward would concern however split scale future well hit future scaling threshold necessary change divide shard bit planning would needed high availability recommended approach would run multiple instance prometheus configured scraping endpoint independently source replica however would also load balancer promql query prometheus api failover primary unresponsive clear work duplicate alert sent alertmanager doesnt feel like paved path prometheus operator saying currently implementing groundwork make possible figuring best approach definitely roadmap jan updated since managed prometheus managed service prometheus amp would address concern evaluated detail next section evaluation managed prometheus managed prometheus amp would outsource operational concern including performance scalability well management custom node group well examine financial cost potentially monitoring outside vpc change architecture adr aim understand issue specifically interested amazon managed service prometheus amp scaling amp scale automatically without configuration high availability amp highly available distributed across multiple automatically contrast question mark selfhosted see resilience amp relatively isolated cluster issue data kept durable storage away cluster lockin configuration syntax interface similar existing selfhosted prometheus maintain low lockin migration cost existing install monitoring namespace configured component terraform calling cloudplatformterraformmonitoring module installs kubeprometheusstack helm chart kubeprometheus among thing kubeprometheus contains number thing prometheus operator add kubernetesnative wrapper managing prometheus crds install prometheus alertmanager grafana thanosruler crds configuring servicemonitor podmonitor probe prometheusrule alertmanagerconfig allows specifying monitoring target kubernetes label kubernetes manifest grafana dashboard prometheus rule example configs nodeexporter scrape target alerting rule cluster issue high availability implemented yet resilience absence resilience offered presence backup node node prometheus instance running fails backup node ready run prometheus instance downtime capped minute backup node spec configuration running access prometheus data httpsgithubcomministryofjusticecloudplatformissuesissue prometheus config held resource servicemonitor prometheusrule alerting would work amp weve spiked terraform module implement httpsgithubcomministryofjusticecloudplatformterraformamp forwarding prometheus instance installed cluster scrape data keep copy remote write forward data amp transfer look pretty robust write ahead log queue forwarding prometheus could simply standard community helm chart httpsprometheuscommunitygithubiohelmcharts simple make upgrade easy dont kubeprometheus doesnt store much enough forward amp todo work link amp break period arrangement prometheus helm chart configures prometheus scrape resource prometheusio annotation however well let user define servicemonitor configs storage throw much data instead day limit day plenty long enough case thanos useful alertmanager amp alertmanagercompatible wed rule sending alert would configure create topic forward user slack channel grafana amazon managed grafana terraform support yet setup aws console meantime stick selfmanaged grafana work fine prometheus web interface previously amp headless come web interface prometheus rule alert existing cluster get prometheus rule httpsgithubcomkubernetesmonitoringkubernetesmixin kubeprometheus compiles json applies cluster new cluster thing new cluster let avoid kubeprometheus copy upgrade prometheus version well manually run jsonnet config generation paste resulting rule terraform module httpsgithubcomministryofjusticecloudplatformterraformampblobmainexamplerulestf still figure tenant rule alert tenant create prometheusrule resource kubectl apply environment repo going get config inserted amp could cronjob copy resource config amp something similar grafana check invalid rule dont get applied cause problem tenant cost look scale cost ingestion sample price ireland euampmetricsamplecount per metric sample next metric sample euampmetricstoragebytehrs per gbmo storage region amp released london region yet time writing however could run another region data ingestion regio free would pay grafana query component check usage related component still new cluster cloudwatch exporter node exporter ecr exporter pushgateway showing alert show user alert config firing currently alertmanagers web interface well equivalent maybe could run alertmanager purely purpose rely amp alertmanager actually sending would show inactive might expose difference functionality maybe give user readonly access console team workspace service could offer user prometheus workspace full monitoring stack fully control terraform module run maybe better everyone centralized one specialized user comparison', 'edgex metric collection approved original proposal approved tsc metric telemetry data defined count rate action resource circumstance edgex instance specific service example metric include number edgex event sent core data application service number request service api average time take process message application service number error logged service control plane event cpe defined event occur within edgex instance example cpe include device provisioned added core metadata service stopped service configuration changed cpe confused core data event core data event represent collection one sensordevice reading core data event represent sensing measured state physical world temperature vibration etc cpe represents detection happening inside edgex software adr outline metric telemetry collection handling note adr initially incorporated metric collection control plane event processing edgex architect felt scope design large cover one adr control plane event processing covered separate adr future system management service sma executor currently provide limited set metric requesting client party application system external edgex namely provides requesting client service cpu memory usage metric resource utilization service executable versus metric happening inside service arguably current system management metric provided container engine orchestration tool example docker engine underlying tooling info sma deprecated since ireland release removed future yet named release going forward user edgex want insight metric telemetry happening directly service task preforming word user edgex want telemetry service activity include sensor data collection much fast etc command request handled many device etc sensor data transformation done application service fast filtered etc sensor data export much sent many export failed etc api request often quickly many success versus failed attempt etc bootstrapping time time come available service activity processing time amount time take perform particular service function respond command request definition metric telemetry data defined count rate action resource circumstance edgex instance specific service example metric include number edgex event sent core data application service via message bus via device service application service ireland beyond number request service api average time take process message application service number error logged service collection dissemination metric data require internal service level instrumentation relevant service capture send data relevant edgex operation edgex currently offer service instrumentation metric first step implementation metric data edgex make metric data available subscribing party application system necessarily consume information future edgex may consume metric data example edgex may future metric number edgex event sent core data app service mean throttle back device data collection future edgex application service may optionally subscribe service metric message bus attaching appropriate message pipe service thus allowing additional filtering transformation endpoint control metric data service point feature supported consideration would made whether event sensor reading message metric message application service time edgex persist metric data except may retained part message bus subsystem mqtt broker consumer metric data responsible persisting data needed external edgex persistence metric information may considered future based requirement adopter demand feature general edgex metric meant provide internal service external application system better information happening inside edgex service associated device communicates requirement service push specified metric collected service specified configuration message endpoint supported edgex message bus implementation currently either redis pubsub mqtt implementation supported service configuration specifies message endpoint service metric metric message topic communication may secured unsecured application service provide mean export secured unsecured message pipe today configuration placed writable area user wish change configuration dynamically turning onoff metric consul change service configuration indicates metric available service service configuration allows edgex system manager select metric word providing configuration determines metric collected reported default metric turned default setting service report metric metric turned service collect sends metric designated message topic metric collection must pushed designated message topic appointed schedule schedule would designated configuration done way similar auto event device service initial implementation one scheduled time metric collected pushed designated message topic future may desire set separate schedule metric deemed complex initial implementation info initially proposed metric associated level allow metric turned level like level associated log message logging level metric data seems arbitrary time considered complex initial implementation may reconsidered future release based new requirementsuse case also proposed categorize label metric essentially allowing grouping various metric would allow group metric turned allow metric organized per group reporting time feature also considered beyond scope initial implementation reconsidered future release based requirementsuse case also proposed service offer rest api provide metric collection information metric collected ability turn collection dynamically deemed scope first implementation may brought back case requirement demand requested metric following list example metric requested edgex community adopter various service area metric would generally collected pushed message topic configured interval example minute defined interval sample metric thought relevant work group may reflect metric supported implementation exact metric collected service determined service implementers sdk implementers case app function device service sdks general following metric apply service service uptime time since last service boot cumulative number api request succeeded failed invalid avg response time millisecond appropriate unit measure apis avg max request size coresupporting latency measure time event take get core data latency measure time command request take get device service indication health event processed configurable period number event persistence number reading persistence number validation failure validation device identification number notification transaction number notification handled number failed notification transmission number notification retry application service processing time pipeline latency measure time event take get application service pipeline access time often failing export sent retried later time current store forward queue size much data size packaged sensor data sent endpoint volume number invalid message triggered pipeline number event processed device service number device managed device request may informative reading count rate note envisioned may additional specific metric device service example onvif camera device service may report number time camera tampering detected security security metric may difficult ascertain cross service metric given nature design per service basis global security metric may scope security metric collection copied service leading lot duplicate code also true threat detection based metric may feature best provided party based particular threat security profile number api request denied due wrong access token kong per service within given time number secret accessed per service name count access failure data persistence layer count service start restart attempt design proposal collect push architecture metric data collected cached service designated time kicked configurable schedule service collect telemetry data cache push designated message bus topic metric messaging cached metric data designated time marshaled message pushed preconfigured message bus topic metric message consists several keyvalue pair required name name metric serviceuptime required value telemetry value collected number hour service required timestamp time epoch timestampmilliseconds format data collected similar nature origin sensed data optional collection array tag tag set keyvalue pair string provide amplifying information telemetry tag may include originating service name unit measure associated telemetry value value type value additional value metric one value example histogram would include min max mean sum value metric name must unique service metric reported multiple service service uptime name required unique across service information key value tag etc string format placed json array within message body example representation example metric message body single value json nameserviceup value timestamp tagsservicecoredatauomdaystypeint example metric message body multiple value json nameapirequests value timestamp tagsservicecoredatauomcounttypeint mean rate raterate info key metric name must unique gometrics requires metric name unique per registry metric considered immutable configuration configuration unlike provided core data device service specify message bus type location metric message sent fact message bus configuration reuse service already message bus common message bus configuration defined common configuration service message queue configuration inclusive metric yaml messagequeue protocol redis tcp host localhost port type redis mqtt publishtopicprefix edgexeventscore standard existing core device topic publishing messagequeueoptional default mqtt specific enable environment variable override client identifier clientid devicevirtual connection information qos quality service value least exactly keepalive second must greater retained false autoreconnect true connecttimeout second skipcertverify false certkey file certkey pemblock specified additional configuration must provided service provide metric telemetry specific configuration area configuration likely different type service additional metric collection configuration provided include trigger collection telemetry metric cache sending appointed message bus define metric available turned false default list metric likely different per service key list metric name true false value specify metric topic prefix metric data published providing prefix edgextelemetrytopic name service metric name servicenamemetricname appended per metric allowing subscriber filter service metric name metric configuration defined writable area configurationtoml allow dynamic change configuration consul specifically writablewritabletelemetry area dictate metric collection configuration like yaml writable writabletelemetry interval publishtopicprefix edgextelemetry servicenamemetricname added publish topic prefix available metric listed metric listed false default serviceup false apirequests false info discussed future edgex release service may want separate message bus connection example one sensor data one metric telemetry data would allow qos setting message bus connection different would allow sensor data collection example messaged higher qos metric alternate approach could modify gomodmessaging allow setting qos per topic thereby avoid multiple connection initial release feature service connection therefore configuration metric telemetry well sensor data library support service gomodmessaging support golang service equivalent service service would determine metric collect push message bus common library chosen edgex language supported currently gometrics golang library publish application metric would allow edgex utilize versus construct library utilized thousand project provides mean capture various type metric registry sophisticated map metric published reported number well known system influxdb graphite datadog syslog gometrics library made original java package httpsgithubcomdropwizardmetrics similar package would selected created per core meeting important provide implementation adopter edgex see difference whether metricstelemetry collected service configuration metric service structure based metric collection mechanism service specifically provided device service sdk may operate differently cover configuration resulting metric message edgex message bus must formattedorganized consideration gometrics golang library library would provide package service expectation parity service may difficult achieve given feature gometrics gometrics still require edgex team develop bootstrapping apparatus take metric configuration register metric defined configuration gometrics gometrics would also require edgex team develop mean periodically extract metric data registry ship via message bus something current gometrics library gometrics offer ability data reported system would required edgex expose capability possibly apis user wanted export subsystem addition message bus per kamakura planning meeting noted gometrics already dependency code due party package see httpsgithubcomedgexfoundryedgexgoblobfddafbcbccffbeaccgosuml community question gometrics per monthly architect meeting manages telemetry data persistence memory database etc memory registry essentially keyvalue store key metric name offer query api order easily support adr suggested rest api yes metric stored registry metricregistry essentially map get getall method provided query metric gometrics package feature become requirement side dozen type metric collection simple gauge counter sophisticated structure like histogram stored registry map data made available report export publish various integrated package influxdb graphite datadog syslog etc nothing mqtt base message service would implemented scratch metrictelemetry count reset needed happen whenever post message bus would work rest yes unregister reregister metric rest api would constructed call capability gometrics another library called opencensus multilanguage metric library including library feature rich opencensus also roughly size gometrics library additional open question consideration given allow metric placed different topic per name add topic name like device name device service future consideration consideration given incorporate alternate protocolsstandards metric collection httpsopentelemetryio httpsgithubcomstatsd metric already library pulled service package may side implementation per monthly architect meeting decided gometrics service creating library open census service either findpick package provides similar functionality gometrics implement internally something providing mvp capability gometrics help avoid much service bloat since already service per monthly architect meeting decided implement metric service first per monthly architect meeting decided support rest api service would provide information metric service provides ability turn instead writable configuration allow consul mean change configuration dynamically adopter chooses consul configuration regard metric collection configuration circumstance would static external api requested future external tool rest api may added see older version idea implementation case per core working group meeting many previous meeting adr decided edgex approach one push via message busmqtt pull rest api approach require service collect metric telemetry specific service collecting service must either push onto message topic message cache memory storage mechanism depending whether storage durable allow rest api call would cause data pulled cache provided response rest call given mechanism require collection process belief push probably preferred today adopter future highly desired pull rest api could added along cache metric telemetry pull per core working group meeting importantly edgex making metric telemetry available internal edgex message bus adopter would create something pull data bus way voiced several call important adopter realize today edgex providing last mile metric data adopter must provide last mile pick data topic make available system something per core working group meeting many previous meeting adr decided prometheus prometheus library mean provide metric reason many push pull favored first implementation see point also see similar debate online plusesminuses approach edgex want make telemetry data available without dictating specific mechanism making data widely available specific debate centered prometheus popular collection library inside service collect data well monitoring system watchdisplay data prometheus popular open source approach felt many organization choose influxdbgrafana datadog appdynamics cloud provided mechanism homegrown solution collect analyse visualize otherwise telemetry therefore rather dictating selection monitoring system edgex would simply make data available whereby organization could choose monitoring systemtooling noted edgex approach merely make telemetry data available message bus prometheus approach would provide collection well backend system otherwise collect analyse display etc data therefore typically work done adopter get telemetry data proposed edgex message bus solution something reporter come gometrics allow data taken directly gometrics pushed intermediary prometheus monitoringtelemetry platform referenced capability may well supported beyond scope edgex adr however even without reporter felt relatively straightforward exercise part adopter create application listens edgex metric message bus make data available via pull rest api prometheus desired prometheus client library would added service would bloat service although available benefit gometrics already hashicorp consul already service implementation detail gometrics package offer following type metric collection gauge hold single integer int value example number notification retry operation update gauge get gauge value example code metricsnewgauge gupdate set value gupdate set value fmtprintlngvalue print current value gauge counter hold integer count counter could implemented gauge example current store forward queue size operation increment decrement clear get counter count value metricsnewcounter cinc add one current counter cinc add current counter making cdec decrement counter making fmtprintlnccount print current count counter meter measure rate int event time one five fifteen minute interval example number rate request service api operation provide total count event well mean rate minute rate metricsnewmeter mmark add one current meter value timesleep timesecond allow time mmark add one current meter value timesleep timesecond allow time mmark add one current meter value timesleep timesecond allow time mmark add one current meter value timesleep timesecond allow time fmtprintlnmcount print fmtprintlnmrate print fmtprintlnmrate print fmtprintlnmrate print fmtprintlnmratemean print histogram measure statistical distribution value int value collection value example response time apis operation update get min max count percentile sample sum variance collection metricsnewhistogrammetricsnewuniformsample hupdate hupdate hupdate hupdate fmtprintlnhmax print fmtprintlnhmin print fmtprintlnhmean print fmtprintlnhcount print fmtprintlnhpercentile print fmtprintlnhvariance print fmtprintlnhsample print timer measure rate particular piece code called distribution duration example often app service function get called long take get function operation update get min max count rate rate rate mean percentile sum variance collection metricsnewtimer tupdate timesleep timesecond tupdate timesleep timesecond tupdate timesleep timesecond tupdate timesleep timesecond fmtprintlntmax print fmtprintlntmin print fmtprintlntmean print fmtprintlntcount print fmtprintlntsum print fmtprintlntpercentile print fmtprintlntvariance print fmtprintlntrate print fmtprintlntrate print fmtprintlntrate print fmtprintlntratemean print note gometrics package offer variant like gaugefloat hold bit float consequence global configuration turn metric offon edgex doesnt yet global config service given potential service publishes metric message topic implementation unless service different pipe topic allow multiple publisher like app service implementation allow service sends different topic probably avoid service bloat edgex enterprise system implement concise economical way metric help side since already module edgex module brought default care concern must given cause much bloat side sma report service cpu memory configuration provides mean startstoprestart service currently outside scope new metric collectionmonitoring future party mechanism offer capability sma may warrant sma irrelevant existing notification service serf send notification via alternate protocol outside edgex communication service provided generic communication instrument micro service independent type data concern future notification service could configured subscriber metric message trigger appropriate external notification via email smtp etc reference possible standard implementation open telemetry statsd gometrics opencensus']"
9,136, Containerized Application Development,['Containerized Application Development'],"['docker container', 'dockerfiles', 'docker', 'docker image', 'dockerhub', 'dockerfile', 'dockercompose', 'kubernetes', 'govuk docker', 'container registry']","['buildah discovery work desire user able build docker image artifact job creates security problem technical limitation within docker adr result discovery work discern whether solution building docker image securely discovery focussed evaluating software product called buildah technical problem worker run within docker container may build another docker image artifact requires host server docker socket exposed worker container docker daemon run root full control host server meaning malicious code running within worker docker container could trivially gain access host system mounting via host system root docker support rbac control restrict access socket way provided docker prevent privilege escalation method occuring buildah solves problem able build docker container without docker vulnerable socket buildah discovery note root access run buildah command requires privilege docker run bit ackward install could package meaning wed maintain requires nonstandard repos ppas installation ppaalexlarssonflatpak lead flatpak developer ppagophersarchive ppaprojectatomicppa buildah official repo build image send local docker dockerio ecr compatible image worked well overall support buildarg volume documentation recommend rebuild creates container specifically buildah run create artifact docker image upload somewhere ready consumption buildah artifact image stored worker faster build run mean manage image nuking old one may expose docker socket reintroduce problem trying avoid buildah image stored dockerio image uploaded creation downloaded executed making thing slower uploaded public dockerio wed careful exposing secret uploaded private dockerio data sovereignty concern work upon secret management allow upload download docker image stored within ecr certain possible specifically noted fast managed iam role endpoint security group etc version controlled kept private similar tool buildah comparison table buildah address problem docker container privilege escalation feature rich support typical case feature compatible docker provides mean provide docker image artifact consequence indepth discovery product may appropriate discovery product might neccessary', 'docker image ruby application order run govuk application docker environment docker image govuk previously created docker image various application enable publishingeetests suite run containerised version govuk publishing stack image defined dockerfile root project repository example dockerfile project essentially defacto docker image govuk application attempting reuse image govuk docker development environment revealed number problem time ruby project gem dependency changed image would rebuilt required reinstalling every single gem could slow process gem dependency stored within project image gem could shared across application meant initialisation process multiple govuk apps could frustratingly slow gem installed different image also resulted high disk usage image exercised publishingeetests ran production mode meant didnt necessarily sufficient dependency development usage google chrome testing image embedded application file meant default edits file would require image rebuilt thus considered whether would appropriate modify image solve problem create different image govuk docker decided reuse existing image would resolve problem different image approach govuk docker problem could resolved reasoned existing image represent standard industry practice web application image practice optimised running production instance application felt production development sufficiently distinct would simpler separate image try consolidate instead created dockerfile stored govuk docker majority govuk docker ruby project base dockerfile contains collection common dependency significant number project project additional dependency dockerfiles allowed ensure project necessary development dependency resolved problem gem installation shared docker mount allowed installation thus updating ruby version gem done outside docker build process meaning task achieved without rebuilding image shared mount across project allows ruby gem dependency reused resolved problem embedded file mount govuk directory mount allows container access application code code modified without requiring image rebuild broad directory allows project access project necessary assist working local version gem consequence govuk docker project lacking individual dockerfile substantially docker configuration manage reduced risk inconsistency also reduces difficulty adding new project govuk docker container govuk docker lack property unit software package code dependency may surprising however unclear container principle easily applied rubyonrails development environment without problem experienced storing dockerfile govuk docker weak coupling minimal set system package installed image application mean change system package coordinating application repository govuk docker however rare occurrence due minimal number system package dockerfile govuk docker govuk multiple dockerfiles application raising risk confusion developer remains dockerfile root repository developer may expect somewhat related govuk docker project', 'layout default title adr navorder permalink record containerization sdr application drafted deciders vivian wong julian morley infrastructure team operation team problem statement currently sdr infrastructure managed puppet sdr application deployed capistrano approach number shortcoming result sdr instability requires significant time infrastructure operation team troubleshooting also hinders agility provisioning new server take time requires manual intervention shortcoming include server inconsistency server vary number way environment dorservices app stage prod within environment dorservicesappa dorservicesappb variation necessary connected different storage others unnecessary problematic different version ruby installed different way different version library etc problematic variation due manual intervention server different puppet specification kept server must manually kept sync past several year also complicated server migration different environment running different development inconsistency local development environment differ deployment environment significant degree inconsistency lead surprise moving application deployment environment course environment cannot identical difference easier detect issue development time puppet capistrano mismatch relying puppet capistrano make activity like upgrading ruby difficult requires coordinating change location performing manual operation much manual intervention overall provisioning deploying maintaining application require much manual intervention infrastructure operation team canonical specification application requirement application requirement library port storage mount must gleaned multiple source multiple puppet configuration devopsdocs repository readmes sharedconfigs etc make server provisioning maintenance error prone proposed containerization particular docker image deployment strategy sdr application variety approach containerization described considered driver consistent deployment sdr application provisioning server separation responsibility operation handle provisioning server infrastructure handle deployment application maintain current approach realistic technical requirement given current priority staffing funding level considered docker image running kubernetes docker image running puppetmanaged server image built server docker image running puppetmanaged server image pulled docker repository docker hub note possible take incremental approach viz start docker image running puppetmanaged server image built server move image pulled docker repository move puppetmanaged server kubernetes outcome determined pro con docker image running kubernetes pro kubernetes provides advanced provisioning feature autoscaling pro kubernetes provides ability quickly spinup teardown testing environment con kubernetes extremely complicated con adopting kubernetes project likely time sdr would candidate docker image running puppetmanaged server image built server approach puppet would manage server running docker puppet would longer handle ruby library etc would continue manage storage firewall port provide secret via environment variable docker compose would run one container server example server might run apache container shibboleth rail container web application server might run multiple instance sidekiq worker container similar current server running multiple sidekiq worker process minimal orchestration requires good fit docker compose also providing ability environment specific configuration together docker docker compose provide clear canonical specification application requirement general container running server would match server current responsibility example argoprod would run argo web application sidekiq worker wouldnt also run dor service app puppet managed storage would shared docker container addition capistrano would deploy application leverage existing tooling deploying code configuration including handling testing branch however instead starting process server capistrano build docker image startstop docker container running image pro reuses existing puppetmanaged infrastructure pro reuses existing tooling deploying code configuration secret management pro requires minimal change development deployment process con guarantee identical container running different server docker image running puppetmanaged server image pulled docker repository similar approach except code deployed server docker image built server instead docker image built elsewhere developer machine circleci pulled server pro guarantee identical container running different server pro consistent prevailing industry practice con requires change development process handle automation build image con requires new undetermined approach configuration secret management preferable infrastructure team able manage configuration without operation team involvement similar existing sharedconfigs approach scope continuous deployment goal adr replace sharedconfigs better secret management may make sense consider adjacent work containerization core service database redis rabbitmq consideration open question shibboleth run container uit firewall load balancer problematic container would cron job handled security best practice process followed container']"
10,144, Messaging System Design Approach,['Messaging System Design Approach'],"['message bus', 'event sourcing', 'system event', 'device service', 'messagebus', 'event', 'command service', 'api', 'rabbitmq', 'implementation']","['system event adr submitter lenny goodell intel change log approved referenced case system event device system event aka control plane event cpe new edgex adr address system event device case extensible design address system event case may identified future extensible design approach fact system event produced consumed different edgex service make architecturally significant warranting adr proposed design address system event device case core metadata publish new systemevent dto edgex messagebus device added updated deleted consumer system event subscribe messagebus receive new systemevent dto data transfer object dto new systemevent dto contain following data describing system event source publisher system event coremetadata type type system event device action action triggered system event addupdatedelete timestamp creation datetime system event owner owner data system event deviceonvifcamera device owner optional based type tag key value pair add addition system event deviceprofileonvifcamera optional based type andor action detail data detail important system event device dto addedupdateddeleted device optional object varies based type andor action object similar objectvalue reading dto note defined dto suffice future system event case messagebus service publish system event core metadata must connect edgex messagebus messagebus configuration similar core data design assumes core metadata capability configuration due planned implementation service metric publishtopicprefix property core metadata messagequeue configuration system event set edgexsystemevent messagebus topic new systemevent dto published multilevel topic allowing subscriber filter topic format topic system event publishtopicprefixsourcetypeaction source publisher system event coremetadata type type system event device action action triggered system event add specific case may add additional level needed device system event case add following level owner owner data system event deviceonvifcamera device owner profile device profile associated device onvifcamera example example system event subscription topic edgexsystemevent system event edgexsystemeventcoremetadata system event core metadata edgexsystemeventcoremetadatadevice device system event core metadata edgexsystemeventcoremetadatadeviceadddeviceonvifcamera add device system event deviceonvifcamera edgexsystemeventcoremetadatadeviceonvifcamera device system event device created onvifcamera device profile consumer consumer device system event likely custom application service described system event device change required app function sdk since already support processing different type via target type capability developer custom application service consume system event following set target type dtossystemevent creating instance applicationservice newappservicewithtargettype factory function write custom pipeline function expects new systemevent dto process accordingly similar tolineprotocol pipeline function expects metric dto servicesmodules impacted core metadata service service single point device addupdatedelete producer device system event core contract module new systemevent dto added repository camera management app service example device system event implemented camera management example updated consume device sdkservice future device system event implemented device sdks switch receiving via messsagebus rather rest callback core metadata anything beyond recognizing future enhancement outofscope adr consideration design approach future case system event device profile system event device service whenif deemed needed another design approach considered support notification send system event via rest would require consumer create subscription receive system event via rest endpoint consuming service subscription would created existing support notification subscription rest api likely subscription would specific event type system event would published posting support notification notification rest api would forward via rest post service subscribed particular system event system event dto would still sent via rest approach complex requires consumer service new rest endpoint receive system event relies rest rather messaging thus approach chosen design satisfy system event device case well possibly future system event case related adrs metric collection adr edgex messagebus publish subscribe service metric northsouth messaging adr edgex messagebus publish subscribe commandsresponses reference control plane event cpe aka system event initially proposed part metric collection adr early march may find discussion relevant cpe adr designarchitecture discussion since march initial adr edgex service level metric collection hanoi', 'device service send event via message bus message bus implementation device sdk device sdk core data persistence event dto validation message envelope application service messagebus topic configuration device service messagequeue core data messagequeue application service messagebus binding secure connection consequence approved currently edgex event sent device service via http core data put event messagebus optionally persisting database adr detail device service send edgex event service via edgex messagebus note though design centered device service cross cutting impact edgex service module note adr dependent secret provider link tbd provide secret secure message bus connection message bus implementation multiple device service may publishing event messagebus concurrently zmq valid multiple device service configured publish zmq allows single publisher zmq still valid one device service publishing event mqtt redis stream valid multiple device service required support multiple publisher implementation currently available service base device service yet messagebus implementation see device sdk detail note documentation clear zmq device sdk device sdk take advantage existing gomodmessaging module enable edgex messagebus new bootstrap handler created initializes messagebus client based configuration see configuration section detail device sdk enhanced optionally publish event messagebus anywhere currently post event core data publish post controlled configuration publish default see configuration section detail device sdk device sdk implement messagebus abstraction similar one gomodmessaging first implementation type mqtt redis stream tbd abstraction allows future implementation added case warrant additional implementation sdk sdk enhanced optionally publish event messagebus anywhere currently post event core data publish post controlled configuration publish default see configuration section detail core data persistence design event sent directly application service going core data thus persisted unless change made core data allow event optionally continue persisted core data become additional secondary optional subscriber event messagebus event persisted received core data also retain ability receive event via http persist publish messagebus done today allows flexibility device service configured post event configured publish event transition device service capability publishing event future new publish approach proven may decide remove posting event core data device sdks existing persistdata setting ignored code path subscribing event since reason persist event race condition marked pushed core data persisting event received messagebus core data may finished persisting event application service processed event requested event marked pushed decided remove mark pushed capability rely time based scrubbing old event event dto development part ireland release event published messagebus event dto already implemented core data addevent api validation service receiving event dto messagebus log validation error stop processing event message envelope edgex service currently custom message envelope data published messagebus envelope wrap data metadata contenttype json cbor correlationid obsolete checksum checksum data cbor encoded identify event api mark pushed checksum longer needed event dto requires set device service always api mark event pushed message envelope updated remove property sdk recreate message envelope application service part api consumption work ireland app service sdk changed expect receive event dtos rather event model also updated longer expect checksum currently message envelope note change must occur consumption directly tied effort app service sdk enhanced secure messagebus connection described see secure connection detail messagebus topic note change recommended required design provides good opportunity adopt currently core data publishes event simple event topic application service running receive every event published whether want event filtered filterbydevicename filterbyresourcename pipeline function application service still receives every event process event extent could cause load issue deployment many device large volume event various device verbose device application service interested note current filterbydevicename good device name known statically instance device defined deviceprofilename really filterbydeviceprofilename allows multiple instance device filtered rather single instance api adding deviceprofilename event ireland filter possible pubsub system advanced topic schema take advantage application service filter event application service actual want publisher event must add deviceprofilename devicename sourcename topic form edgexeventsdeviceprofilenamedevicenamesourcename sourcename resource command name create event allows application service filter event device want subscribing deviceprofilenames specific devicenames specific sourcenames example subscribe topic schema edgexevents event core data subscribe topic schema edgexeventsrandomintegerdevice event device created randomintegerdevice device profile edgexeventsrandomintegerdevicerandomintegerdevice event randomintegerdevice device edgexeventsrandomintegerdeviceint event reading fromint device resource device created randomintegerdevice device profile edgexeventsmodbusdevicehvacvalues event reading hvacvalues device command device created modbusdevice device profile messagebus abstraction allows multiple subscription application service could specify receive data multiple specific device profile device creating multiple subscription edgexeventsrandomintegerdevice edgexeventsrandombooleandevice currently app sdk allows single subscription topic configured could easily expanded handle list subscription see configuration section detail core data existing publishing event would also changed new topic schema one challenge core data doesnt currently know deviceprofilename devicename receives cbor encoded event doesnt decode event published messagebus also core data doesnt know sourcename api enhanced change addevent endpoint event eventprofiledevicesource deviceprofilename devicename sourcename always know matter request encoded new topic approach enabled via publisher publishtopic deviceprofilename devicenameand sourcename added configured publishtopicprefix toml publishtopicprefix edgexevents added publish topic prefix see configuration section detail configuration device service device service following additional configuration allow connecting publishing messagebus describe messagebus topic section publishtopic include deviceprofilename devicename messagequeue messagequeue section added similar core data today publishtopicprefix instead topicto enable secure connection username password replaced clientauth secretpath see secure connection section detail added enabled property control whether device service publishes messagebus post core data toml messagequeue enabled true protocol tcp host localhost port type mqtt publishtopicprefix edgexevents deviceprofilenamedevicenamesourcename added publish topic prefix messagequeueoptional default mqtt specific enable environment variable override client identifier clientid device service key connection information qos quality sevice value least exactly keepalive second must greater retained false autoreconnect true connecttimeout second skipcertverify false certkey file certkey pemblock specified clientauth none valid value none usernamepassword clientcert secretpath messagebus path secret store clientauth none core data core data also require additional configuration able subscribe receive event messagebus describe messagebus topic section publishtopicprefix deviceprofilename devicename added create actual public topic messagequeue messagequeue section changed topic property change publishtopicprefix subscribeenabled subscribetopic added device service configuration username password replaced clientauth secretpath secure connection see secure connection section detail addition boolean subscribeenabled property control service subscribes event messagebus toml messagequeue protocol tcp host localhost port type mqtt publishtopicprefix edgexevents deviceprofilenamedevicenamesourcename added publish topic prefix subscribeenabled true subscribetopic edgexevents messagequeueoptional default mqtt specific enable evnironment variable override client identifier clientid edgexcoredata connection information qos quality sevice value least exactly keepalive second must greater retained false autoreconnect true connecttimeout second skipcertverify false certkey file certkey pemblock specified clientauth none valid value none usernamepassword clientcert secretpath messagebus path secret store clientauth none application service messagebus similar application service messagebus configuration change allow secure connection messagebus username password replaced clientauth secretpath secure connection see secure connection section detail toml messagebusoptional mqtt specific client identifier clientid app sevice key connection information qos quality sevice value least exactly keepalive second must greater retained false autoreconnect true connecttimeout second skipcertverify false certkey file certkey pemblock specified clientauth none valid value none usernamepassword clientcert secretpath messagebus path secret store clientauth none binding binding configuration section require change subscribe topic scheme described messagebus topic section filter event specific device profile device subscribetopic change string property containing single topic subscribetopics string property containing comma separated list topic allows flexibility property single topic wild card application service receives event today receive event randomintegerdevice randombooleandevice profile toml binding typemessagebus subscribetopicsedgexeventsrandomintegerdevice edgexeventsrandombooleandevice receive event randomintegerdevice randomintegerdevice profile toml binding typemessagebus subscribetopicsedgexeventsrandomintegerdevicerandomintegerdevice receives event toml binding typemessagebus subscribetopicsedgexevents secure connection stated earlier adr dependent secret provider alllink tbd adr provide common secret provider edgex service access secret available messagebus connection secured via following configurable client authentication mode follows similar implementation secure mqtt export secure mqtt trigger application service none authentication usernamepassword username password authentication clientcert client certificate key authentication secret specified pulled secret provider configured secretpath secret injected secret provider scope adr covered secret provider link tbd adr consequence sdk doesnt support zmq redis stream must mqtt broker running device service configured publish messagebus since weve adopted publish topic scheme deviceprofilename devicename api must restrict character device name allowed topic issue api already exists restricting allowable character rfc suffice newer zmq may allow multiple publisher requires investigation likely rework zmq implementation gomodmessaging found mark push api removed core data core data client app sdk consider moving app service binding writable scope adr', 'northsouth messaging approved tsc vote proposed design today data flowing sensorsdevices southside edgex enterprise application database cloudbased system northside accomplished via rest message bus sensor device data collected device service sent via rest message bus core data core data relay data application service via message bus sensor data also sent directly device service application service via message bus bypassing core data message bus implemented via redis pubsub default via mqtt application service data sent northside endpoint number way including via mqtt summary data collected sensor device sent southside northside entirely message bus technology desired today communication party system enterprise application cloud application etc edgex order acuate device get latest information sensor accomplished via rest party system make rest call command service relay request device service also rest built mean make messagebased request edgex devicessensors manages note rest call optionally made via api gateway order provide access control future release edgex desire allow party system make request southside via message bus specifically party system send command request command service via external message broker command service would relay request via message bus managing device service via one allowed internal message bus implementation could mqtt redis pubsub today device service would message trigger action devicesensor receives rest request respond via message bus back command service turn command service would relay response party system via external message bus summary adr proposes core command service add support external mqtt connection manner app service provide external mqtt connection allow act bridge internal message bus implemented via either mqtt redis pubsub external mqtt message bus note purpose initial northtosouth message bus communication external party communication command service limited mqtt core command message bus bridge core command service serve edgex entry point external northtosouth message bus request south side party system granted access edgex internal message bus therefore order implement north south communication via message bus specifically mqtt command service take message party external mqtt topic pas internally onto edgex internal message bus eventually routed device service devicessensors southside reverse response message southside also sent internal edgex message bus command service bridged external mqtt topic respond party system requester note note ekuiper allowed access directly internal edgex message bus special circumstance party external system communication ekuiper sister project deemed edgex reference implementation rule engine future release edgex even ekuiper may routed external internal message bus bridge better decoupling security message bus subscription publishing command service require mean publish message device service via edgex message bus internal message bus would messaging client gomodmessaging create new messageclient connect message bus publish designated request message topic see topic configuration command service also connect edgex message bus internal message bus order receive response device service request message bus made core command gomodmessaging messageclient subscribe receive response message device service similar fashion device service subscribe publish edgex message bus internal message bus get command request push back response command service lang device service like command service gomodmessaging module messagingclient get command request send command response edgex message bus based device service subscribe publish edgex message bus internal message bus note device service already gomodmessaging publishing eventsreadings message bus internal message bus command service also subscribe party mqtt topic external message bus order get command request party system command service relay command request appropriate device service via internal message bus forming message bus message bus bridge likewise command service accept response device service edgex message bus internal message bus publish response party system via party mqtt topic external message bus command query via command service today party system make rest call core command get possible command executed two query rest api endpoint deviceall get command device devicenamename get command specific device name stand reason party system want send command via messaging would also want get understanding command available via messaging reason core command service also allow message request get command get command particular device name word core command service must support command query via messaging support command request via messaging case command query rest response include actual rest command endpoint example rest query would return core command path url parameter construct rest command request shown example json corecommands name coolingpoint get true path apivdevicenametestdevicecommandcoolingpoint url httplocalhost parameter resourcename resource valuetype int messaging make query response message must return information pas message appropriate topic make command request therefore query response messaging would include something like following json corecommands name coolingpoint topic edgexcommandrequesttestdevicecoolingpointget parameter resourcename resource valuetype int name coolingpoint topic edgexcommandrequesttestdevicecoolingpointset parameter resourcename resource valuetype int note per core meeting json serf general example implementation address getset readwrite differentiation considered implementation detail resolved developer note query response contain url since assumed broker address must already known order make query message structure rest based command request response http request line contains important information path target request http method type indicating get put request http line provides information response code body payload http message contains request detail parameter device put call response information event associated reading get call since message bus protocol lack generic message header mechanism http providing requestresponse metadata accomplished defining message envelope object associated requestresponse therefore message described adr must provide json envelope payload object requestresponse message topic name act like http path method rest request topic name specify device receiver command request path http request message envelope message defined adr json formatted request response share common base structure outer json object represents message envelope convey metadata requestresponse correlation identifier added relayed request message well response message envelope party system know associate response original request note correlation see article detailed description unique value added every request response involved transaction could include multiple requestsresponses one microservices meant correlate request response meant label every message involved potentially multirequest transaction request identifier returned response request providing traceability single requestresponse envelope also contain api version something provided http path rest command request http may also contain dspushevent dsreturnevent query parameter get command optionally provided keyvalue pair represented message envelope query parameter optionally allows parameter future json correlationid aeaccbcdabfe apiv queryparams dspusheventtrue dsreturneventtrue note rest request dsreturnvent message envelope would returned payload would event return command message payload request message payload command service relayed device service would mimic httprest request body payload provides detail needed executing command south side example get put message note envelope wrap encases message payload payload may empty typical get request json correlationid aeaccbcdabfe apiversion requestid eeafebeb queryparams dspusheventtrue dsreturneventtrue correlationid aeaccbcdabfe apiversion requestid eeafebeb payload ahutargettemperature ahutargetband ahutargethumidity accuracy value note payload could empty therefore optional message structure exemplified top example response message payload would contain response south side typically edgex eventreading object case get request would also include error message detail example response message get put request shown note message envelope wrap response payload json correlationid aeaccbcdabfe apiversion requestid eeafebeb errorcode payload event apiversion fafbfccfafa devicename string profilename string created origin reading string tag gatewayid houstonstore latitude longitude correlationid aeaccbcdabfe apiversion requestid eeafebeb errorcode payload message string note get command response may include cbor data message envelope content type indicator indicate payload either cbor json message envelope content type indicator rest communication message bus communication alert open discussion per working group meeting review validating message version case per lennyintel validate incoming rest request particular version apis per monthly architect meeting yes validating done rest either implement validation message communication also add validation rest communication future levski release add future planning meeting topic api version response per iainanderson would api version implied request would mean response per monthly architect meeting dto already api version payload automatically additionally decided add message envelope reason rest path request includes version message communication lack urlpath beneficial also include version envelope addition payload part dto easily determined envelope without dig around payload per core meeting discussion really request redundant based already correlation per monthly architect meeting request correlation two different thing request returned response request providing traceability single requestresponse correlation track across entire transaction many service requestresponses providing traceability across transaction often across many service needed kept request payload per monthly architect meeting kept message envelope message payaload per core meeting discussion code mimic resthttp code response really want mimic http message bus approach suggested farshidtz maybe error boolean message indicate error condition per monthly architect meeting errorcode provide indication error errorcode error indicating error two enums error condition today future determine additional error add enums error code number error errorcode set payload contains message string indicating information error error errorcode message string payload code error code belong payload envelope would header rest reference iotaap mqtt rest bridge provides code message string translation example mean handle problem something similar per monthly architect meeting errorcode message envelope payload error payload contains single message string query message payload request message payload query command service would mimic httprest request body payload provides detail needed executing command south side example query get command note envelope wrap encases message payload payload empty query parameter include offset limit per rest counter part json correlationid aeaccbcdabfe apiversion requestid eeafebeb queryparams offset limit example query get command specific device name device name would topic query message would without information removed message queryparams optional correlationid aeaccbcdabfe apiversion requestid eeafebeb response message payload query would contain information necessary make messagebased command request example response message shown note message envelope wrap response payload json correlationid aeaccbcdabfe apiversion requestid eeafebeb errorcode payload apiversion devicecorecommands devicename testdevice profilename testprofile corecommands name coolingpoint get true topic edgexcommandrequesttestdevicecoolingpointget url brokeraddress parameter resourcename resource valuetype int devicename testdevice profilename testprofile corecommands name coolingpoint set true topic edgexcommandrequesttestdevicecoolingpointset url brokeraddress parameter resourcename resource valuetype string resourcename resource valuetype bool topic naming party system topic party system application must publish command request message edgex specified mqtt topic external message bus subscribe response message topic follow following pattern publishing command request topic edgexcommandrequestdevicenamecommandnamemethod subscribing command response topic edgexcommandresponse query following topic publishing query command request topic edgexcommandqueryrequest subscribing query command response topic edgexcommandqueryresponse command service topic command service must subscribe request topic party mqtt topic external message bus get command request publish topic send device service via edgex message bus internal message bus subscribe response message topic device service internal publish response message topic party mqtt broker external message topic command service would follow following standard subscribing party command request topic edgexcommandrequest publishing device service request topic edgexcommandrequestdeviceservicedevicenamecommandnamemethod subscribing device service command response topic edgexcommandresponse publishing party command response topic edgexcommandresponsedevicenamecommandnamemethod query following topic subscribing party command query request topic edgexcommandqueryrequest publishing party command query response topic edgexcommandqueryresponse device service topic device service must subscribe edgex command request topic internal message bus publish response message edgex command response topic following naming standard applied topic name subscribing command request topic edgexcommandrequest publishing command response topic edgexcommandresponsedeviceservicedevicenamecommandnamemethod configuration edgex command service device service must contain configuration needed connect publishsubscribe message topic edgex message bus internal includes configuration access message bus secure insecure command service must also provided configuration connect party mqtt broker topic external communication may done secure insecure fashion core command service provided access party mqtt broker external similar edgex application service command service access external mqtt broker get command request send party response require command service two message queue configuration setting internal external command service configuration example command service configuration provided toml messagequeue internalmessagequeue protocol redis host localhost port type redis requesttopicprefix edgexcommandrequest publishing request device service deviceservicedevicenamecommandnamemethod added publish topic prefix responsetopic edgexcommandresponse subscribing device service response authmode usernamepassword required redis messagebus secure insecure secretname redisdb externalmqtt protocol tcp host localhost port requestcommandtopic edgexcommandrequest subscribing party command request responsecommandtopicprefix edgexcommandresponse publishing response back party system devicenamecommandnamemethod added publish topic prefix requestquerytopic edgexcommandqueryrequest responsequerytopic edgexcommandqueryresponse note core command contains messagequeue configuration today additivenew configuration therefore backward compatible edgex implementation device service configuration example device service configuration provided toml messagequeue already existing message queue configuration sending eventsreadings message bus protocol redis host localhost port type redis authmode usernamepassword required redis messagebus secure insecure secretname redisdb publishtopicprefix edgexeventsdevice added publish topic prefix messagequeueoptional default mqtt specific enable environment variable override client identifier clientid devicerest connection information qos quality service value least exactly keepalive second must greater retained false autoreconnect true connecttimeout second skipcertverify false certkey file certkey pemblock specified new configuration allow device service also communicate via message bus core command commandrequesttopic edgexcommandrequest subscribing inbound command request commandresponsetopicprefix edgexcommandresponse publishing outbound command response added publish topic prefix note device service configuration existing based already communicate message bus publishing eventsreadings last two line added allow device service subscribe publish command message fromto message bus edgex service internal message bus request application service edgex service future may want also message communication make command request application service make command request today via rest order support following added command service also internal request topic internal response topic prefix configuration allow internal edgex service make command request query request toml messagequeue internalmessagequeue protocol redis host localhost port type redis requesttopicprefix edgexcommandrequest publishing request device service deviceservicedevicenamecommandnamemethod added publish topic prefix responsetopic edgexcommandresponse subscribing device service response internalrequestcommandtopic commandrequest subscribing internal command request internalresponsecommandtopicprefix commandresponse publishing response back internal service devicenamecommandnamemethod added publish topic prefix internalrequestquerytopic commandqueryrequest internalresponsequerytopic commandqueryresponse authmode usernamepassword required redis messagebus secure insecure secretname redisdb new command message client created allow internal service app service instance conveniently message bus communication core command client service configuration also expanded include corresponding topic usemessagebus flag enables new messaging based commandclient created example client configuration would look something like following toml client clientscorecommand usemessagebus true protocol redis host localhost port commandrequesttopicprefix commandrequest devicenamecommandnamemethod added publish topic prefix commandresponsetopic commandresponse commandqueryrequesttopic commandqueryrequest commandqueryresponsetopic commandqueryresponse question separate topic device would one device service suffice defined devicename parameterized topic one topic sufficient device service edgexcommandrequest would client non edgex service application want get list available command via message instead calling rest valid question could provided via later addition command service service like metadata future tackled immediately dynamic configuration message subscription user friendly operation today requiring configuration change future might want think creating additional apis addingupdatingdeletingquery external subscription store redisdb one could also consul change configuration would require configuration question added writable section acceptable one response published device service correlation send back acknowledged scheduled starting done correlation life span tofrom initial requester response back requester would make sense echo command name response reality check solved via topic naming also per lennyintel needed dont http response response topic doesnt extra path info request correlation needed match response request make complex would sendingreceiving binary data cbor supported northsouth message implementation today command service device service support cbor get operation set sdk suppports suggest getting feature parity place sdks exploring cbor support messaging binarycbor payload update per monthly architect meeting support get cbor payload message bus communication nonedgex party service application would bypass api gateway per monthly architect meeting since command service serving external internal message bus broker issue worth calling message bus security paradigm quite whats provided api gateway provides access control edgex api gateway security configuration defined edgex instance edgex service act bridge external message bus external bus properly configured application bus interact edgex instance thus security configuration defined external broker edgex finally note mqtt broker support topic acls based client username note number open question message structure section still addressed alert per tsc meeting discussion around error response reopened still polite disagreement whether keep error response simple documented adr offer errorcode enumeration similar http response code common problem part discussion question whether error code enumeration exactly http response code etc generic nonhttp response error code unique implementation resolution question explore implementation time enumeration http explored development brought forth via info adr handle securing message bus communication service covered universally upcoming adr future consideration desired query command could return information make either rest message request presumably query response would rest message query request information returned allows party application choose whether rest message bus make command request mentioned adr ekuiper allowed access directly internal edgex message bus special circumstance party external system communication future release edgex even ekuiper may routed external internal message bus bridge better decoupling security noted validation communication rest message bus done future future might want think creating additional apis addingupdatingdeletingquery external subscription store redisdb part consideration noted one could also consul change configuration would require configuration question added writable section consequence reference core command api']"
11,118, Search Indexing Architecture Decision,['Search Indexing Architecture Decision'],"['elasticsearch', 'indexing', 'search index', 'search engine', 'lucene', 'solr', 'opensearch', 'new index', 'index', 'documentdb']","['record upgrade elasticsearch highlevel migration plan new cluster architecture elasticsearch compatibility connecting multiple elasticsearch cluster synchronising state data sync production staging integration testing elasticsearch decomissioning elasticsearch work done change made repository also significant change govukaws govukawsdata govukpuppet highlevel migration plan based experience migrating elasticsearch decided time wanted isolate complexity two elasticsearch cluster searchapi make searchapi work elasticsearch elasticsearch give searchapi support multiple elasticsearch cluster index change cluster add url parameter select cluster query perform onetime import data elasticsearch elasticsearch synchronise update missed import running confirm elasticsearch elasticsearch cluster consistent set test finderfrontend etc select elasticsearch cluster gradually phase elasticsearch elasticsearch retire elasticsearch approach took last time rummager searchapi running parallel necessitated one carrenza one aws led lot change govukpuppet didnt really give anything reusable adding multicluster support searchapi directly get something reusable new cluster architecture based experience running elasticsearch quarter first opted three dedicated master node type clargeelasticsearch six data node type rlargeelasticsearch found data node powerful enough switched rxlargeelasticsearch talking aws solution architect changed rxlargeelasticsearch due wider govuk scaling concern caused political activity increased data node rxlargeelasticsearch also shrunk cluster staging production size cope indexing load final cluster size environment master data production clargeelasticsearch rxlargeelasticsearch staging clargeelasticsearch rxlargeelasticsearch integration tmediumelasticsearch rlargeelasticsearch cluster configured appelasticsearch terraform project elasticsearch compatibility deprecation message elasticsearch error message elasticsearch figure needed change change needed index query index change switching string field text keyword field removing field includeinall query change like instead doc morelikethis query replacing match type phrase matchphrase query replacing index query query indicesshould change introduced inefficiency query generation ask elasticsearch real name alias added new buildquery metric keep track elasticsearch issue opened february solution problem also changed text similarity metric classic similarity new similarity connecting multiple elasticsearch cluster elasticsearchyml file contains list cluster cluster specify schema configuration file mean try different indexlevel setting new cluster example changing text similarity metric elasticsearch code multicluster support implemented searchconfig indexclient class cluster selection implemented searchparameterparser class involved significant refactoring main architectural made index writes cluster ensuring cluster consistency perform query default cluster unless url parameter given test cluster one searchconfig singleton per cluster change result detail cluster leaking throughout searchapi unfortunate addressed future refactoring work example passing around searchconfig instance rather asking cluster singleton synchronising state planned either take snapshot elasticsearch restore elasticsearch approach last upgrade script copy data across turned unnecessary searchapi running cluster couple week started think synchronising data time everything republished either directly result dependency resolution govuk government detailed index consistent elasticsearch pagetraffic index also handled multicluster work traffic data saved cluster index needed manual work metasearch hold best bet attempting republish searchadmin kept failing due transient network issue getting best bet seemed impossible metasearch index script python elasticsearch import elasticsearch elasticsearch transporterror transporterror elasticsearch import elasticsearch elasticsearch transporterror transporterror elasticsearchhelpers import bulk datetime import datetime import index metasearch genericdoctype genericdocument eshostport osgetenvesoriginhost httpelasticsearch estargetport osgetenvestargethost httpelasticsearch esclient elasticsearcheshostport esclient elasticsearchestargetport def preparedocsforbulkinsertdocs doc doc yield docid source docsource def bulkindexdocumentstoesdocuments try bulk esclient preparedocsforbulkinsertdocuments indexindex doctypegenericdoctype chunksize except transporterror printfailed index document stre def fetchdocumentsfrom pagesize scrollidnone try scrollid none result esclientsearchindex genericdoctype fromfrom sizepagesize scrollm scrollid resultsscrollid else result esclientscrollscrollidscrollid scrollm doc resultshitshits return scrollid doc except transporterror printfailed fetch document stre return stre estatuscode name main start datetimenow dcount esclientcountindexindex doctypegenericdoctypecount printpreparing index document esformatdcount offset pagesize scrollid none offset dcount scrollid doc fetchdocumentsfromoffset pagesizepagesize scrollidscrollid printindexing document esformatoffset offsetpagesize bulkindexdocumentstoesdocs offset pagesize printfinished secondsformatdatetimenow start data sync production staging integration done way elasticsearch data sync script needed modified allow different host httpelasticsearch httpelasticsearch otherwise matter writing configuration testing elasticsearch test set like configuration govukcdnconfig logic finderfrontend pas one two url parameter searchapi based cdnlevel test logic searchapi choose cluster based parameter finderfrontend general test process covered dev doc test monitored clickthrough rate proportion search refinement proportion search exit test revealed significant degradation metric compared elasticsearch unable proceed switch without work improve search result two main change elasticsearch impacted search result quality switching classic similarity similarity issue arose elasticsearch decided ahead new similarity moving elasticsearch removal query coordination factor affecting scoring multiclause must query meant even switched back classic similarity wouldnt get result elasticsearch query coordination factor multiclause must query scored sumclause score nummatching clause numclauses query clause match overall score multipled effect make document match multiple clause tend rank higher document match fewer clause even fewer clause matched really well assumption number matching clause important predictor relevance without query coordination factor query scored sumclause score figuring improve search query straightforward particularly systematic process elasticsearch query ruby bool matchphrasetitle query matchphraseacronym query matchphrasedescription query matchphraseindexablecontent query matchalltermswtitle acronym description indexablecontent query matchanytermswtitle acronym description indexablecontent query minimumshouldmatchallsearchabletext query elasticsearch query settled ruby shouldcoordquery matchalltermswtitle query matchalltitleboost matchalltermswacronym query matchallacronymboost matchalltermswdescription query matchalldescriptionboost matchalltermswindexablecontent query matchallindexablecontentboost matchalltermswtitle acronym description indexablecontent query matchallmultiboost matchanytermswtitle acronym description indexablecontent query matchanymultiboost minimumshouldmatchallsearchabletext query matchminimumboost shouldcoordquery reimplementation query coordination factorbased scoring functionscore query also changed matchphrase clause elasticsearch query matchallterms clause adjusted field boosting factor ran test found elasticsearch new query clickthrough rate within percentage point elasticsearch old query decided good enough ahead switch decomissioning elasticsearch step followed switch elasticsearch permanently decomission elasticsearch switch searchapi variant test disable cluster indexing done disable test govukcdnconfig finderfrontend remove elasticsearch configuration govukpuppet remove elasticsearch govukaws didnt want permanently configuration required care change set elasticsearchuri environment variable value elasticsearchburi environment variable govukpuppet swap cluster configuration cluster configuration unset elasticsearchburi environment variable govukpuppet approach avoided coordinate simultaneous deploys searchapi govukpuppet', 'record upgrade elasticsearch highlevel migration plan running searchapi rummager parallel elasticsearch compatibility looking future elasticsearch compatibility new cluster architecture synchronising state switching application staging teething problem production teething problem data sync production staging integration dockerising development work done change rummager searchapi contained also significant change govukaws govukawsdata govukpuppet various repository small change well terminology rummager elasticsearch search service deployed carrenza staging production searchapi elasticsearch search service deployed aws highlevel migration plan decided two highlevel objective planning closetozerodowntime switch dont want set new infrastructure carrenza consequence needed least temporarily two elasticsearch cluster elasticsearch carrenza elasticsearch aws rest plan came create fork rummager called searchapi run different port deployed aws make searchapi work elasticsearch arrange update sent rummager also sent searchapi set new elasticsearch cluster perform onetime import data elasticsearch elasticsearch synchronise update missed import running confirm elasticsearch elasticsearch cluster consistent plek make application read search searchapi plek make application write search searchapi retire rummager elasticsearch anything related licencefinder also elasticsearch store small amount data reindex minute decided complication running second licencefinder soon new elasticsearch cluster existed production switched licencefinder running searchapi rummager parallel largely matter following set new rail application documentation main difference basing configuration searchapi existing puppet etc code rummager change needed rummager configuration changing port first available free port changing redis namespace rummager searchapi changing rabbitmq namespace rummager searchapi additionally searchapi hostname needed adding terraform configuration elasticsearch compatibility elasticsearch release note includes large section breaking change neither developer team knew elasticsearch start quarter section served guide necessary documentation main change address structural change query language generally straightforward resolve two complex issue revisited moving elasticsearch default text similarity metric changed classic algorithm encountered problem result ordering possibly due switched back classic algorithm elasticsearch string type split two new type keywords text different behaviour elasticsearch internally transforms single string field two field new type transparently handling indexing querying update schema textkeywords looking future elasticsearch compatibility elasticsearch allow multiple document type index elasticsearch allow two way solve problem combine type one large type add type field application add separate index type decided combine type index one big type added documenttype field distinguish elasticsearch anyway document lucene index must type think imposes additional overhead searchapi still validates document schema different field hold type external api changed new cluster architecture decided aws managed elasticsearch cluster selfmanaging elasticsearch undesirable nobody know manage elasticsearch version puppet old cant version puppetelasticsearch support elasticsearch much harder scale designed new cluster architecture based resource allocated current elasticsearch cluster proved sufficient time writing cluster three dedicated master node type clargeelasticsearch six data node type rlargeelasticsearch cluster configured appelasticsearch terraform project synchronising state two step ensuring search update sent rummager also sent searchapi import current elasticsearch data first ensured update sent searchapi wouldnt point importing data would immediately become three way data get rummager publishingapi via rabbitmq searchadmin http whitehall http publishingapi update already handled setting searchapi similar way rummager handled http update traffic replay traffic sent rummager also sent searchapi reliable occasionally dropped document case resending document whitehall rummager made also appear searchapi data import script fetch document elasticsearch transform new singletype format insert elasticsearch elasticsearchmigrationhelpers created fresh search index ran import script replayed update missed due running switching application handled application switchover plek gave searchusing application new parameter puppet specify url set plekservicerummageruri plekservicesearchuri environment variable could configure hieradata allowing application switched independently application configured environment switched environment independently constraint order change application update search done application query search avoid inconsistent state thing querying rummager thing updating searchapi production switchover divided application riskiness collection licencefinder finderfrontend contenttagger hmrcmanualsapi searchadmin whitehall frontend backend set rummager xray recording verify monitoring still talking staging teething problem discovered significant problem manual testing staging delayed production rollout week strange result ordering resolved switching back classic text similarity back string field mentioned detail every query returned every best bet due mistake made combining type index would automatically unlock due aws process run every four minute locking index cluster unhealthy unlocking healthy changed readonlyallowdelete introduced benign potentially confusing race condition importing page traffic data reindexing new schema issue commented code switch alias without lock since readonlyallowdelete prevents alias changed page traffic loader daily process wont race condition switch alias without lock since readonlyallowdelete prevents alias changed running schema migration traffic must represented anyway race condition irrelevant equivalent synonym expanded didnt figure happening default configuration expand equivalent synonym configuration overrided problem fixed turning equivalent synonym explicit mapping query would throw bad request error best bet analysis also didnt figure happening possibly problem elasticsearch ruby library unsuccessful replicating problem curling cluster directly problem solved handling exception production teething problem encountered issue switching production suspect caught staging staging doesnt publishing activity also possibility traffic replay staging set elasticsearch performance issue query queueing elasticsearch didnt realise problem first due unfamiliarity new metric ended doubling size cluster also switching faster data node long slow query slow query log revealed sometimes getting character query would take second execute changed searchapi reject query character length finderfrontend truncate query probably also problem elasticsearch elasticsearch error error due unmapped type sorting query multiple index field exist index saw many error trying sort field present govuk index fine result govuk index error frequent obscured actual problem changed query searchapi generates include default sort behaviour missing field sorting end probably also problem elasticsearch searchapi performance issue two search machine rather three provisioned terraform configuration aws search machine created two machine whreas three carrenza searchapi could handle two third traffic rummager initially bumped unicorn worker aws search machine half added missing third machine realised problem failing generate sitemap file aws search machine running memory trying generate sitemap file working one point stopped changed sitemap file link per file rather revealed bug would eventually hit anyway sitemap generation failed sitemap file bug fixed data sync production staging integration elasticsearch snapshot data sync three snapshot repository backed bucket govukproduction backed govukproductionelasticsearchmanualsnapshots govukstaging backed govukstagingelasticsearchmanualsnapshots govukintegration backed govukintegrationelasticsearchmanualsnapshots elasticsearches write read repository production writes govukproduction staging read govukproduction writes govukstaging integration read govukstaging writes govukintegration sometimes staging writes metadata file production bucket integration writes metadata file staging bucket cant avoided elasticsearch requires write access snapshot repository even ever restores snapshot lambda fix object permission happens taking restoring snapshot triggered govukenvsync script terraform provision bucket cant tell elasticsearch python script python import import sys import boto import request requestsawsauth import awsauth host httpelasticsearch region euwest service credential botosessiongetcredentials awsauth awsauthcredentialsaccesskey credentialssecretkey region service sessiontokencredentialstoken def registerrepositoryname rolearn deletefirstfalse printname url host snapshot name deletefirst requestsdeleteurl rraiseforstatus printrtext payload type setting bucket name elasticsearchmanualsnapshots region region rolearn rolearn header contenttype applicationjson requestsputurl authawsauth jsonpayload headersheaders rraiseforstatus printrtext deletefirst deletefirst osenviron sysargv integration rolearn arnawsiamroleblueelasticsearchmanualsnapshotrole registerrepositorygovukintegration rolearn deletefirstdeletefirst write registerrepositorygovukstaging rolearn deletefirstdeletefirst read elif sysargv staging rolearn arnawsiamroleblueelasticsearchmanualsnapshotrole registerrepositorygovukstaging rolearn deletefirstdeletefirst write registerrepositorygovukproduction rolearn deletefirstdeletefirst read elif sysargv production rolearn arnawsiamroleblueelasticsearchmanualsnapshotrole registerrepositorygovukproduction rolearn deletefirstdeletefirst write else printexpected one integrationstagingproduction dockerising development cant mange elasticsearch puppet due version constraint upgrading puppet upgrade eventually elasticsearch far much work dockerised elasticsearch development data replication process changed download govukintegrationelasticsearchmanualsnapshots bucket copy docker container restore latest snapshot', 'record transition mainstream format publishing api derived search index definition throughout document format refers rummager field format document type provided publishing api contentstoredocumenttype every contentstoredocumenttype mapped single format mainstream example search index want retire document also apply government detailed index building new search index publishing api data currently replacing existing search index mainstream government detailed single index govuk derived publishing api existing index populated least different application change improve quality data search system relies format handled consistent manner update document publishing event publishing withdrawing unpublishing tagging easier rebuild revert old backup lifecycle search index rumamgers search index long lived content regularly updated derive popularity field recent pageviews adr popularity updating without index lock update rather rebuilding search index every time separate task place reindex content currently search index zero downtime something adding new field otherwise changing elasticsearch mapping field work properly problem addressing neither mechanism add remove document search index mean edition ever published search index doesnt get updated stay next time document updated edition unpublished without rummager notified stay search index forever long term wed like able easily rebuild whole govuk index scratch avoid kind problem arent aiming right immediate able populate govuk index document already exist start index search api also want able reindex everything published within short period time day easily recover index backup moving format one time intending switch new index formatbyformat querying old new index filter select index get format mean retire old search indexing code publishing apps search index longer needed without populate everything also revert back old index simple configuration change something wrong problem weve discovered approach relevancy document within index depends document index index partially populated tfidf statistic representative affect search result firstly well implement task bulkreindex chunk content publishing api rummager process content way regular publishing update reindexing format let initially populate new index reindexing range let bring govuk index restoring backup secondly change indexing process format new index format following phase phase untransitioned search time rummager read untransitioned format old index document belonging untransitioned format stored govuk index filtered index time rummager ignore publishing api message affecting untransitioned format nightly update job update popularity field untransitioned format mainstream index also copy untransitioned format document mainstream index govuk index doesnt matter order two thing happen net effect untransitioned format considered tfidf statistic transitioned format ready returned govuk index phase indexed search time behaviour untransitioned format index time rummager insert document govuk index nightly update update popularity field govuk index one task well delete existing data format govuk index reindex net effect data govuk index come publishing api data indexed format phase transitioned search time rummager read transitioned format govuk index document belonging transitioned format stored mainstream index filtered index time rummager insert document govuk index nightly update update popularity field govuk index net effect search api publishing apiderived data transitioned format consequence copying mainstream data govuk add layer complexity nightly popularity updater wont able get rid weve retired index since weve decided implement ability generate index scratch content removed search unpublished stay way forever continue address manually removing content search admin']"
12,74, Software Development Architectural Decisions,['Software Development Architectural Decisions'],"['python version', 'support python', 'python package', 'python path', 'pythonpaths', 'python python', 'pythontuf', 'konduit pythonpaths', 'python', 'home assistant']","['minimum supported python version updated python version clarification architecture issue superseded adr home assistant currently set minimum python requirement penultimate minor python version released upstream home assistant try support latest two released minor upstream python version example current latest version support python would like support python however python longer supported point new minor python version released dropped minor version deprecated period month removed deprecated home assistant print warning message inform user supported micro version decided minor version basis codified version check code home assistant proper consequence june deprecate python august adopt home assistant support latest minor release python mean drop support python three installation method based docker container home assistant container supervised maintained kept home assistant project', 'python path resolution proposed proposed sham azeem discussed adam gibson paul dub konduit serving ability run python script pythonstep bunch setup pythonstep includes name data type input output script location python library path remain static system system apart python library path variable since every system different set python installation either conda virtual environment regular python installs becomes hassle manage pythonstep configuration thats widely usable across different system point creating adr specify simple enough workflow take care python library path resolution konduitserving cli profile simple configs basically try find python conda installation executables found inside path environment variable proposal base proposal able assign identification token different python install location instead knowing specifying absolute python path location user specify identification token system found python installs still going available specifying custom python install present system environment path variable pythonpaths command example workflow follows shell script konduit pythonpaths output python install location follows text python installs path cprogram file xmicrosoft visual studiosharedpythonpythonexe version path cusersshamsminicondapythonexe version path cusersshamsappdatalocalmicrosoftwindowsappspythonexe version conda installs path cusersshamsminicondascriptscondaexe version environment name base path cusersshamsminiconda version name path cusersshamsminicondaenvspy version name path cusersshamsminicondaenvspy version path cusersshamsminicondascriptscondaexe version environment name base path cusersshamsminiconda version name path cusersshamsminicondaenvspy version name path cusersshamsminicondaenvspy version virtualenv installs path cvirtualenvspython version path cvirtualenvspython version path cvirtualenvspython version based info gained command konduit pythonpaths much easier specify python path configuration file type pythonpath environment property json type python pythonconfig type conda pythonpath environment possible value type property take one following value python conda virtualenv custom pythonpath python install typepython conda install typeconda otherwise contain absolute path python executable file typecustom absolute root directory path typevirtualenv environment looked typeconda locate python installation particular conda environment default priority default python path expected following way usage profile profile extended incorporate python path setting running serve command item name pythonstep config going example shell script konduit profile create pythonenv pythontypeconda pythonpath condaenvpy otheroptions konduit profile create pythonenv ptconda cepy otheroptions profile serve command usage look like following shell script konduit serve configjson pythonserver pythonenv command run python path automatically updated configuration value specified profile setting registering python path needed python conda installation isnt available inside path environment variable register pythonpaths add subcommand example shell script konduit pythonpaths add typepython pathepythonpythonexe konduit pythonpaths add tpython pepythonpythonexe type either python virtualenv conda running command see following output text python path location epythonpythonexe registered specifying different python path different pythonsteps want specify different python path different pythonsteps dont want profile update configuration dynamically specify pythonpathresolution static particular step configuration wont affected profile setting installed package detail also able see installed package python installation withinstalledpackages wip flag example shell script konduit pythonpaths withinstalledpackages konduit pythonpaths wip output look something like text python installs path cprogram file xmicrosoft visual studiosharedpythonpythonexe version package abslpy aenum apipkg appdirs argcomplete astor astroid attrs autopep black bleach cachetools certifi chardet click colorama coverage cycler cython dbf dbfpy decorator docutils execnet gast googleauth googleauthoauthlib googlepasta grpcio hpy hurryfilesize idna imageioffmpeg importlibmetadata isort jinja joblib jsonschemapopo githttpsgithubcomeclipsedeeplearningjgitbeaeacefaeebbbbeggjumpysubdirectoryjumpy kera kerasapplications keraspreprocessing keyring kiwisolver konduit lazyobjectproxy markdown markupsafe matplotlib mccabe mock networkx numpy filecusersshamsdownloadsnumpybmklcpcpmwinamdwhl oauthlib opencvpython opteinsum packaging panda pathspec pep pillow pkginfo protobuf pyj pyarrow pyasn pyasnmodules pycodestyle pydatavec pydlj pygments pyjnius pylint pymongo pyparsing pyspark pytestcache pytestcov pytestpep pythondateutil pytz pywinctypes pyyaml readmerenderer regex request requestsoauthlib requeststoolbelt rsa scikitlearn post scipy setupextjanitor six tensorboard tensorflow tensorflowestimator termcolor toml torch cpu torchvision cpu tqdm twine typedast urllib webencodings werkzeug wrapt zipp path cusersshamsappdatalocalmicrosoftwindowsappspythonexe version example workflow example workflow involves actual api code snippet regular python install assuming konduit pythonpaths give following information particular system regular python installs text konduitpc konduit pythonpaths python installs path cprogram file xmicrosoft visual studiosharedpythonpythonexe version package want specify python install pythonstep configuration java code would look similar java pythonconfig pythonconfig pythonconfigbuilder pythontypepythonconfigpythontypepython pythonpath build conda install assuming konduit pythonpaths give following information particular system conda installs text konduitpc konduit pythonpaths conda installs path cusersshamsminicondascriptscondaexe version environment name base path cusersshamsminiconda version name path cusersshamsminicondaenvspy version name path cusersshamsminicondaenvspy version want specify conda install conda environment pythonstep configuration java code would look similar java pythonconfig pythonconfig pythonconfigbuilder pythontypepythonconfigpythontypeconda pythonpath environmentpy build virtualenv install assuming konduit pythonpaths give following information particular system text konduitpc konduit pythonpaths virtualenv installs path cvirtualenvspython version path cvirtualenvspython version path cvirtualenvspython version want specify virtualenv install pythonstep configuration java code would look similar java pythonconfig pythonconfig pythonconfigbuilder pythontypepythonconfigpythontypevirtualenv pythonpath build custom python install registered pythonpaths python install assuming installed location dpythonpythonexe thats registered pythonpaths command java api code would look similar java pythonconfig pythonconfig pythonconfigbuilder pythontypepythonconfigpythontypecustom pythonpathdpythonpythonexe build validation error path found kind error would occur custom python install configuration since python path might misspelled error message similar following would work shown text unable find python path installed location dpythonpythonexe directory path doesnt exist invalid specific python type would happen doesnt exist specified configuration would show text invalid specified conda install type available conda install path cusersshamsminicondascriptscondaexe version path cusersshamsminicondascriptscondaexe version similar message shown case type python virtualenv conda environment found occur conda install type nonexistent conda environment specified would show text invalid conda environment specified conda installed location cusersshamsminicondascriptscondaexe available environment specified conda install name base path cusersshamsminiconda version name path cusersshamsminicondaenvspy version name path cusersshamsminicondaenvspy version consequence advantage run python import sys printospathsepjoinpath path syspath path find python library location manually profile make sure intended python path system transition disadvantage entirely straightforward beginning documentation discussion adam adam maybe add bit auto discovery also add registration command like konduit register pathtopythonbinary kind like pycharm sham yes ill add detail auto discovery basically going find python conda executables path environment variable assign respective also registration command good idea register keep python path even inside path env variable adam specify bit work also pointing conda environment one thing might want specify conda environment sham yup shell command take care youll specify conda environment name well put type conda config command httpsgithubcomkonduitaikonduitservingpullfilesdiffaedbbdcaecber condaenv flag adam another note might virtual envs well sure hard specifying whether create separate virtual env could also good running conda wonder could bootstrap conda environment konduit well long access python binary good sham hmm virtualenv would workflow shouldnt hard add basically lot running condapythonvirtualenv command there problem basic command listing package version dependency resolution python would take bit effort fine case clash deps resolving clash gonna mess try directly konduit ill keep bootstrapping new python environment whether virtualenv conda separate adr ill work specifying preinstalledcreated python installsenvironments sham ill keep bootstrapping new python environment whether virtualenv conda separate adr ill work specifying preinstalledcreated python installsenvironments adam could show code example well itd good see something like pythonpathprofileid well specify basic validation well like couldnt find path etc sham sure calling location custom python type locationepythonpythonexe could make sense ill add example endtoend workflow ive changed location pythonpath example theyre exactly endtoend give whole workflow idea pretty well paul paul ive got question came reading adr system python ship python way supposed work bind pretty hard defining system different kind python different system bit magic default magic explicitly logging user actually know happening something doesnt work anymore theyve moved jar file different system work deployment type jar static setting different python path different step work create one machine serve different one case overall sure direction pythonstep would expect given cpython bytedeco package provide known python executable known place ship pip venv ground expect python step able aware requirementstxt file also able set necessary library environment python step sham thanks paul appreciate review like suggestion venv cpython bootstrapping new environment requirementstxt file ill answer query one one system python ship python dont necessarily system available python installation work without add anything extra top konduit installation working configuration separate python installation script zip definitely extract particular location specify path typecustom pathinstalllocation able work way supposed work bind pretty hard defining system different kind python different system like mentioned previous answer doesnt system python separate folder python binary present system would consider utility manage python installation various location different tool way going work different system profile setup separately system thing change serve command right profile example konduit serve configyaml pythonserver profile pythonprofile way even config transferred another system profile make sure right python even configuration different setting profile setting would override whats present python step config bit magic default magic explicitly logging user actually know happening something doesnt work anymore theyve moved jar file different system yup every activity going logged python step initialised prominent right behaviour followed demonstrate workflow adr well work deployment type jar sure understand question correctly mean would work konduitserving uberjar uberjar would cli feature command related python path pythonpaths pythonpaths add would available well simply something like java jar konduitservingjar pythonpaths see available also profile would available well static setting different python path different step work create one machine serve different one case thought case user might want run python step cpu gpu example python step want tensorflow another might wanna tensorflowgpu would largely machine specific profile override config setting python path keeping pythonpathresolution static would stop default value setting would pythonpathresolution dynamic profile would affect step configuration cpython venv idea good incorporate current workflow adding type like type bootstrap requirement pathofrequirementstxt path pathtonewvenv however thinking keeping installation validation detail type python specification separate adr since would lot edge case installation fails deal paul like suggestion venv cpython bootstrapping new environment requirementstxt file separate python installation script zip definitely extract particular location specify path typecustom path able work quite sure point cpython came across correctly specifically mean following exists xml dependency groupidorgbytedecogroupid artifactidcpythonartifactid versionversion dependency platform specific jar file contain full python installation along standard library pythonstep already depending orgbytedeconumpyplatform turn depend cpython package talking ive taken look code see pythonsteprunner already pythonexecutioner datavec turn actually package java interface turn mean already internal interpreter think proposal somewhat misguided given knowledge fully expect thing crash hard debug way bundled python executable different set library best expect slightly different standard lib worst compiled different python version sham based call javacpp cpython distro startup package venv pip bootstrap compatible environment case system installed package might lead case compiled package different python version compatible cpython get shipped javacpp validation problem isnt fully solved since interpreter going case come javacpp cpython included venv pip safest way nobad internet case simple python path executable take python executable location run python step library included package package clash appending excluding library path javacpp package ill create another adr mention point decide direction', 'installation method home assistant core define supported installation method per adr running home assistant core application directly python provide full supervisor experience thus provide supervisor panel addons supported operating system version major linux distribution latest stable major version window wsl macos python via homebrew supported python version running home assistant core supported running application official python virtual environment running home assistant core without virtual environment systemglobally installed python package supported detail supported python version defined adr documentation operating system require extra library package installed prior installing python requirement case documentation shall link installation instruction python requirement requires case available possible name library package installed aim include installation instruction every required expertise installation requires installing python venv support default except debian based system create virtual environment install home assistant core via pip package require compilation user install compiler development package development package provided operating system break system start system started responsibility user based operating system run full network access work access usb device work box maintaining home assistant installation maintenance requires time effort skill experience method python upgrade home assistant upgrade python every year happen current operating system doesnt support new minimum required version box case find unofficial python package system compile python source installing python dependency python package compilation user responsible right compiler development package installed updating home assistant updating happens via pip commandline tool maintaining operating system home assistant core run python virtual environment anything outside responsibility user security update responsibility user maintaining component required supervisor supervisor conclusion expert installation method based integration youre running lot extra package installed consequence update documentation install method required experience expected maintenance move existing documentation match supported installation method community guide wiki notify user onboarding expected maintenance installation method']"
13,107, Infrastructure Management Strategy,['Infrastructure Management Strategy'],"['terraform', 'terraform module', 'aws resource', 'cloud platform', 'kubernetes', 'cloudformation', 'aws lambda', 'aws', 'infrastructure code', 'provisioning']","['support terraform terraform recently released version upgrading terraform significantly effort previous terraform upgrade due number incompatibility hypr client module time whilst client continue module new module created also available client terraform provides significant number improvement enable module verbose workarounds achieve result written module support terraform module user master branch module contain version module whilst branch hold version common pattern prevalent terraform module community time user still encouraged upgrade terraform providing version module miss functionality provided module time consequence branch terraform version supported provides user module relevant autonomy user yet moved terraform branch time switch terraform version module user incorporate branch source definition documentation provided usage section due limitation optional argument number optional argument terraform aws mks cluster resource version module may become far complex version module', 'terraform directory structure amended terraform data structure consistent directory structure terraform beginning initial requirement want separate code data future open source code without disclosing implementation detail maybe keeping data private github repository want able encrypt sensitive data repository want support sensitive data encryption part process without manage secret different repository different script etc want create terraform module reuse code want separate terraform code different project one managing infrastructure tier application stack especially important separate resource govuk application initial solution present three directory data module project data directory contains subdirectory per terraform project store variable value customised per environment data directory also contains secret file sensitive data encrypted sop module directory contains subdirectory per terraform provider project directory contains terraform stackstiers project named deploy common infrastructure govuk prefix networking dns zone deploy application group application app prefix frontend puppetmaster data base commontfvars integrationtfvars myapplication commontfvars integrationtfvars integrationsecretsjson module aws routezone mysqldatabaseinstance network project base integrationbackend maintf variablestf myapplication integrationbackend maintf variablestf consequence manage secret explained going decrypt secret file run time make sure clean unencrypted file running terraform command set requires relative path import module terraform stack depend directory initialising project feel comfortable module well consider put github repository source terraform stack via url', 'terraform across multiple aws account actual made earlier december january created multiple aws account host different tdr environment happy made alpha terraform configure infrastructure see terraformmd terraformvscloudformationmd alpha ran one aws account terraform workspace could create multiple environment wanted end one environment prototyping user testing alpha ran terraform script dev machine beta wed prefer run terraform controlled environment like jenkins advantage make easier control access production infrastructure developer permission run jenkins job therefore make versioncontrolled peerreviewed change production without credential giving full access production account make easier deploy change environment turn provides record recent terraform run started there chickenandegg problem running terraform hosted system like jenkins would helpful could terraform create jenkins infrastructure create multiple terraform project run different stage backend project run management account bootstrap rest infrastructure run development machine creates iam role terraform state storage dynamodb environmentspecific terraform script project terraform state stored manuallycreated bucket dynamodb table project run management account backend bootstrap script run also run development machine terraform state saved storage set bootstrap script environment project run tdr environment account integration staging production script run jenkins since environmentspecific terraform run jenkins aws iam role give jenkins permission run terraform mean dont generate aws secret key could outside environment stolen jenkins run terraform']"
14,290, Cloud Infrastructure Management,['Cloud Infrastructure Management'],"['kubernetes', 'cloud platform', 'cluster', 'elasticsearch', 'docker', 'pod', 'aws', 'eks', 'apigroups', 'cloud']","['kubernetes cluster per environment scope web sre team multiple environment sometimes called tier web sre service environment generally serve different purpose audience different uptime requirement environment optionally serve required point path towards production environment mozilla service environment generally include development dev staging stage production prod sometimes also testing environment usually ephemeral generally environment infrastructure provider account differ infrastructure application platform level kubernetes level exception testing tends ephemeral within particular dockercompose spin kind cluster pipeline multiple way differentiate persistent dev stage prod environment infrastructure application platform within kubernetes common pattern separating persistent environment distinct kubernetes cluster per environment one namespace per environment shared kubernetes cluster distinct node pool per environment within single cluster along selector taint toleration web sre service environment dev stage prod kubernetesbased distinct kubernetes cluster per environment dev stage prod within infrastructure account caveat environment required see distinct kubernetes namespace within cluster per application deploy application distinct cluster relevant expected application environment stage application deploy stage cluster prod application deploy prod cluster etc deploy operationals infrastructure change upgrade distinct cluster specifically moving change dev present stage prod reverting stage match prod within appropriate timeframe end goal keeping stage prod cluster close alignment development cluster availability uptime expectation application infrastructure include support outside business hour stage prod cluster availability uptime expectation application infrastructure require support outside business hour however specific project negotiated sres project stakeholder development cluster optionally include automatic infrastructure platform update first place review change effect goal promoting update stage prod feasible reviewed approved stable infrastructure release stage cluster deploy infrastructure change able relatively quickly move change stage prod avoid divergent result staging production application reviewed approved productionready stable application infrastructure release production cluster understand purpose stage cluster validate infrastructure application code headed production expect deployment stage cluster step release deployment pipeline application infrastructure moving production adr include following application within cluster name namespaces applicationspecific kubernetes resource keeping discourseprod namespace production cluster redundent also explicit left sres working service immediate push migrate existing service pattern cause lot churn instead going forward migration starting new service explicit declaration slos slas environment beyond development cluster roundtheclock support expectation multitenant cluster versus singletenant cluster pattern environment apply either case expectation development cluster exists left service sres determination approach nonperistent cluster local development pipeline environment implement cluster per environment left sres implementation consequence pro pattern support meaningful separate staging environment especially infrastructure change testing impact application trickier case dev staging prod application live production cluster clearer boundary environment within infrastructure con work bring current service alignment including fair amount migration infrastructure update hence adr considered pattern involve creating new service performing migration admin developer user kubernetes cluster hop cluster compare environment state log etc mitigated somewhat kubernetes command line tooling though resource itseappsprod cluster implementation httpsgithubcommozillaititseappsprodinfra itseappsstage cluster implementation httpsgithubcommozillaititseappsstageinfra good short synapsis kubernetes tier separation approach found production kubernetes rosso lander brand harris httpswwworeillycomlibraryviewproductionkubernetes', 'eks whats proposed amazon eks running main cluster host moj service team application replaces usage kops reasoning benefit eks managed control plane master node reducing operational overhead compared kops scaling control plane node reduces risk api availability sudden increase api traffic managed node reducing operational overhead kubernetes upgrade smoother kops rolling upgrade problematic upgrade kops caused work around networking issue team see kops upgrade particularly stressful risk register open door elb ingres managed seen preferable selfmanaged nginx requires upgrade scaling etc avoid security challenge managing token exported kops export kubeconfig already run manager cluster eks gained lot insight experience eks configuration auth chosen oidc auth broker github identity provider developer service team auth github continues common sso amongst good tiein jml process see adr github identity provider auth useful broker couple important rule run login time ensures user ministryofjustice github organization staff get kubeconfig login website like grafana insert user github team oidc token claim rbac authorize user correct namespaces future azure sso growing moj there case switching adopted amongst user iam auth benefit immediately revoking access maybe could federated login github would give temporary kubecfg sync github team info iam completed auth credential issuer chosen cloud platform kuberos weve long kuberos issuing kubecfg credential user original version kuberos unmaintained pretty simple updated dependency httpsgithubcomministryofjusticecloudplatformkuberos maintain fork considered gangway similar kuberos release year kubelogin team would distribute client secret user seems odd trouble securely sharing secret overcome perceived difficulty issuing kubecfg credential requires user install software rather serverside centrally kubehook compatible eks doesnt support web hook authn dex doesnt web frontend issuing cred oidc broker completed authorization chosen existing rbac configuration well continue existing rbac configuration previous cluster completed node management chosen managed node group future experimenting fargate selfmanaged node managed node group automates various aspect node lifecycle including creating auto scaling group registration node kubernetes recycling node fargate node fully automated node least manage benefit isolation pod automatic scaling doesnt support daemonsets aim take advantage much automation possible minimize team operational overhead risk initially well managed node group looking fargate workload completed future fargate consideration pod limit quota limit fargate pod per region per aws account could issue considering currently run pod request aws raise limit currently sure scope multicluster stage separation load different aws account settle issue daemonset functionality replacement fluentbit currently log shipping elasticsearch aws provides managed version fluent bit fargate configured ship log elasticsearch prometheusnodeexporter currently export node metric prometheus fargate node managed aws therefore hidden however collect useful metric pod running fargate scraping cadvisor including cpu memory disk network support prometheus run still managed node group likely workload consider people check deployment investigated ingres cant nginx load balancer front investigated dont fargate take advantage spot instance reduced cost however fargate priority main driver engineer time cost node group chosen main node group majority workload prometheus node group well minimize number node group minimize operation overhead prometheus continue design previous cluster separate node group larger node dedicated prometheus pod prometheus resource hungry memory disk iop two large node single availability zone access volume store metric data node group may valueable spot fargate node explore future completed prometheus node group node instance type chosen rxlarge rxlarge raxlarge main node group chosen prefix configured image choosing instance type past experience memory tends limiting factor gone memory optimized range instance type latest cost efficient range without going arm processor see consideration however rxlarge current cluster well stick move raxlarge migration choice aws cni networking new cluster initially added constraint number pod per node due limitation number enis address however aws released prefix giving higher pod density per node enables avoid limit support added aws cni requires instance nitrobased range primary choice rxlarge vcpus memory fallback cloud provider run rxlarge vcpus memory raxlarge vcpus memory future might consider arm processor range wed consider added complexity crosscompiled container image chosen rxlarge rxlarge prometheus node group existing cluster rxlarge well continue add fallback rxlarge memory optimized range core rxlarge memory optimized range core rxlarge place main node group temporarily high number instance prefix backlog pod networking cni chosen aws vpc networking cni link httpsdocsawsamazoncomekslatestuserguidepodnetworkinghtml awss cni pod networking ipam cni routing pod given eni address underlying vpc rather overlay network pod traffic routed node native vpc advantage awss cni default eks native aws fully supported aws low management overhead offer good network performance concern awss cni would address every pod limit per node depending instance type number enis support calculation node instance type show change instance type cost cluster increase acceptable likely engineering cost maintaining supporting full calico networking custom node image considered calico networking advantage needing address per pod associated instance limit open source however wouldnt support cloud provider networking issue maintain customized image calico installed likely change eks time frequently cause breakage networking setup installation requires recycling node good fit declarative config completed node image chosen amazon eks optimized amis link httpsdocsawsamazoncomekslatestuserguideeksoptimizedamihtml eks amis default therefore lowest maintenance cant see reason needed change launch config would give flexibility completed cluster scaling chosen manual cluster scaling future consideration autoscaling initially continue manually scaling updown node manually kops cluster proven fine since workload change slowly alert setup warn capacity low prompt team scale cluster cluster autoscaling considered soon though embrace one cloud key advantage scale updown swiftly good efficiency example usage hosted site much lower night aim get workload scale automatically cluster follow suit addition tenant nonproduction environment run night consideration autoscaler maintain spare capacity workload scale dont wait node startup take minute may require tuning tenant encouraged autoscale pod effectively horizontal pod autoscaler capitalize cluster autoscaling scaling nonprod namespaces agreement service team manual scaling place autoscaler still desired network policy enforcement chosen calico network policy enforcement policy full blown calico networking link httpsdocsprojectcalicoorggettingstartedkubernetesmanagedpubliccloudeksinstallekswithamazonvpcnetworking implement kubernetes network policy api needed isolate tenant recommended eks team previous live cluster familiar cps namespaces default networkpolicy allows incoming request pod namespace ingresscontroller namespace tenant namespaces aws security group creating securitygrouppolicy resource namespace would attach security group allowing specify inbound outbound network traffic advantage similar networkpolicy security group specify aws resource like rds instance would network restriction addition auth restriction cps rds instance currently network accessible across vpc explore future defence depth completed aws calico network policy enforcement installed chosen calico typha install typha performance benefit cache request made calico felix api control plane logging chosen enable cluster logging link httpsdocsawsamazoncomekslatestuserguidecontrolplanelogshtml control plane log enabled log api authentication controller manager scheduler log cloudwatch maybe export elsewhere discussion covered adr logging completed node terminal access chosen aws system manager session manager bastion node link httpsdocsawsamazoncomprescriptiveguidancelatestpatternsinstallssmagentonamazoneksworkernodesbyusingkubernetesdaemonsethtml httpsgithubcomawscontainersroadmapissues aws system manager session manager benefit easy install daemonset auth via team member aws cred tied jml process access removed immediately leave team norm terminal command logged useful audit purpose eks best practice take advantage system manager feature future including diagnostic compliance monitoring note requires permission hostnetwork true privileged true may psp node failing boot join cluster properly live likely pod want characterize node node managed traditional method node access would ssh via bastion involves shared ssh key shared credential acceptable security practice completed implementation ticket httpsgithubcomministryofjusticecloudplatformissues runbook usage httpsrunbookscloudplatformservicejusticegovukeksnodeterminalaccesshtml podsecuritypolicies chosen default eks psp eksprivileged awsnode pod chosen apply cps existing privileged psp kubesystem namespace restricted psp rest eks come psp eksprivileged allows anything however security best practice limit ensure pod arent allowed run privileged root eks best practice say recommend scope binding privileged pod service account within particular namespace kubesystem limiting access namespace practice found awsnode pod needed already privileged restricted psps apply existing cluster completed psp deprecation psps deprecated april removed due april psps considered difficult way specify policy clear replace opa gatekeeper possibility look move psps whatever standard selected kep coming month outstanding backlog apps access aws role chosen iam role service account irsa instead kiam completed kubeiam usage migrated irsa kubeiam running new cluster chosen block access instance metadata node role completed access instance metadata blocked irsa benefit irsa kiam kubeiam kiamkubeiam require running managing daemonset container kiamkubeiam require powerful aws credential allow box assume role appropriate configuration kiamkubeiam aim provide container specific role however security concern approach kubeiam remember set defaultrole annotation set pod node boot may short window kiamkubeiam start protection instance metadata comparison irsa injects token pod avoiding concern kubeiamkiam attacker able get root node could access credential therefore aws role comparison irsa breach might bring access aws role associated service role blocking access instance metadata eks node various aws permission provided node role avoid pod permission block access implementation detail provided httpsawsgithubioawseksbestpracticessecuritydocsiamrestrictaccesstotheinstanceprofileassignedtotheworkernode achieved globalnetworkpolicy', 'multicluster time writing september cloud platform host user application single kubernetes cluster april decommissioned old kops cluster favour utilising eks see adr eks time decommissioning kops cluster cloud platform hosted namespaces new eksbased kubernetes cluster key reasoning behind utilising single kubernetes cluster service team described adr namely strong isolation already required apps different team via namespaces network policy difference isolating environment maintaining cluster environment cost effort risk cluster diverging might miss problem testing devstaging cluster arent prod however outside service team application cloud platform already multiple cluster management ephemeral test purpose information regarding multicluster challenge described adr cloud platform team eks matured usage eksbased cluster grown since april namespaces time writing continue grow future due believe good time start separating workload multicluster environment nuance cost eks widely understood worth noting current cluster time writing considered large cluster kubernetes stipulates consideration large cluster pod per node node total pod total container cloud platform near limit decided adopt multicluster approach start adopting multicluster splitting workload environment type production nonproduction treat nonproduction cluster second production cluster within team though may slight difference compute power nonproduction workload dont require intensive power production workload hour operation may turn nonproduction cluster hour weekend appetite user save cost allow user selfselect preferred environment initially though may change automate future consequence running multiple cluster derisk upgrading kubernetes version nonproduction workload separate production workload reduce blast radius creating hard separation cluster reduce cluster singlepoints failure ingres controller increase separation outside kubernetes workload improve resilience customise nonproduction cluster meet nonproduction production cluster meet production however come monetary team cost expect cost running cloud platform increase significantly insignificantly efficient resource usage node may fill increased complexity cluster management']"
15,231, Modular Architecture Decisions,['Modular Architecture Decisions'],"['implement', 'interface', 'implementation', 'module', 'framework', 'interactor', 'structure', 'dependency', 'constructor', 'architecture']","['drop legacy new code restriction originally prestashop made mainly static class dependency injection address problem decided non namespaced code would progressively refactored core namespace would contain code dependency injection furthermore core code wouldnt allowed depend directly non namespaced class could indirectly mean adapter class would act bridge new old code direct dependency core legacy rule led evergrowing collection adapter resulted greatly increased code complexity duplication case service legacy adapter core implementation subtle difference one furthermore constraint backward compatibility increase difficulty refactor code core surface public api larger following applies core prestashopbundle class referred core shortness new core class placed either core prestashopbundle namespace following rule established previously new class must added adapter namespace added legacy root namespace core class may depend instance legacy class provided following rule respected legacy class may either injected parameter constructed within caution must exerted legacy class produce side effect global state dont guarantee internal consistency case class accessed dedicated service enforce consistency core class must call static method class except factory method stateless tool method within service dedicated encapsulate static class core class may access data provided static class method static class relying dedicated service application service repository data provider core class must reimplement code found legacy class without deprecating original methodclass optionally making rely new implementation adapter namespace must phased eventually class adapter namespace must copied core namespace original adapter class must emptied made extend core class deprecated fully removed following major adapter service must deprecated copied core namespace well code must depend adapter class service consequence refactoring become easier complexity code duplication reduced hand harder refactor legacy class without introducing breaking change', 'building object main factory problem statement create instance class application pas dependency constructor ensure object shared others created every time needed driver want object creation typesafe able code intelligence feature ide strict type checking php static analysis capability phpstan want object creation support adhering solid principle single responsibility principle lead lot class depending hierarchy also instance creation solution separate concern instantiation class openclosed principle solved interface make necessary create different instance interface support switching instance testing dontrepeatyourself principle mean one place class instantiated edgeedge integration test environment resembles production environment closely possible still able switch service isolate side effect database content filesystem randomness etc want keep public api code method exposed web command line framework code small possible ideally api expose case considered adhoc instantiation new pimple dependency injection container come silex framework creating central factory outcome consequence chose central factory class called funfunfactory typesafety ide comfort autocompletion trumped concern boilerplate also saw binding specific library framework plus still pimple dic internally get shared object wrapped access instance pimple typesafe getters hindsight pimple holder shared object turned bad idea creation function build object far away code request easy way jump getter creation function realized switched pimple createsharedobject function refactor creation function naming convention give consumer hint object shared singleton newly created method starting get return shared object method new return fresh object every call extracted case bounded late stage project attempted split factory case contextfactory class attempt halfhearted difficult since underlying dependency connection entity manager make sense application bounded funfunfactory become long disorganized unfortunately pay attention boundary across bounded abstraction layer apply naming convention consistently funfunfactory contains setter make possible switch implementation testing also set environmentspecific service like logging see class wmdefundraisingfrontendfactoriesenvironmentsetup namespace architectural perspective bad since potentially allows controller switch service shouldnt allowed pro con adhoc instantiation new static construction method see matomo code base whenever class service lead severe problem dependency configurable making class harder test duplicate instantiation code base leading inconsistency shotgun surgery whenever constructor class change pimple dependency injection container pimple arrayaccess interface organize dependency create object inside anonymous function assigned array key code access array key pimple call creation function return created instance caching future access creation function get container parameter get dependency good class shared default good wellintegrated silex framework bad type checking type hint could partially overcome extensive property docblocks would far away creation function bad concept public private instance central factory good type safe good follow object tree construction jump definition movement editoride good define whats public whats private bad factory function almost every class bad enforce much structure allow pattern singleton wrapper link drawback choice improved upon adr', 'refactoring replacement legacy progress pun legacy code prestashop sadly lot recent code well rely heavily classsingleton legacy although convenient time many flaw completely mutable lot bug prestashop related value changed unexpected code way built well known mostly fragmented many different place current code lack validation exception feedback clear way knowing badly setup resulting hoursdays debugging understand root cause sometimes equivalent principle symfony forced rely legacy code even modern implementation legacycontext service merely accessor contextgetcontext static method principle contextualized data bad thing even required many case however build code independent prioritize stateless code rely parameter instead global state contextualized still real especially browser session purpose adr define new architecture intends replacing legacy modern code aim removingreducing current drawback ultimately new architecture completely replace old one least responsible building legacy one backward compatibility moder architecture split first new architecture split current contains everything split multiple sub way inject relevant part service sub split also mean built independently thus optimizing build process allowing process one would needed actually built identified main sub list may increase future shop language currency country employee api client new one building sub built upon three main component subcontext classservice gettersfunction allowing access data data must immutable ensure remains unchanged processrequest class basically immutable dto include method needed employeecontexthasauthorization subcontextbuilder provides getterssetters specify parameter required build subcontext class build method return subcontext instance builder also implement prestashopprestashopcorecontextlegacycontextbuilderinterface interface requires implementing buildlegacycontext initialize legacy backward compatibility course data build legacy must synced one modern service builder charge fetching data advance building operation however capable detecting required argument build like entity responsibility left listener symfony listener per sub responsible getting data required inject inside subcontextbuilder nothing responsible triggering actual build data initialized listener must kept minimum like locale code entity advanced fetching left builder listener also responsible defining default fallback needed multiple listener sub allow adapting initialization depending case environment one back office one oauth api one cli command one frontoffice one day based three element symfony favor subcontext service easily injectable place subcontextbuilder factory allows build service subcontext service must lazy service several reason improves performance even cost minimum wont built unless actually built late possible first time actually way leaf time process overridechange value set initial listener thus allowing flexibility instant building solve issue far service depending built request even data available resulting error code even run dto subcontext dto service immutable object field must readonly php readonly kept private field accessible via getter method required always accessible like shopcontext countrycontext mandatory always populated even requires fallback value direct dtos meaning extra layer represent subcontext data directly optional empty possible initialize sometime like employeecontext cannot created user logged core via oauth api case apiclientcontext equivalent extra dto data stored sub dto field optionalnullable subcontext service example country required example architecture divided three element country currencycontext currencycontextbuilder currencycontextlistener youll notice currencycontext class force setting data therefore allows accessing field straight away build method trigger exception called required data since required must fail early workfow indicate initialization failed listener run early request kernel event even mandatory shouldnt trigger exception case cannot find data could another listener example employee optional example optional sub employee employeecontext employee sub dto employeecontextbuilder employeecontextlistener youll notice nullable dto field allowing check code employee currently available sub dto follows rule mandatory immutable dto field accessible first level builder build method called even employee set trigger exception simply skip build phase dependency replacement new sub available time new code usage legacycontext allowed anymore usage contextgetcontext allowed anymore turn sub missing data accessible via legacy one sub must enriched new one may needed part sub needed service like language iso code still inject whole sub small needed part also start refacto current code rely legacycontext especially well able fix many improper dependency injection like yaml myservice serviceprestashopadapterlegacycontextgetcontextshopid replaced course autowiring possible even encouraged internal code also adapted mybetterservice prestashopprestashopcorecontextshopcontext lot code adapted better change actually impact class constructor adn internal code public behavior andor method line backward compatibility policy class onlymostly service constructor modified course change done major version ideally breaking change legacy building new sub bring stability building architecture also naturally centralize building process offer control highly unstable key component could also benefit place still relying legacy subcontextbuilder implement alternate interface legacycontextbuilderinterface single buildlegacycontext method since builder already able build new sub based parameter provided also correctly setfill legacy dedicated listener charge looping builder implement legacycontextbuilderinterface build legacy automatically long required data provided thanks new legacy building well able remove legacy code piece piece modern code symfony page since dont rely anymore legacy would still correctly built though core code still relies refactored rely new sub also mainly keep backward compatibility especially module backward compatibility layer implemented sub consider removingcleaning legacy code built legacy first place may split internal config front office back office many thing example done included file like configincphp new handled symfony theyre available yet']"
16,157, Cluster Traffic Management Strategy,['Cluster Traffic Management Strategy'],"['kubernetes', 'haproxy', 'load balancer', 'aws', 'cloud foundry', 'endpoint', 'nginx', 'balancer', 'paas', 'ingres controller']","['serve http traffic keeping tcp port http closed hsts preload list add domain hsts preload list requirement serve valid certificate redirect http http host serve subdomains http actually check wwwdomaincom serve hsts header base domain http request endpoint provide requirement cloud foundry app endpoint already serf right hsts security header haproxy could configured serve additional preload includesubdomains flag cannot keep port http closed endpoint implement second elb listening http http haproxy http http redirect serve right header increase dependency haproxy service must serve root domain apex domain allowed serve cname record rootapex domain must configure record domain issue serving service elb cloudfront implement basic aws api gateway default mock response return right http header stricttransportsecurity actual content response irrelevant custom domain name creates aws cloud front distribution provide public access api aws route alias resource record serve aws cloud front distribution record consequence setup aws api gateway domain name required access ssl certificate uploading certificate different step create aws cloud front distribution manually', 'title adr haproxy request rewriting adr haproxy request rewriting want serve hsts header http request apps domain safeguard existing user mitmed insecure connection improve user experience select hostname doesnt protocol note without preloading browser wont help first time user want leave open able override header tenant application wish feature requires conditionally process modify request header several possible implementation implement logic gorouter gorouter shall process add header required supporting specific hsts header allowing configure sort behaviour default value allow inject additional header missing current gorouter implementation support feature require added add intermediate proxy example nginx haproxy front gorouters elb implement external cdn front paas origin paas entry point commercial cdn capacity add additionally header conditionally aws elb support logic short term consequence cannot solve problem want add additional logic cdn optional part platform try keep simple possible consider optional solution would implement logic gorouter requires development effort merged upstream implement short term second proxy front gorouter implement haproxy front router haproxy default solution official distribution really powerful good support enough feature cover setup colocated gorouter proxying directly localhost ssl termination haproxy plain text gorouter two service colocated reuse code official haproxy job cfrelease although fork add additional setting haproxy configuration future work implement propose add logic gorouter allow define additional header consequence positive able easily add logic rewrite http communication application haproxy haproxy ssl termination better performance gorouter although low impact elb terminating end user connection keep alive connection gorouterhaproxy haproxy support websockets http multiplexing implement http http redirect haproxy negative add additional latency every request maintain custom haproxy release another moving part monitor take account see also adr', 'want serve hsts header http request apps domain safeguard existing user mitmed insecure connection improve user experience click hostname doesnt protocol note without preloading browser wont help first time user want leave open able override header tenant application wish feature requires conditionally process modify request header several possible implementation implement logic gorouter gorouter shall process add header required supporting specific hsts header allowing configure sort behaviour default value allow inject additional header missing current gorouter implementation support feature require added add intermediate proxy example nginx haproxy front gorouters elb implement external cdn front paas origin paas entry point commercial cdn capacity add additionally header conditionally aws elb support logic short term consequence cannot solve problem want add additional logic cdn optional part platform try keep simple possible consider optional solution would implement logic gorouter requires development effort merged upstream implement short term second proxy front gorouter implement haproxy front router haproxy default solution official distribution really powerful good support enough feature cover setup colocated gorouter proxying directly localhost ssl termination haproxy plain text gorouter two service colocated reuse code official haproxy job cfrelease although fork add additional setting haproxy configuration future work implement propose add logic gorouter allow define additional header consequence positive able easily add logic rewrite http communication application haproxy haproxy ssl termination better performance gorouter although low impact elb terminating end user connection keep alive connection gorouterhaproxy haproxy support websockets http multiplexing implement http http redirect haproxy negative add additional latency every request maintain custom haproxy release another moving part monitor take account see also adr']"
17,214, Build System Configuration Decisions,['Build System Configuration Decisions'],"['configuration file', 'maven', 'build tool', 'gradle', 'configuration', 'dependency', 'config', 'kotlin', 'environment variable', 'implementation']","['adr groovy proposed author damir murat damirmuratgit gmailcom reviewer none would like language language extension close similar compatible java offer significant productivity booster time java syntax often verbose common various ide trick foldingunfolding party library lombok deal clutter would much better language clear concise syntax almost naturally understandable java developer time language provide userfriendly extension java sdk best practice implementation various programming pattern architectural system klokwrkproject groovy primary language consequence positive groovy concise productive java language straightforward syntax java developer feel like java without clutter groovy entirely compatible java fact almost every java code compiled executed groovy code java library groovy viceversa besides concise straightforward syntax groovy offer various extension jdk implement many bestpractice pattern annotation ast abstract syntax tree transformation groovy time statically compiled dynamic language allowing programmer tune dynamism appropriate problem hand great support generalpurpose testing via spock library groovy various scripting developing fullfeatured enterprise application groovy opensource language maintained community hosted apache foundation groovy offer way quick prototyping minimal ceremony klokwrkproject development team significant experience groovy negative java tooling often lag support groovy although situation currently quite acceptable ide support code coverage support static analysis area much work needed future graalvm native image support related graalvm native image support look like thing improving considerably java latest graalvm release graalvm nativeimage startup time groovy tutorial also graalvm native image still limited usage domain fullfeatured enterprise application critical fast application start rather quickly run pro con graalvm nativeimages neutral many noninformed developer perceive groovy strictly dynamic language capable suitable scripting learning curve especially one want groovy idiomatic way however learning curve significantly lower jvm language considered java kotlin considered extensively since development team basic knowledge language ecosystem nevertheless kotlin look like interesting much different java case groovy look like many kotlin feature counterpart groovy significant exception coroutines might addressed project loom future reference design pattern groovy groovy language documentation spock library groovy action second edition graalvm nativeimage startup time groovy tutorial pro con graalvm nativeimages', 'polyglot codebase proposed rely scala part code base problem statement james written java long time recent year java modernized lot decade slow progress however meantime software relying jvm started supporting jvm language keep relevant includes groovy clojure scala recently kotlin name open language problem james adoption driver nowadays library framework targeting jvm expected support usage one several language james mail server also development framework reach expectation time developer language adopt function programming idiom solve problem considered strategy let user figure make polyglot setup document usage polyglot mailets popular language document usage polyglot component popular language actually implement mailets popular language actually implement component popular language language upperroman clojure groovy iii kotlin scala decide mean write mailets scala demonstrate done running server also mean writing andor refactoring server component scala starting relevant positive consequence modernize part james code leverage scala richer ecosystem language overcome java limitation topic attract people would like java negative consequence add even knowledge requirement contribute james scala build time longer java build time pro con let user figure make polyglot setup pro dont anything new con boring like new challenge java declining despite language modernization mean long term people contribute james document usage polyglot mailets popular language pro lot work yet open james attract people outside java developer community con documentation without implementation often get outdated thing move forward dont really gain knowledge polyglot matter community wont able help user much document usage polyglot component popular language pro open james attract people outside java developer community con documentation without implementation often get outdated thing move forward complex subject probably harder actually implementing component another language dont really gain knowledge polyglot matter community wont able help user much actually implement mailets popular language pro probably lot work mailet simple class probably easy jvm language make learn work maybe help basic polyglot experience small enhancement codebase document process illustrate actual code open james attract people outside java developer community con negative impact build time dependency download actually implement component popular language pro leverage modern language complex component make learn work maybe help basic polyglot experience small enhancement codebase document process illustrate actual code open james attract people outside java developer community con make codebase complex requiring knowledge another language negative impact build time dependency download clojure pro functional language con weak popularity prior experience among current active commiters statically typed hence likely fit size project groovy pro advanced current java topic con prior experience among current active commiters replaced jvm community kotlin last year iii kotlin pro great intellij support good part scala fpish arrow coroutine handling highperformance con prior experience among current active commiters lack construct like proper pattern matching persistent collection despite progress done arrow kotlin community aim mostly writing better java scala pro rich community ecosystem existing knowledge among current active commiters con work master slow build probably require code change reference mailing list thread', 'title arch openjdk parent index haschildren false layout post sidenav doc permalink adrsarchopenjdkversions proposed response iitd architecture iitd infrastructure team others official statement openjdk positioning proposed deciders licence change technical story description httpsappsnrsgovbccaintjirabrowsearch problem statement response iitd architecture iitd infrastructure team others official statement openjdk positioning driver official information innovation technology division iitd statement needed direction forward target version javajdks iitd hosted java application oracle changed licensing model subscription model charging oracle java product release java end legacy versioning release cadence model java new beginning oracle extended public update oracle jdk january commercial production least end individual desktop oracle jdk part interchangeable oracle openjdk build want move next release java java java suggested release oracle oracle posted information roadmap java client including applet web start oracle plan discontinue contribution jdk update project january security reason iitd architecture encourages upgrademigration java application least jdk scope iitd server client side application owned iitd run java considered endorse migration openjdk continue oracle jdk pay license fee outcome jdk later oracle openjdk jdk binary window macos linux available releasespecific page jdkjavanet targz zip archive example archive jdk may found jdkjavanet may extracted command line tar xvf openjdkbintargz unzip openjdkbinzip depending archive type positive consequence remove dependency oracle jdk licensing reduces security vulnerability older jdk version java still predominant without saying version java updated immediately even version significant remediation fleet vulnerability vulnerability httpswwwcvedetailscomproductoraclejrehtmlvendorid httpswwwcvedetailscomproductoracleopenjdkhtmlvendorid negative consequence slow performance may occur migration issue addressed link detail topic discussed iit available architectural response document hosted iit confluence internal required httpsappsnrsgovbccaintconfluencedisplayaropenjdkforalljavaapplications infrastructure smt confluence space oraclejdk openjdk last edits circa march httpsappsnrsgovbccaintconfluencedisplaysmtoraclejdkvsopenjdk httpsblogsoraclecomjavaplatformgroupupdateandfaqonthejavasereleasecadence httpswwworaclecomtechnetworkjavajavaseovervieworaclejdkfaqshtml previous oracle java license model several free binary code license bcl paid oracle commercial term simplify provide full licensing transparency clarity oracle java provides two distinct java release oracle openjdk release open source gnu general public license classpath exception gplvcpe since java oracle java product release includes oracle jdk java later oracle jre java web start java otn license agreement java license permit personal development testing prototyping demonstrating cost organization getting oracle java binary cost simply continue oracle openjdk release available jdkjavanet getting oracle java binary cost personal user development continue get oracle java release javacom personal user oracle technology network otn developer wishing oracle jdk oracle jre require java subscription supported customer oracle product continue get oracle java binary oracle support oracle software delivery cloud customer location note many thirdparty software vendor beside oracle develop test certify software oracle jdk recommend run application application vendor may isv agreement oracle provide java update run application vendor product case separate license oracle java running application please contact application vendor determine whether application vendor authorized distribute java application migrate java jdk dependency oracle jdk openjdk upgrade older version least jdk preference encourage team target move jdk']"
18,50, Code Formatting Decision Strategy,['Code Formatting Decision Strategy'],"['style eslint', 'typescript', 'eslint prettier', 'eslint', 'stylelint', 'code style', 'enforce code', 'coding style', 'linting code', 'tslint']","['eslint want enforce consistency code catch many error automatically able linting code good practice achieve aim eslint standard linter modern javascript good support typescript react though plugins check code style eslint let prettier precedence eslint prettier conflict style recommended configuration plugins possible run eslint part test suite consequence eslint enables agree enforce code style without keep head developer meaning shouldnt discus individual code style violation come', 'eslint want enforce consistency code catch many error automatically able linting code good practice achieve aim eslint standard linter modern javascript good support typescript though plugins check code style eslint let prettier precedence eslint prettier conflict style recommended configuration plugins possible run eslint part test suite consequence eslint enables agree enforce code style without keep head developer meaning shouldnt discus individual code style violation come', 'eslint want enforce consistency code catch many error automatically able linting code good practice achieve aim eslint standard linter modern javascript good support typescript though plugins check code style eslint let prettier precedence eslint prettier conflict style recommended configuration plugins possible run eslint part test suite consequence eslint enables agree enforce code style without keep head developer meaning shouldnt discus individual code style violation come']"
19,63, Data Management System Optimization,['Data Management System Optimization'],"['atlasdb', 'concurrency', 'lock table', 'cassandra', 'timelock', 'column', 'table', 'metadata', 'operation', 'storage']","['twostage encoding transaction table value transaction cassandra timestamps value cassandra written together timestamp note differs column column atlasdb schema operation cassandra typically wallclock time server paxos completed timestamp writes applied multiple node quorum disagree reading value key latest write win pick value highest write timestamp however returning perform blocking read repair update dissenting node quorum information latest write update happens return value user important correctness protocol cassandra lightweight transaction weakness known consistency issue cassandra checkandset operation operation occurs following happens cassandra implemented storageproxycas round paxos phase run paxos state stored paxos system table round paxos successful proposed evaluated value read fails match expected value successful request made seeking paxos round committed node applies mutation proposed paxos round committed node part response handler plus node block get read consistency many node quorum eachquorum possible may fail second third substep step however node may performed mutation recommended successful paxos round others user receiving failure response response seems fine long ensure relevant paxos round value read resolved practice accomplished serial consistency level however reading serial costly furthermore another problem cassandra ttl system keyspace hour includes paxos system table cassandra keep track paxos promise thus failed mutation actually applied node followed extended failure network partition could result inconsistent read particular supposing threenode cluster successful value coordinator successfully applied mutation quorum read cell could read quorum chosen quorum included value newer assuming weird writetime shenanigan problem atlasdb rely putunlessexists transaction table cassandra practice mean transaction given timestamp could read committed uncommitted furthermore possible quorum node believe transaction uncommitted decide transaction rolled back would accept putunlessexists cell interpreted atlas meaning transaction aborted range movement cassandra determines node participate paxos based replication group relevant key however may change runtime change cluster topology naturally lead disjoint quorum cassandra currently resistant case would require substantial change paxos algorithm thus treating dealing case outside scope proposal criterion good solution problem demonstrate following characteristic repeatable read nonnull value commit timestamp aborted read table given timestamp future read return value primary issue current system eventual read efficiency eventually reading cell transaction table require rpc atlasdb coordinating cassandra node within cassandra rpc pair cassandra node relevant replication group write efficiency writing cell transaction table require rpcs possible bearing mind constraint optimising property important two theory pue table want define putunlessexists table support two operation putunlessexists get interface may look superfluous keyvalueservice interface already similar endpoint must support repeatable read java public interface putunlessexiststable void putunlessexistsmultiplemap keyvalues throw keyalreadyexistsexception listenablefuturemapk getiterablek key primitive define putunlessexists value pair value state enumeration state indicates whether value staging committed java public interface putunlessexistsvalue value putunlessexistsstate state public enum putunlessexistsstate staging committed perform write putunlessexists value following protocol noting kvslevel operation puev staging putv committed perform read following protocol read current value database committed return present return null staging perform casv staging staging perform putv committed return notice protocol meet criterion outlined eventual read efficiency steady state value going committed read protocol committed value involves single read quorum consistency argument read consistency bit subtler found appendix adr intuitively protocol put committed value writer reader know quorum node value written staging never change value moving staging committed read committed value transaction service putunlessexists table provide implementation transaction service functionality required transaction service essentially match putunlessexiststablelong long arent actively aware improvement ticket encoding strategy transaction cell strategy figuring put individual value put problem one challenge face working cassandra choose suitable timestamps writes cassandra seems willing tolerate inconsistency clock drift allowed atlasdb lightweight transaction cassandra apply writes serverside timestamp coordinator though combined bit conditional logic ensures write take place term paxos another one writetime greater assuming paxos consistent fine however write protocol performs hard put second step choose timestamp settled choosing large constant maximal timestamp hard put exceed reasonable timestamps wed encounter practice still allow deletion user perform clean transaction range workflow part backup restore plausible could included exposing new endpoint cassandra thrift api copy get timestamp later whats logic allows user hit directly would require new endpoint thus new dependency cassandra fork making rollout messy second step write protocol casv staging committed requires another round paxos writes implementation define putunlessexiststablek interface described switch transaction service rather keyvalue service directly useful allows avoid change transaction service logic much deployment keyvalueservices dont actually problem oracle postgres simply instantiate much simpler implementation pass relevant call keyvalue service part needed rework checkandsetcompatibility classification keyvalue service separately track failure detail consistency guarantee remainder section largely focus implication deployment running cassandra consensus forgetting store introduce new abstraction consensusforgettingstore table intended capture behaviour cassandra identified read repeatable still remain consistency guarantee define main operation consensusforgettingstore required support protocol outlined java public interface consensusforgettingstore atomic put unless exists operation method throw exception consistency guarantee subsequent pue may succeed fail nondeterministically subsequent get may return optionalofvalue optionalempty even optionalofothervalue another pue failed past nondeterministically void putunlessexistscell cell byte value throw keyalreadyexistsexception atomic operation verifies value cell successful link consensusforgettingstoreputcell byte called subsequent get guaranteed return optionalofvalue subsequent pue guaranteed throw keyalreadyexistsexception void checkandtouchcell cell byte value throw checkandsetexception listenablefutureoptionalbyte getcell cell put operation offer consistency guarantee exception thrown multiple put cell different value may result nonrepeatable read void putcell cell byte value note multicell version actual implementation omitted interest conciseness implementation method generally involve passing value perhaps small amount wiring nonetheless interface useful allows simulate legitimate failure layer part testing resilientcommittimestampputunlessexiststable implement putunlessexiststablek interface consensusforgettingstorek applying protocol discussed theory section also make going store tablelevel key value keyvalue service implemented twophaseencodingstrategy discussed earlier reuse logic ticketsencodingstrategy encoding start timestamp key cell ensures still profit performance optimisation written specifically transaction shared modulus generation timelock protection overall hotspotting opted reuse transaction table avoid unnecessary overhead cassandra reasoning justify specific table parameter tuned specific way remains valid transaction value also reuse ticketsencodingstrategys deltaencoded varlong simply append one byte end value staging value committed value primary concern would space readability optimal space consider likely method achieve better efficiency adding one bit rather one byte also since majority value committed could simply zero byte staging one committed probably value relative simplicity approach well defense user attempting read data incorrect encoding strategy deployment testing much work transaction schema version done transaction general easily repurposed one simply set targettransactionsschemaversion configuration going required schema lease process transaction set existing transaction test become parameterised run legacy transaction schema curiously time transaction written wired also implemented simulated test version consensus forgetting store forget value configurable probability fuzz test putunlessexists table give stronger evidence protocol safe implemented correctly considered one possible solution could perform read serial consistency override default system keyspace ttl cassandra longer however would still pick reasonable bound paxos log content system keyspace could kept furthermore read serial consistency achieve eventual read efficiency require cassandra node perform round paxos internally adding two rpcs coordinator act paxos proposer node another possible approach involves state machine user allowed propose value part read protocol opposed required commit staging value written author think could probably work difficult reason one concerned behaviour concurrent proposal whether dueling proposersstyle situation possible consequence transaction rolled globally cassandra deployment longer exposed correctness bug hope though dont strong evidence owing small sample size extreme difficulty root causing reduce incidence atlasdb corruption ticket general backup restore workflow case manually manipulate one transaction table aware new serialized form exist particular transaction table read isolation without caution value unambiguous indicating whether written transaction transaction user aware staging value safe though final possible even likely performance transaction worse transaction strictly work value passed database strictly larger however dont expect add constant amount overhead small steady state isnt fair comparison case transaction approach correct appendix proof read consistency first prove lemma lemma first time quorum node latest writes contains combination staging committed longer possible write anything staging committed clear condition satisfied pue value cannot succeed since every read include recent write remains show two put protocol putv committed assume write protocol putw committed must successful puew staging since operation would satisfy condition lemma implying pue must occurred quorum node latest writes already contained contradiction since pue would failed assume read protocol casw staging staging must staging written condition lemma satisfied choose earliest write one replicated read could result pue also could result earliest write read part would read latest value quorum node assume read protocol putw committed similarly successful casw staging staging must occurred beforehand condition lemma satisfied follows proof prove second lemma lemma committed written longer possible write anything staging committed committed written put write protocol must followed successful puev staging quorum node staging satisfying lemma written put read protocol must followed successful casv staging staging satisfying lemma result soon committed written committed never written since protocol return committed value point onwards returned ergo repeatable read', 'targeted sweep legacy sweep achieve transactional guarantee atlasdb maintains historical version cell written eventually existing future transaction start timestamp large enough historical version never visible resulting unnecessary cruft issue due taking storage space underlying also certain access pattern require scanning obsolete historic version leading significant performance degradation time process removing old historic version cell table atlasdb called sweep refer current implementation sweep atlasdb relying legacy sweep legacy sweep iterative procedure given table reference start row sequentially scan historic version cell searching candidate written transaction start timestamp commit timestamp lower sweep timestamp sweep timestamp timestamp expected lower start timestamp open transaction rendering last historic version cell prior sweep timestamp effectively invisible thus obsolete version therefore safely deleted legacy sweep continues scan enough candidate found processing least one full row cell deletes obsolete historic version two main mode running legacy sweep background sweep background task repeatedly chooses table sweep proceeds run iteration sweep row processed manual sweep triggered rest endpoint cli perform iteration full sweep given table start row note corner case sweep timestamp may mistaken readonly transaction running longer one hour also subject clock drift write transaction loses lock committing table allow readonly transaction must therefore defensively write garbage deletion sentinel empty value timestamp valid start timestamp readonly transaction encounter sentinel signalizes could historic version able read may deleted transaction must therefore abort table allows readonly transaction therefore requires sentinel defined table sweep strategy conservative allows readonly transaction requires sentinel thorough time identified number issue architecture implementation legacy sweep outlined legacy sweep slow even table historic version cell swept still scan historic version cell table take week extreme case performance get worse data written increasing number entry legacy sweep must iterate particularly large problem user table whose access pattern mandate must swept regularly performance stability reason background sweep busy week sweeping large table must resort manual sweep avoid performance stability degradation obviously errorprone subject random failure since manual sweep interrupted reason automatically retried moreover legacy sweep depends complicated heuristic decide table sweep next given table must swept frequently slowness legacy sweep significant developer time must spent tweaking heuristic produce desired effect legacy sweep exert significant pressure underlying scanning table find historic version cell case cause significant pressure underlying particular cassandra even though interested start commit timestamp transaction writes performed understanding cassandra internally still load content cell memory regardless legacy sweep get stuck table regularly new row added increasing lexicographical order example table keyed steadily increasing fixed varlong legacy sweep end sweeping table indefinitely iteration discover new row therefore never declaring table fully swept consequence table swept issue noticed manually resolved problem legacy sweep architectural cannot solved simply improving implementation therefore decided change architecture sweep require scanning table find candidate delete instead maintains sweep queue contains information writes atlasdb conjunction ranged tombstone ranged deletion delete version cell two timestamps allows sweep table parallel without read data table swept note time writing adr cassandra implementation ranged tombstone actually avoids reading table targeted sweep targeted sweep queue targeted sweep queue persisted queue containing relevant metadata write committed atlasdb metadata encapsulated writeinfo object contains following tablereference table written cell written row column name start timestamp transaction performed write flag specifying write tombstone delete regular write high level sweeping targeted sweep queue targeted sweep follows whenever transaction commit writes persisted writes put corresponding writeinfo queue targeted sweep read entry front queue depending sweep strategy table specified entry acquire appropriate sweep timestamp compare start timestamp transaction sweep timestamp start timestamp must pause try later check commit timestamp transaction transaction committed abort transaction aborted delete write pop queue read next entry transaction committed timestamp greater equal sweep timestamp pause try later otherwise insert ranged tombstone follows strategy conservative write garbage deletion sentinel put ranged tombstone deleting version cell write timestamp deleting sentinel write strategy thorough write tombstone put ranged tombstone deleting version cell write timestamp deleting potentially existing sentinel write otherwise put ranged tombstone deleting version cell write timestamp deleting potentially existing sentinel write pop queue read next entry detailed implementation targeted sweep targeted sweep queue refer implementation section consequence benefit targeted sweep perform sweep several order magnitude time faster legacy sweep verified cassandra far load underlying significantly reduced verified cassandra far order sweeping fair suffer issue caused frequently appending new row end table drawback added overhead committing transaction information must persisted sweep queue part commit note caused significant regression benchmark implementation since assume thread may die moment implementation detail correct ordering crucial describe targeted sweep schema define necessary term fine timestamp partition timestamp divided integer division fine timestamp partition write fine timestamp partition start timestamp transaction wrote coarse timestamp partition analogous fine partition except number divided sweep strategy tablelevel property specifying sweep timestamp calculated deletion sentinel whether latest write swept away well case delete also sharding strategy enable better parallelisation targeted sweep maximum number shard supported effectively splitting queue number disjoint queue note number shard increased cannot lowered sweepable cell table table store actual information writes atlasdb transaction committed data persisted writes transaction partitioned based shard sweep strategy table fewer writes partition persist necessary information number cell nondedicated row row component calculated described writes partition insert single cell nondedicated row acting reference one dedicated row exclusively writes transaction partition row component timestamppartition varlong derived start timestamp writes row nondedicated row fine timestamp partition dedicated row start timestamp metadata byte blob encoding targetedsweepmetadata follows bit sweep strategy thorough conservative bit marking dedicated row nondedicated dedicated bit shard number inclusive bit dedicated row marking ordinal number inclusive bit unused note row component hashed avoid hotspotting column component timestampmodulus varlong storing start timestamp write modulo writeindex varsignedlong whose purpose overloaded case storing fewer writes row writeindex nonnegative increasing number deduplicate multiple writes start timestamp storing writes instead put single cell nondedicated row acting reference one dedicated row case writeindex negative number indicating many dedicated row dedicated row contain cell dedicated row writeindex deduplication since entry row timestampmodulus value writereference containing remaining required metadata targeted sweep tablereference cell boolean specifying write tombstone since persisted size cell object byte dominating size entry expect transaction per row allowing writes transaction row size one nondedicated row exceed practice lower note could technically full transaction row probability exceeding value infinitesimal allow maximum entry per dedicated row ensures dedicated row larger still allowing million writes single transaction practice even long number shard greater note cell nondedicated row calculate writes start timestamp simply multiplying timestamppartition adding timestampmodulus sweepable timestamps table auxiliary table locating next row sweepablecells table read since timestamp partition sparse therefore requiring many lookup locate nonempty row nondedicated row sweepablecells represented single cell table row component shard varlong containing shard row entry timestamppartition varlong corresponding coarse timestamp partition entry row sweepconservative boolean encoded blob specifying row contains entry thorough conservative sweep column component timestampmodulus fine timestamp partition row sweepablecells fall coarse partition specified timestamppartition row component value unused empty byte array locate first row sweepablecells entry timestamp start row sweepabletimestamps corresponding coarse partition read first column great enough timestampmodulus increasing row necessary cell found way timestampmodulus timestamppartition row sweepablecells looking sweep progress per shard table table store targeted sweep progress well information number shard sweep queue row component shard varsignedlong containing shard tracking progress sweepconservative boolean encoded blob specifying looking conservative thorough sweep sweepabletimestamps named column value varlong containing timestamp targeted sweep swept shard strategy persist number shard sweep queue distinguished row table row defined shard sweepconservative true note writes table atomic check set value allowed increase request update cell lower value effect writing sweep queue whenever snapshottransaction commit writes persisted enqueues sweep queue sweep queue creates list writeinfo containing relevant information partition list according sweep strategy table information read table metadata cached number shard sweep queue shard determined hash tablereference cell partition put entry sweepabletimestamps table start timestamp transaction finally partition put writeinfo sweepablecells table fewer entry list write many cell table row column component calculated described writeindex starting increasing entry entry list put single cell table acting reference dedicated row row reference writeindex negative number absolute value equal number dedicated row number writes divided rounded put cell dedicated row timestamppartition start timestamp transaction fine partition metadata encodes row dedicated row row ordinal number rest note entire start timestamp timestamppartition dedicated row avoid clash case nondedicated row multiple reference dedicated row reading sweep queue reading sweep queue done order writing given shard strategy minimum exclusive timestamp targeted sweep read sweepprogresspershard table described later maximum exclusive timestamp targeted sweep sweep timestamp following want locate fine timestamp partition first row sweepablecells entry greater minimum exclusive timestamp mint starting coarse partition mint getrowscolumnrange check cell sweepabletimestamps satisfying condition increase coarse partition repeat either candidate found coarse partition grows larger coarse partition maxts maxts maximum exclusive timestamp latter occurs guaranteed entry sweepablecells shard strategy timestamps specified range otherwise fine timestamp partition effectively reference row sweepablecells expected contain least one cell may true degenerate case thread writing cleaning queue process read cell row start timestamp greater last swept timestamp referring dedicated row necessary either finish row exceed maxts read entry plus additional entry ensure read entry start timestamp latest one cleaning sweep queue entire nondedicated row sweepablecells row sweepabletimestamps needed anymore remove follows given shard strategy fine timestamp partition delete entry sweepablecells corresponding row note cassandra entire row deleted single tombstone first read nondedicated row find reference dedicated row delete delete nondedicated row necessary given coarse timestamp partition also delete row defined shard strategy coarse timestamp partition sweepabletimestamps targeted sweep implementation targeted sweep read write metadata sweep queue instead sequentially scanning table find historic version entry read sweep queue check commit timestamp transaction performed write start timestamp commit timestamp lower sweep timestamp single ranged tombstone delete prior version cell note cassandra implementation deletealltimestamps method writes ranged tombstone require reading information therefore provides substantial improvement comparison legacy sweep find previous timestamps deleted one one commit timestamp greater sweep timestamp targeted sweep must wait sweep timestamp increase enough entry processed generally issue since likely happen targeted sweep processing writes written within last hour targeted sweep number background thread strategy controlled install config continuously cycling shard find next shard sweep thread request timelock lock shard strategy successful start iteration targeted sweep otherwise request lock next shard finally giving pausing cycle unsuccessfully shard mechanism ensures synchronization across multiple node service assuming thread successfully acquired lock following calculate sweep timestamp sweep strategy read last swept timestamp shard strategy sweepprogresspershard get batch writeinfo sweep queue read sweep queue described mint last swept timestamp maxts sweep timestamp find candidate skip step start timestamps batch check transaction committed must abort behaviour legacy sweep transaction committed sweep timestamp must progress targeted sweep past start timestamp remove writes greater start timestamps batch delete writes referenced aborted transaction note direct deletes ranged tombstone partition remaining writeinfos cell tablereference take greatest start timestamp partition one ranged tombstone per partition writes cell table batch going lower start timestamp therefore going deleted well strategy conservative write garbage deletion sentinel put ranged tombstone deleting version cell write timestamp deleting sentinel write strategy thorough write tombstone put ranged tombstone deleting version cell write timestamp deleting potentially existing sentinel write otherwise put ranged tombstone deleting version cell write timestamp deleting potentially existing sentinel write new sweep progress described greater detail guarantee minimum start timestamp swept future greater fine coarse partition previous last swept timestamp clean sweep queue accordingly previously explained persist sweep progress sweepprogresspershard shard strategy finally regardless success failure iteration unlock timelock lock shard strategy schedule next iteration sweep thread delay second calculating sweep progress wish update progress greatest value guarantee swept multiple case consider order find candidate row sweepablecells reading sweep queue update sweep timestamp none timestamps batch committed sweep timestamp read entry sweepablecells sweep timestamp update minimum sweep timestamp first timestamp would written next row sweepablecells otherwise update greatest timestamp among writes batch', 'ticket encoding transaction table transaction technical architectural record still work progress transaction table atlasdb atlasdb transaction table keep track whether transaction started given timestamp committed aborted case committed commit timestamp logically table mapping longs longs special value meaning transaction aborted transaction inflight yet either commit abort entry table starttimestamp committimestamp table accessed via transactionservice class atlasdb offer simple interface codeblock java public interface transactionservice checkfornull long getlong starttimestamp maplong long getiterablelong starttimestamps void putunlessexistslong starttimestamp long committimestamp throw keyalreadyexistsexception practice calling single multitimestamp version get may read table though nonnull result may cached putunlessexists performs putunlessexists operation table supported mean thrift call cassandra insert exists sql statement relational physical representation cassandra atlasdb table similar schema cassandra blob partition key store row called key clustering key two component blob column called column biginteger timestamp called column blob value transaction table column always single byte corresponding string column special value practice dont pay much attention value interestingly key varlong encoding start timestamp value similarly varlong encoding commit timestamp cassandra representation transaction table introduced may look follows key column column value xffffffffffffffffff xefefd xefefdb detail varlong encoding fiddly exhibit several desirable property varlong encoding orderpreserving nonnegative number varlongts varlongts vice versa significant allows one perform range scan transaction table given timestamps atlas positive longs straightforward way varlong encoding support negative number meaning encoding positive timestamps transaction successfully committed transaction aborted however choice encoding also issue two particularly relevant one follows varlong encoding number near numerically also close bytespace notice encoded form differ three lowestorder bit furthermore writes transaction table take place number numerically close correspond actively running transaction thus key close bytespace considering cassandra consistent hashing handle data partitioning given point time majority writes cluster end going node hotspotting undesirable lose horizontal scalability writes bottlenecked single node regardless size cluster varlong encoding particularly efficient purpose fails exploit characteristic distribution data particular special value particularly large varlong encoding negative longs always encode byte storing full value commit timestamp also wasteful given know generally slightly higher value start timestamp principle good transaction service thus define three principle help guide make implementation transaction service good one horizontal scalability good transaction service must horizontally scalable possible increase write bandwidth increasing number database node andor service node performing writing compact representation good transaction service unnecessarily excessive disk space addition saving disk usage compact representation also improves ability query result cached memory improve time taken execute query data read disk many key value service allow cache specified memory overhead compact timestamp representation improves ability cache store logical information thus improving hit rate range scan good transaction service support range scan without needing read entire table postfilter various backup restore workflow atlasdb important able execute restores timely fashion reasonable usage pattern rup good transaction service underlying service must follow standard principle constitutes reasonable usage pattern underlying service implement ticket encoding strategy along feature needed support efficient operation strategy supporting feature introduced following section ticket encoding strategy logical overview divide domain positive longs disjoint partition constant size call size partitioning quantum partition contiguous range start multiple thus first partition consists timestamps second assign constant number row partition seek distribute start timestamps evenly possible among row number increase practice least significant bit timestamp row number would thus store value associated timestamps row given partition value interval disambiguate timestamps dynamic column key varlong encoding timestamps offset relative base formally given timestamp proceed follows denotes integer division identify row belongs given identify column belongs given notice given similarly decode original identify relevant partition given identify offset column key given identify offset second part row key given original timestamp may easier think timestamp written tuple row component pair column key divide bijection tuples range exclusive range exclusive furthermore bijection orderpreserving ordering tuples interpreted lexicographically diagram illustrate clearly work physical implementation ticket store information transaction committed new encoding scheme transaction table cassandra given want avoid hotspotting ensure horizontal scalability ensure row may writing data distributed differently bytespace thus reverse bit row encoding codeblock java private static byte encoderownamelong starttimestamp long row starttimestamp partitioningquantum rowsperquantum starttimestamp partitioningquantum rowsperquantum return ptbytestobyteslongreverserow fixedlong encoding constant byte instead variable chosen ensure range scan supported reversed form variable length encoding tend amenable range scan dynamic column key simply varlong encoding changed encoding value well transaction successfully committed delta encoding scheme instead take varlong difference commit start timestamps typically small positive number help keep size table separately transaction aborted store empty byte array special value instead negative number unnecessarily large choosing choose value based characteristic keyvalueservice storing timestamp data recalling principle rup simplify discussion section assume divide cassandra following atlas team recommended cassandra best practice seek bound size individual row considering varlong take byte positive integer explicitly empty byte array represent transaction failed commit estimate maximum size row given choice value notice number timestamps actually store given since row partition different given startcommit timestamp pair row key occupies byte said sstables row key represented thus focus column key value column key varlong encoded number bounded thats largest offset might actually store value theoretically could full byte large positive number though practice likely considerably smaller selected value configuration row store startcommit timestamp pair thus number row varlong encoding able represent within byte account bit space cassandra create composite buffer include column part physical row take additional byte thus byte column key byte value leading total byte per startcommit timestamp pair row bounded leaf quite bit headroom worth mentioning practice implementation reason unlikely full startcommit timestamp pair single row practice value likely byte rather byte case even adverse circumstance still avoid generating excessively wide row streamlining putunlessexists putunlessexists operation performed serial consistency level cassandra meaning read writes paxos consensus thrift expose checkandset operation apis codeblock java casresult casrequired binary key required string columnfamily list expected list update required consistencylevel serialconsistencylevelconsistencylevelserial required consistencylevel commitconsistencylevelconsistencylevelquorum throw invalidrequestexception ire unavailableexception timedoutexception sufficient original transaction schema row key store information one start timestamp putunlessexists operation empty row row one column however notice sufficient transaction row may contain data multiple start timestamps api requires provide list old column dont know beforehand considered reading existing row adding new column cql api behaviour insert exists column match semantics want however solution found unacceptable performance benchmarking thus decided extend thrift interface add support multicolumn putunlessexists operation semantics want different empty list succeeds long existing column column family provided key overlap set column added codeblock java casresult putunlessexistsrequired binary key required string columnfamily list update required consistencylevel serialconsistencylevelconsistencylevelserial required consistencylevel commitconsistencylevelconsistencylevelquorum throw invalidrequestexception ire unavailableexception timedoutexception multinode contention residue improvement still run issue client whether across multiple service node node issue multiple request parallel putunlessexists request requires round paxos cassandra maintains paxos sequence level partition key request would contend far paxos concerned even column actually disjoint internally cassandra node trying apply update partition whether update applied order take place agreed paxos although node accepting multiple proposal dont conflict one round consensus committed time since update conditional also cassandra leaderless implementation paxos meaning dueling proposer issue might slow individual round protocol multiple node trying concurrently propose value batching request client side partition could useful though still limited performance would poor service many node cassandra table tuning creating table cassandra one may specify table property tune way data handled knowledge access pattern data layout transaction table improve performance cassandra able provide specific case bloom filter cassandra keep track bloom filter sstable memory avoid read sstable data file bloom filter keep track whether sstable contains data specific row thus allows cassandra determine without performing operation whether given sstable probably contains data row definitely contain data row probability bloom filter return false positive configurable via bloomfilterfpchance though accurate bloom filter require ram cassandra documentation suggests typical value lie typically within atlasdb false positive rate set depending whether table append heavy read light mean given size tiered compaction strategy whether negative lookup expected frequent determined user schema static final double defaultleveledcompactionbloomfilterfpchance static final double defaultsizetieredcompactionbloomfilterfpchance static final double negativelookupsbloomfilterfpchance static final double negativelookupssizetieredbloomfilterfpchance far transaction concerned observe number partition small thus afford low setting thus set bloomfilterfpchance empirically observed setting bloom filter transaction table writing every timestamp one one billion contrast bloom filter transaction atlasdbs existing setting index interval cassandra partition index ondisk file store mapping partition key offset within sstable cassandra maintains partition summary sstable memory sample partition index every key store offset location mapping given key within file value may tuned smaller value require memory improve performance seek within partition index shorter transaction number partition expected small set minindexinterval maxindexinterval forcing partition index perfect compression chunk length cassandra compress sstables block disk block configurable size choosing larger block may enable better compression since similarity column value may exploited expense needing read data disk read occurs typically atlas set reduce amount transaction expect user often reading data relatively smaller working set case larger chunk size enables better compression increase proportion said working set maintained memory experimented several setting found good balance empirical evaluation cassandra table parameter ran several benchmark internal testing stack different level concurrency different hit rate transaction table benchmark ensured actually performed disk operation evaluate performance done sstableloader ingest billion timestamp pair test stack stress tool consume almost free memory outside cassandra heap avoid operating system page cache also create fresh transactionservice benchmark run circumvent applicationlevel caching following benchmark run hit rate every timestamp queried corresponded existing transaction attempted run test various configuration optimisation discussed simply transaction algorithm transaction algorithm standard atlasdb table setting bloomfilterfpchance minindexinterval maxindexinterval unspecified chunklengthkb refers explicit configuration bloomfilterfpchance refers explicit configuration minindexinterval maxindexinterval ckn refers explicit configuration chunklengthkb concurrency metric bfii bfiick bfiick result may better visualised graph example concurrent reader next set benchmark run hit rate note rare practice practice miss try roll back transaction inserting entry transaction table concurrency metric bfii bfiick bfiick determined final choice setting bfiick brought transaction read performance mostly line transaction hit rate also notice seems regression unoptimised transaction optimisation probably useful seems performance hit lower percentile hit rate test though deem bad read miss make future read value modulo race condition hit meaning unlikely steady state cell loader background cell loading atlasdb load data multigetslice cassandra endpoint codeblock java map multigetslicerequired list key required columnparent columnparent required slicepredicate predicate required consistencylevel consistencylevelconsistencylevelone throw invalidrequestexception ire unavailableexception timedoutexception slicepredicate cassandra struct allows client specify column want read column loaded key presented notice method support multiple key one predicate thus atlas try perform get collection cell rowcolumn pair first group pair column parallel dispatch request cassandra row relevant example one cell atlas would send three request column key column key column key note practice request make range predicate cassandra cell dont include timestamps latest timestamp cell existed isnt something know priori multiget multislice model work well transaction transaction cell end distributed reasonably evenly among column case thus attempting determine whether atlas value committed perform many request parallel request end many resource cassandra connection pool also incur lot overhead term scheduling network want able batch call together added another endpoint thrift interface palantirs fork cassandra provides codeblock java struct keypredicate optional binary key optional slicepredicate predicate map multigetmultislicerequired list request required columnparent columnparent required consistencylevel consistencylevelconsistencylevelone throw invalidrequestexception ire unavailableexception timedoutexception implementing endpoint cassandra side difficult may seem little wasteful may require key predicate specified transaction would likely mostly distinct improved performance still faced significant regression relative cell loader determined cassandra worker pool loading value satisfy readcommand calling thread request also allowed participate thus creating large batch would turn likely detrimental read performance even cassandra node actually able handle higher concurrency safely selective batching thus settled compromise sending large singular request inundating cassandra smaller one unlike original cellloader make batch parameter configurable two parameter cross column load batch limit may combine request different column one call merged call exceed size single query load batch limit single request never larger size expect still partition request load cell column first thereafter given column number cell least cell column exclusively take one batch batch size greater otherwise cell may combined cell column batch size guarantee cell given column batch however guarantee column returned batch first last column batch term implementation simply maintain list cell eligible crosscolumn batching partition list contiguous group size way row key included twice may possible reduce amount data sent wire cassandra possibly internal read bandwidth solving underlying binpacking problem ensure rowkey occurs consider duplicate many key want load cell many column may worth considering future binpacking npcomplete algorithm like firstfit decreasing give good approximation implemented yet overhead constant factor many case transaction expect number cell per column small consider assuming uniform distribution even single transaction read value maximum batch size probably exceed example supposing one cell partitioned column number cell column follows suppose column cell column least cell column fewer cell loaded single request column cell cell loaded three parallel request remaining column fewer cell visit column lexicographical order first batch consisting cell column cell column second batch consisting cell column cell column though note request done parallel notice optimal end sending request cell column twice incurs network serialization overhead possible combine request column single batch size thereby removing overhead however discussed problem computationally difficult general benchmarking tested selective batching cell loader original algorithm fullbatching algorithm always batch cell regardless row column tested loader general atlasdb user workload row static column row static column workload specific transaction row dynamic column important would prefer separate codepath transaction current behaviour loading query row many different column regardless table also previously observed inefficient first ran benchmark single thread aforementioned workflow test dynamic column random unlikely overlap time reported millisecond row column metric cellloader full batching cellloader static static static static static static dynamic dynamic dynamic result may better visualised graph example response time notice row test cellloader performs marginally better cellloader probably able make rpcs instead recall full batching algorithm performs worst probably owing cassandra latency one requestor thread apart worker pool executing request row test cellloader performance similar expected underlying call cassandra cluster column single rpc full batching algorithm performs poorly however row dynamic column test cellloader performance poor may make many distinct rpcs owing different column key full batching algorithm still suffers one requestor thread cellloader able divide approximately parallel rpcs performs best overall also ran benchmark concurrent reader workflow time reported millisecond row column metric cellloader full batching cellloader static static static static static static dynamic dynamic dynamic magnitude full batching perform well cellloader also much lower possibly worker pool finite size even full batching algorithm case reader contributes one requesting thread although cellloader spin many requesting thread cassandra side cassandra cluster unable actually thread work concurrently live migration coordination service consequence write performance read performance data compression operational concern backup restore cassandra dependency safe installation considered timelock paxos mechanism transaction rowtickets algorithm multiget multislice exactly future work dbkvs transaction']"
20,196, Data Storage Decisions,['Data Storage Decisions'],"['postgres', 'azure blob', 'database', 'aws', 'azure', 'blob storage', 'dynamodb', 'storage dedicated', 'storage', 'storage encrypted']","['title joex job executor weight problem statement docspell multiuser application processing user document must thought distribute processing job much restricted set resource maybe user core process document time simply fifo enough since provides unfair distribution first user submits document occupy core quite time user would wait try find fair distribution among user strictly meaning collective docspell job executor separate component run process take next job queue executes associated task run document processing job text extraction text analysis etc task execution survive restarts state task code must recreated persisted state processing fair respect collective must possible run many job executor possibly different machine quickly enable processing power removing peak task execution fail able retry task reason error may temporarily example talking third party service enable repairing without stopping job executor error might easily repaired program installed whatever case good know task retried later considered contrast adrs sketching thought current implementation job description serialized written database table becomes queue task identified name job executor implementation must map name code lookup task perform task argument serialized string written database task must decode string conveniently done json provided circe decoder provide fair execution job organized group new job requested queue first group selected roundrobin strategy ensure good enough fairness among group group map collective within group job selected based priority submitted time fifo job state see note stuck job allowing multiple job executor mean getting next job fail due simultaneous running transaction retried succeeds taking job put scheduled state job executor unique manually supplied job marked handed executor task fails state updated state stuck stuck job retried future queue prefers return stuck job due specific point time ignoring priority hint detail job property something random group taskname choose task run submitteddate worker job executor state one waiting scheduled running stuck cancelled failed success waiting job inserted queue scheduled job handed executore marked job executor running task currently executing stuck task failed retried eventually cancelled task finished cancel request failed task failed execeeded retries success task completed successfully queue take nextjob operation take workerid priority hint roughly like select next group roundrobin strategy select job group state stuck waiting time elapsed state waiting given priority possible job ordered submitted time stuck job whose waiting time elapsed preferred two priority within group high low configured counting scheme determines select certain priority example counting scheme would select two high priority job low priority job take operation try prefer priority fall back job priority available group corresponds collective collective get roughly equal treatment job queue executor sleep must waked run job submitted executor notified stuck job job going stuck state task failed state task rerun maximum retry count reached problem notify executor waiting time elapsed one executor put job stuck state mean others start looking queue minute would possible tell existing executor schedule wake future would miss executor show later waiting time increased exponentially retry retry meant minimum waiting time executor wakeup periodically check new work time necessary fallback stuck job queue nothing submitted long time system job get submitted would awake executor', 'replace kbparallels another solution avoid deadlock related adr note adr place keep current discussion httpsdocsgooglecomdocumentdawjaymoqcogkpotjxxevoyynftcecbdnevturo rather make still planning scoping testing involved fully design system still determined scope adr relates bulk execution xsv analysis relates bulk execution intro sometimes calculation requires many resource one node walltime memory disk calculation get spread across multiple machine final step app kbparallels create report step may result computed job create final report order following apps mechanism called kbparallel kbbowtie refseqimporter kbconcoct kbphylogenomics kbhisat kbmetadecoder kbbwa current implementation batch analysis kbbatchapp kbase following issue current adequate user shouldnt code order run batch analysis also difficult even familiar kbase code find object name dependency kbparallel change kbparallel could affect batch subsequently apps queue deadlocking user max slot queue current implementation one management job created manage job submits could lead deadlock scenario management job waiting submit computation job cannot slot kbp spawn kbp job batch job spawn batch job missing ability able run manage cancel track job subjobs along ability specify resource differently main sub job good way test hard benchmark measure performance code split necessary doesnt properly display progress batch job author bioboris mrcreosote planning current idea documented outcome pending research iron detail first pas would likely limit number kbparallel run next pas would want create comprehensive generalized solution submitsplit aggregate recipe convenience common operation creating set report thing nature would also want user study want functionality want may inform design backend system deprecate kbp instead break apps part fan process parallel pip fan step user launch job normal possibly job marked job make easier display job correctly initially ideally would marked spec might lot work could potentially marked catalog along job requirement job figure pip sub job job sends following info job parameter sub job app job kbphylogenomicsbuildmicrobialspeciestreereduce start subjobs associate job probably retry handling subjobs deal transient error whenever subjob finish check see subjobs finished true start job providing output subjobs list reduce job job finished job done various job communicate storing temporary data caching service blobstore latter job clean blobstore node complete could make helper app workflow engine wdl cromwell reinventing wheel new endpoint speed reduce complexity step note dag endpoint dag would least first job followed subdag external somewhere first job youd generate new dag workflow defines cluster followed job run subdag dagman support python binding following two way htcondorsubmitfromdag take name dag filename submit resulting object like regular job htcondordags library programmatically construct dag workflow computer memory write dag file submit function mentioned several different way want there useful example show general workflow binding httpshtcondorreadthedocsioenlatestapispythonbindingstutorialsdagcreationandsubmissionhtmldescribingthedagusinghtcondordags consequence implement new narrative however work would happen regardless due looking improve batch upload analysis kbase take significant time research engineer solution still determined scope adr relates bulk execution xsv analysis relates bulk execution considered ignore issue make apps run kbparallels limited instance kbparallels per user avoid deadlock remove kbparallels change apps collection apps submit split aggregate endpoint create dag different devops solution rewriting kbp swapping lightweight subset kbp feature pro con general note current implementation kbp separate kbp queue multiple machine save spot user job maximum running job take waste compute resource especially node sit idle user still get job spot job run overall system make another queue requires taking compute node currently dedicated queue without changing apps kbp running multiple kbp apps machine interfere want avoid scrap kbp favor lightweight avoid previous issue modify apps kbp lightweight lightweight would guarantee computation besides job management occured could management job sit wait job without interfering job system increase number slot per user simple solution quick turnaround fix deadlock issue small number job doesnt fix deadlock issue user still submit kbp job address deadlocking issue still broken regular run batch run small amount user take entire system able submit job node continue taken job little computation job get node capacity still wasted kbp job sit around waiting job run limit kbp job maximum active kbp job per user simple solution requires maintain list kbp apps add kbplimit job list condor kbplimit added list apps frequently updated apps modified new app kbp app list wont limited kbplimit unless owner let know existing app longer kbp app still limited unless owner let know node continue taken job little computation job get node user may able effectively job spot capacity still wasted kbp job sit around waiting job run limit kbp job maximum active job per user seperate queue kbparallels apps pro user able effectively job spot allows group kbp job onto fewer machine instead giving entire node requires going app understanding worst case computational order set estimated cpu memory app apps interfere innocent apps take creating new queue requires balancing many active kbp node many node available job capacity still wasted kbp job sit around waiting job run build kbp lightweight version kbp queue design new verison apps must modified new kbp lightweight version either modify kbp create new toolpackage instead kbp launch management job called job manager sits kbp queue alongside kbp job job launched queue launch setup job user parameter andor optionally download data user parameter figure job manager parameter result information gathered initial download user parameter generate final parameter sent job manager launch fan job directly launch fan job return job job manager enough parameter launch andor monitor fan job monitormanage job possibly retry upon failure fan job download data perform calculation save back system return reference saved object job manager launch one fanin job based user parameter result fan job fanin aka groupreducereport job downloads object system creates set grouping save object back system final data report uploaded back system job manager return reference result report job proscons kbp job run small subset machine deadlock issue fixed change required address deadlocking issue still broken regular run batch run reuse kbp needed basis would rewrite apps kbp new paradigm modify apps local submission remove kbp moving job simple solution quick turnaround fix deadlock issue fix issue limited number larger resource machine continued dependency deprecated kbp tool app run may take longer since fewer resource may available app run', 'queue lambda run backend file check user uploads folder tdr consignment tdr run set check file mvp release check antivirus scan sha checksum compared checksum calculated user browser file format useful metadata preservation system ingests file transferred tdr highlight file national archive accept like passwordprotected file file user might uploaded mistake like executable file file uploaded bucket workflow run scan uploaded file user may upload several thousand file size could range byte gigabyte warn user file check may take time think user experience better user review file check result soon possible uploading file ideally take minute rather hour may add file check future additional checksum algorithm antivirus software architecture flexible enough add remove check later aws lambda run file check sqs coordinate step see architecture diagram file check workflow fit overall architecture step workflow user uploads file tdr frontend trigger upload event aws sends topic message contains object key includes tdr file uuid message passed first sqs queue lambda connected sqs queue take message extract file downloads file efs temporary file store lambda finished add message three sqs queue one backend check sqs message trigger respective backend check lambda checksum file format antivirus lambda sends message api update sqs queue containing result check final lambda pick message sends result consignment api stored database check single short process make suitable serverless workflow user uploads thousand file process parallel let sqs lambda work backlog without scale instance also trying serverless service tdr possible remove patch server testing far havent reached maximum lambda execution time minute hit limit could consider moving slow step task doesnt execution time limit slower start would way trigger task sqs would otherwise fit workflow way lambda moment considered step function instead sqs coordinate task decided tried alpha prototype found failed hit lambda concurrent usage limit currently function hit limit user uploads consignment thousand small file dont problem sqs message wait queue aws let start another lambda initial step downloading file lambda necessary antivirus file format task access file disk file downloaded efs rather lambda disk storage efs network file store access slower local file lambda disk storage limit lambda currently efs let run check much larger file file check lambda read file efs store file downloaded save transfer storage cost reduces complexity individual file check separate lambda sending result api introduces another sqs queue lambda several advantage making file check send result api separately api update step fails retry api update dont rerun whole potentially slow file check choose throttle api update lambda api cannot handle load receiving result thousand file without reducing throughput file check reduces number api client make easier roll api client library update file check apply business logic report raw result check example file format step find zip file report result api record failure warning antivirus check perform action addition reporting result copy file clean quarantine bucket depending whether file passed failed virus scan file clean bucket later tdr workflow exporting data file quarantine bucket inspected tdr admins want find file failed scan queue configured retry message three time attempt fail message sent dead letter queue able inspect message queue monitor size spot processing problem current limitation load testing time writing havent done systematic load testing uploading large file consignment large number file may modify architecture deal performance issue find slow file format identification droid extract file format information droid designed run folder file rather individual file lambda function normally make file format step slower antivirus checksum step becomes problem user wait hour result could consider extracting core logic droid suitable running lambda could also consider batching file would require change message passed step potential concurrency issue result check identical check repeated unless file replaced check function updated example update antivirus file format signature mvp user way replace file tdr support future user replace passwordprotected file unencrypted one think store metadata version file sometimes update file check currently way user admin rerun file check seems quite likely well build functionality order rerun failed file check low volume careful scrutiny transfer expected mvp dont think concurrency problem old data overwrites new data current architecture parallel file check make possible revisit find way fix one store check version number alongside metadata optimistic locking prevent metadata older version number overwriting newer metadata another add pessimistic lock prevent new check given file starting old check finished significant change architecture could slow processing another possible concurrency issue come temporary file store currently file saved folder name based consignment mean message added queue file file downloader try download file location twice could lead conflict file check running incomplete file could fix saving file unique folder uuid lambda task passing file download task file check way mark check complete currently infer file check complete antivirus file format checksum metadata saved database every file consignment long term plan also save kind consignment process determine state without inspect potentially thousand database row implement well decide update result file check arrive independently there obvious point theyre finished without looking rest result later refinement discovered performance issue large number file uploaded didnt change architecture described decide change configuration value see adr detail']"
21,43, Email Delivery Architecture,['Email Delivery Architecture'],"['alert system', 'email service', 'email notification', 'email alert', 'notification type', 'email provider', 'notification', 'alert api', 'mail', 'sending email']","['monitoring alerting amended updated reflect team paused email alert api acquired undesirable reputation application produce high volume unactionable alert frequent source govuk incident monitoring alerting approach longer appears sufficient application behaviour application volume work changed past year previous year application would reach peak sending million email day whereas sending million email day elevated volume work increased system latency impacted alert contributed incident approach monitoring application wider email alert system two fold one approach monitor email alert apis internal state see completing task within particular time frame whether high quantity work approach endtoend check verifies gmail account received email within time frame approach flawed monitoring high quantity work whether work completed time frame left email alert api vulnerable producing alert application busy yet fully operational alert unactionable support engineer alert resolve work completed little engineer speed process example application accumulated email send raise critical alert even sending fastest rate notify would allow alert therefore waste time support engineer instance occurring represent problem doesnt require intervention increase risk engineer wouldnt act alert actually represent application broken since alert cant relied upon reliably suggest application broken arent outofhours alert therefore even reflect genuine problem engineer would contacted engineer instead learns problem outofhours due type monitoring approach endtoend check alert content published travel advice publisher arrive particular gmail inbox within specified time period monitoring inbox limited set email flaw email alert api break outside office hour engineer would notified unless travel advice content published alert also monitor beyond boundary email alert system considering whether govuk publishing notify gmail working leading alert representing problem irrelevant system outside control govuk engineer gmail identifying email spam finally nature time based alert also vulnerable system busy sufficiently high volume travel advice publishing guarantee alert trigger reviewing monitoring approach helped understand reputation unactionable alert reviewing govuk incident email alert api helped identify pattern incident reflected ambiguity whether system broken felt room improvement decided system raise critical alert email alert system broken majority alert contact engineer outofhours unpacking consider email alert system comprised application email alert api email alert frontend email alert service excludes monitoring whether govuk publishing notify working independently monitored reflects condition weve formed system raise critical alert unless highly confident broken whereas warning appropriate situation decided email alert system broken represents failing lead user noticeable issue unless engineer intervenes since alert represent broken system requiring intervention suitable contact engineer outofhours otherwise user experiencing issue decided reapproach alerting mindset look replace alert vulnerable reflecting busy extend beyond boundary system proposed team working piece work paused december unclear whether future team formed complete work consequence changing approaching monitoring alerting across email alert system application order meet new alerting criterion monitoring email alert api sidekiq worker configure critical alert contact engineer outofhours individual sidekiq job fail multiple time following worker dailydigestinitiatorworker digestemailgenerationworker processcontentchangeworker processmessageworker sendemailworker weeklydigestinitiatorworker worker database store number time attempted perform action alert raised quantity failed attempt reach threshold suggestion based evidence consistent failure expect threshold worker even though sendemailworker speaks external service notify may likely fail failure rate low make consecutive failure unlikely ensure job mentioned worker actually run configure recoverlostjobsworker identify run attempt protects lost work problem sidekiq crashing expect sendemailworker digestemailgenerationworker job run quickly duration second know high volume run regularly reach volume concerned scenario consistently fail take long time engineer informed could occur could take long time sufficient retries occur alert raised retry placed back queue reflect concern contact engineer either worker completed one successful run within time period despite work existing suggested time period minute sidekiq retry cadence recovery window job updated ensure job always attempted within time period ensure dont alert work isnt attempted contact engineer outofhours determine digestrun record created within reasonable time frame suggestion hour creating trivial essential operation relies schedule lack record suggests either schedule broken code running scheduled time failed also monitor run recoverlostjobsworker metricscollectionworker succeeding least within time frame suggestion hour job continue running even system busy lack running indicates longer confidence system monitored able recover thus alert created contact engineer outofhours succeed time frame sidekiq worker dont affect user noticeable aspect email alert api emaildeletionworker wont raise critical alert failing instead raise warning sidekiq worker exhaust retries placed dead set approach chosen pragmatic technique monitor multiple worker without adding alert individual worker worker retry cadence considered alert reasonable time frame retries exhausted sidekiq default day long modify latency check email queue treating transactional email different latency check email expectation prompt delivery lack block user progressing task raise warning min critical engineer contacted outofhours latency reach min latency check made significantly sensitive latency change time warning occurs measure multiple hour compared minute intention serve alert support engineer system exceptionally busy may unhealthy alert reach critical hit hour point would indicate consistently creating email faster application send may mean system perpetually backlogged web application decided mechanism alert people outofhours email alert api web application appears broken noting currently engineer unlikely contacted event whole email alert api application becoming unavailable unless travel advice published noted wasnt particularly consistent approach across govuk apps raising outofhours alert single instance fails healthchecks felt would good take simple approach avoid surprising engineer decided appropriate place alert would application load balancer healthy instance application represents instance web application healthy request succeed alert removed remove following alert become superseded content change unprocessed hour message unprocessed hour incomplete digestruns hour monitoring email alert service email alert service listens publishing api event communicates email alert api currently alert monitored inhours system fails visibility outofhours problem intend improve consolidate existing alert contact engineer outofhours pattern failure processing event monitoring email alert frontend email alert frontend act public interface subscriber interaction email alert system web application monitored similar way email alert api web application considering whether load balancer healthy instance removal email alert monitoring email alert monitoring aforementioned endtoend check monitor whether travel advice medical safety content change result email received gmail inbox intend retire check believe new approach monitoring email alert system cover aspect check within scope email alert system broadly email alert monitoring verifies content published outofscope email alert system publishing api concern whether publishing application successfully communicated email alert api outofscope email alert system publishing application concern whether content change converted email inscope email alert system monitored identifying processcontentchangeworker failure whether email sent govukemailcheckdigitalcabinetofficegovuk inscope email alert system monitored identifying sendemailworker failure whether notify actually sends email outofscope email alert system notify concern whether gmail successfully receives email outofscope email alert system google concern completed configuring alert cover inscope scenario remove email alert monitoring govuk stack', 'record initial architecture notify integration introduction october govuk email team began project intending expand responsibility emailalertapi application handle creating sending alert enter system changed system intends notify transportation layer send email part considering conducted piece work outline architecture application document present treating digest time putting together unclear exactly product functionality would regarding digest however want considered architecture clarity explains best guess understanding digest work prior appropriate product digest expected continue current behaviour merging multiple subscription single email essential digest return content user subscription digest processed joe processed min steve may get slightly different result something published time subscribed thing produce content two subscription different digest frequency receive notification piece content higher frequency digest take effect point subscription joined weekly digest wednesday sent friday youd get alert content joined send remaining digest email someone change subscription preference specific architecture visualised two diagram domain model represents data model relate sequence diagram represents component communicate achieve specified functionality document concept diagram outline role responsibility contentchange responsibility representing piece content either new changed produce alert shall store information title content description content change note associated update path content store information subscription criterion match link tag supertypes field required auditing request came able determine whether processed worker processedat presence persisted forever may future association entity model subscription criterion information link tag super type considered part alert supertype may represent type alert system subscriber list responsibility type list user subscribe storing information specific list shall store information list name require list name unique dont know differentiate identically named list user yet temporarily maintain relationship corresponding govdelivery list store information subscription criterion match link tag documentsupertypes information source subscriber list originated may future association entity model subscription criterion information link tag super type release requirement list name unique able establish way differentiate list store statistic related list stored even removed logging purpose dependent impact unique constraint determine scenario subscriber list removed subscriber responsibility represent individual user subscribe content store information related subscription user may shall store information email address support email address changing deleteable user may wish remove account break history email sent future authentication purpose may store preference affect user subscription ability disabled problem user account store personalisation information name associated statistic regarding subscriber determine allow term removing subscriber associated data log subscription responsibility store subscriber association subscriber list associated data preference shall store association subscriber store association subscriber list store preference associated particular subscription removed system user unsubscribes subscriber list provides log user subscribed associated content maintaining record allows restore preference subscription resumed may store statistic subscriber activity relating subscription future something paused determine happen either subscriber list subscriber removedarchived subscriptioncontent responsibility represent content change emailed subscription shall association contentchange association subscription nullable association email know whether processed allow multiple subscription associated subscriber associated contentchange emailgeneration responsibility consolidate allow multiple entry contentchange subscription allow content change reprocessed event error support email record removed system auditing process deleteable scenario user change subscription email sent determine archiving strategy persisted forever match persistence strategy email instance email responsibility distinct email system sending sent shall created time email alert api sends email store information recipient subject body notify markdown delivery shall one recipient know anything content led generation exist database non permanent time period associated subscriber logging purpose may archived log somewhere later analysis deliveryattempt responsibility storing information communicate transportation service email shall store information email associated provider notify provider reference look email successpendingfailure provider specific errorswarnings created whenever email attempted sent updated sufficient information known whether email sent deleted email deleted may created success state known delivery attempt successful synchronously future different transport mechanism expand beyond email contentchangecontroller responsibility storing contentchange enquing processing returning success shall create contentchange model enqueue subscriptioncontentworker job determine responsibility worker failing run subscriptioncontentworker responsibility responsible determining subscription receive email prospectively creating subscriptioncontent object accordingly shall given sufficient input lookup contentchange liaise subscriptionmatcher determine subscription match contentchange create subscriptioncontent entity subscription match contentchange trigger emailgenerationworker email run immediately able multiple instance worker processing contentchange concurrently may accept frequency argument match subscription frequency order allow rebuilding email went error subscriptionmatcher responsibility given certain criterion determine subscription object associated shall accept contentchange object input input determine subscription object match existing logic subscriberlistquery determine considering might scale huge number subscription emailgenerationworker responsibility responsible finding subscription object sent given frequency producing email object sent shall find subscriptioncontent entity match frequency processed take one subscriptioncontent entry user create corresponding email entity update subscriptioncontent entry associate email indicate processed liaise emailrenderer convert one subscriptioncontent entity subject body email take input frequency immediate weekly daily able consolidate subscriptioncontent entry refer contentchange able process subscriptioncontent object multiple concurrent process may interface reconsidered allow increased ability build email parallel determine user multiple subscription different frequency class consolidate digesttimer responsibility triggering emailbuilderworker run given frequency shall trigger emailgenerationworker run daily interval trigger emailgenerationworker run weekly interval determine interval mechanism would place handle failing occur emailrenderer responsibility converting one email entry notify markdown formatted text shall take input one subscriptioncontent entity frequency generate subject plain text generate body notify markdown able generate differently formatted email weekly daily email may sufficient information include unsubscribe link email emaildeliveryworker responsibility finding email sent creating deliveryattempts updating email accordingly shall communicate transportation layer notify send email create deliveryattempt object associate email update email deliveryattempt object based outcome request able retry request notify fail retryable temporary network outage able abort trying send email situation retrying wont result success bad request may react transportation layer returning code determine happen email exhausted retries happen email fails due bad request deliverymonitor responsibility determining whether notify able send email tried send shall communicate transportation layer notify determine pending delivery attempt still progress successful failure update email deliveryattempt object based outcome request able abort attempting send email notify return temporary failure email address able removesuspend user account notify return permanent failure corresponding email address able retry sending email notify return notify failure error indicates sending retried determine system rule behaviour follow removingsuspending account receive permanent failure message notify', 'email delivery responsibility email asynchronous communication medium inherently two boolean scenario true email received email sent mail server mail server deliver email recipient mail server resolving whether email sent rather simple question resolved synchronously mail server accept email however determining whether email received complex delivery synchronous may subject automated retrying problem occur typically medium email client consider first scenario reporting email point email becomes sent latter scenario delivery isnt typically reflected email client normal assume email received successfully unless later receive email indicating bounce occurred initial email alert api notify integration designed consider sending delivery email system monitor whether notify managed deliver email recipient automatic retry mechanism notify problem delivering email september decided reevaluate concept conflation concerned notify functionality duplicated email alert api adverse effect complexity system decided email alert api responsibility send email responsibility deliver responsibility notify defined email alert api process sending email ability successfully send request notify perform action consider equivalent mail client successfully sending email smtp server consider notify callback mechanism learn email delivered reflect switched meaning email sent mean email notify email delivered failed mean werent able send email notify instead previous meaning meant may sent notify notify failed deliver email consequence longer retry delivering email notify via amazon automatically retry delivery email fail transitory reason recipient mail server full offline doesnt succeed reasonable period time notify inform telling email experienced temporaryfailure email alert api longer retry mechanism reattempt email failed delivered resolve aspect duplicated functionality two system area ambiguity long email alert api retry removed longer attempt resend email first delivery fail allowed delete deliveryattempt model purpose disambiguate different notify request longer record delivery data email question pondered change data receive notify delivery email data longer play role state modelling email sending decided represented email conflated failure send email meant failure send failure deliver decided store delivery success failure database email currently known data since email table day retention period added later meet later identified data remains available via notify debugging whether email delivered continue act permanentfailure notification remove subscription nonoperational email address done without storing additional data email also continue store aggregate metric email sending power dashboard']"
