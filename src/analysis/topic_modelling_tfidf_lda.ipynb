{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Library/Frameworks/Python.framework/Versions/3.12/lib/python312.zip', '/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12', '/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/lib-dynload', '', '/Users/nikolakis/Library/Python/3.12/lib/python/site-packages', '/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages', '/Users/nikolakis/Projects/ADR-thesis/src/scrapping']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Add the parent directory of 'scrapping' to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join(current_dir, '..', 'scrapping')))\n",
    "\n",
    "# Now you can import the function\n",
    "from text_cleaner import read_and_clean_adrs, read_and_clean_adrs2\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from markdown2 import markdown\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(sys.path)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Path to the ADR directory\n",
    "adr_directory = \"../../data/ADRs-Updated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_texts = read_and_clean_adrs(adr_directory, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic models thereby aim to uncover the latent topics or themes characterizing a set of documents. In this way, topic models are a machine learning-based form of text analysis used to thematically annotate large text corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF + LDA (hyperparameters obtained from grid search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# good results: \n",
    "# 1) n_components=4, learning_decay=0.5, learning_offset=10, max_df=0.9, min_df=5, max_features=2000\n",
    "# 2) n_components=5, learning_decay=0.5, learning_offset=10, max_df=0.9, min_df=5, max_features=2000\n",
    "# 3) n_components=4, learning_decay=0.5, learning_offset=10, max_df=0.9, min_df=5, max_features=1000\n",
    "\n",
    "# Notes: From hyperparam tuning\n",
    "# n_components=4, learning_decay=0.5, learning_offset=10, max_df=0.9, min_df=5, max_features=1000\n",
    "\n",
    "# Define the vectorizer and LDA model\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.9, min_df=5, max_features=1000)\n",
    "lda = LatentDirichletAllocation(n_components=4, learning_decay=0.5, learning_offset=10, n_jobs=-1, max_iter=50)\n",
    "\n",
    "X = tfidf_vectorizer.fit_transform(cleaned_texts)\n",
    "\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prominent Topics\n",
    "- Topic 1: Cloud and Infrastructure --> prevalent in all tries\n",
    "- Topic: Data and data storage --> prevalent in almost all tries\n",
    "- Topic 2: Programming language and frameworks --> Have to dig in a bit with different hyperparameters\n",
    "- Topic 3: Authentication and Security --> have to dig in a bit with different hyperparameters\n",
    "- Topic 4: General architecrture and design (classes, apis)--> prevalent in all tries --> maybe focus on this and split up into different topics\n",
    "- Topic 5: Linting, formating and conventions --> have to dig in a bit with different hyperparameters\n",
    "- Topic 6: Building and releasing (CI/CD, Testing) --> have to dig in a bit with different hyperparameters\n",
    "- Topic 7: ADRs about architectiral decisions --> seen with >4 topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate TF-IDF + LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "n_top_words = 20\n",
    "print_top_words(lda, tfidf_vectorizer.get_feature_names_out(), n_top_words)\n",
    "\n",
    "print(\"\\n\\nDistributions\")\n",
    "# see hoe many documents are in each topic\n",
    "topic_distribution = lda.transform(X)\n",
    "for i in range(4):\n",
    "    print(f\"Topic {i}: {np.sum(topic_distribution[:, i])}\" + f\" ({np.sum(topic_distribution[:, i]) / len(cleaned_texts) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to visualize\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.lda_model.prepare(lda, X, tfidf_vectorizer, mds='tsne')\n",
    "pyLDAvis.display(panel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for LDA and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vectorizer and LDA model\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "lda = LatentDirichletAllocation(random_state=42)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', tfidf_vectorizer),\n",
    "    ('lda', lda)\n",
    "])\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [1000, 2000, 3000, 4000],\n",
    "    'lda__n_components': [4, 5, 6, 7, 8, 9, 10],\n",
    "    'lda__learning_decay': [0.5, 0.7, 0.9],\n",
    "    'lda__learning_offset': [10, 15, 20, 30]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(cleaned_texts)\n",
    "\n",
    "# Evaluate the best model\n",
    "print(\"Best Parameters: \", grid_search.best_params_)\n",
    "best_lda_model = grid_search.best_estimator_\n",
    "\n",
    "# Print top words for each topic\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.named_steps['lda'].components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "\n",
    "n_top_words = 10\n",
    "tfidf_feature_names = grid_search.best_estimator_.named_steps['tfidf'].get_feature_names_out()\n",
    "print_top_words(best_lda_model, tfidf_feature_names, n_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
